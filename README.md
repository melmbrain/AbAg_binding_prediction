# Antibody-Antigen Binding Prediction (IgT5 + ESM-2)

**Deep learning model for predicting antibody-antigen binding affinity using dual protein language models.**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

**Current Version**: v2.6.0-beta (2025-11-25)
**Status**: âš ï¸ Experimental Release - v2.7 in development
**Architecture**: IgT5 (antibody) + ESM-2 3B (antigen) with Cross-Attention

---

## ğŸ“¦ Releases

### v2.6.0-beta (2025-11-25) - Current

**Status**: âš ï¸ Experimental (known stability issues - not for production)

- **Model**: IgT5 + ESM-2 3B with cross-attention
- **Performance**:
  - Spearman Ï: 0.390
  - RMSE: 2.10
  - Recall@pKdâ‰¥9: 100%
- **Training**: 15 epochs on A100 80GB (~60 hours)
- **Known Issues**:
  - Recall instability (oscillates 18% â†” 100%)
  - Invalid predictions (negative pKd values)
  - See [v2.6/README_v2.6.md](v2.6/README_v2.6.md) for details
- **Download**: [GitHub Release](https://github.com/melmbrain/AbAg_binding_prediction/releases/tag/v2.6.0-beta) | Model Card: [README_v2.6.md](v2.6/README_v2.6.md)

### v2.5.0 (2025-11-13) - Previous Stable

- **Model**: ESM-2 650M
- **Performance**: Spearman 0.42, RMSE 1.95
- **Status**: âœ… Stable (use for production)
- **Download**: See [CHANGELOG.md](CHANGELOG.md#250---2025-11-13)

### v2.7.0 (In Development) - Next

**Expected**: 2025-12-01

- **Fixes**: Stable MSE loss, prediction clamping, NaN detection, complete RNG state
- **Expected Performance**: Spearman 0.45-0.55, stable recall 50-70%
- **Roadmap**: [V2.7_IMPROVEMENTS.md](V2.7_IMPROVEMENTS.md)

### Pre-trained Models (Hugging Face)

Models hosted at: [Kroea/AbAg-binding-prediction](https://huggingface.co/Kroea/AbAg-binding-prediction)

```python
from huggingface_hub import hf_hub_download

# Download v2.6-beta (experimental - 16GB)
model_path = hf_hub_download(
    repo_id="Kroea/AbAg-binding-prediction",
    filename="best_model_v2.6_beta_esm2_3b.pth"
)

# Or use v2.5 (stable - 4.7GB)
model_path = hf_hub_download(
    repo_id="Kroea/AbAg-binding-prediction",
    filename="best_model_v2.5_esm2_650m.pth"
)
```

> âš ï¸ **Important**: v2.6.0-beta has documented stability issues (recall oscillation, invalid predictions). For production, use v2.5 or wait for v2.7 stable release.

---

## ğŸš€ Quick Start (Google Colab)

### 1. Upload to Google Drive
Place these files in `/MyDrive/AbAg_Training/`:
- `train_ultra_speed_v26.py`
- `agab_phase2_full.csv`

### 2. Run in Colab
```python
# Cell 1: Setup
from google.colab import drive
drive.mount('/content/drive')
import os
os.chdir('/content/drive/MyDrive/AbAg_Training')

# Cell 2: Install
!pip install -q transformers pandas scipy scikit-learn tqdm sentencepiece faesm bitsandbytes accelerate

# Cell 3: Train
!python train_ultra_speed_v26.py
```

**That's it!** Auto-resumes from checkpoints.

---

## ğŸ“Š Performance

- **Speed**: 4.45 iterations/second (confirmed working)
- **Time per epoch**: ~26 minutes
- **50 epochs**: ~21-22 hours total
- **Speedup**: ~5.5Ã— faster than original baseline
- **Memory**: ~12GB GPU (T4/V100/A100)

---

## ğŸ”§ Active Optimizations (17/19)

âœ… **Batch embedding generation** - 2-3Ã— faster (biggest win!)
âœ… **Sequence bucketing** - 1.3-1.5Ã— faster
âœ… **INT8 quantization** - Encoders only
âœ… **Activation checkpointing** - Enables batch 16
âœ… **BFloat16 mixed precision**
âœ… **FlashAttention** - If FAESM available
âœ… **Fused AdamW optimizer**
âœ… **Gradient accumulation** (Ã—3)
âœ… **DataLoader prefetching** (4 workers)
âœ… **Async checkpoint saving**
âœ… **TF32 precision** (A100)
âœ… **Cudnn benchmark mode**
âœ… **Fast tokenizers**
âœ… **Disk auto-cleanup**
âŒ **torch.compile** - DISABLED (prevents CUDA graphs errors)

---

## ğŸ“ Important Files

| File | Purpose |
|------|---------|
| `train_ultra_speed_v26.py` | âœ… **Main training script (WORKING)** |
| `WORKING_CONFIG.md` | Config documentation (4.45 it/s) |
| `notebooks/colab_training_SIMPLE.ipynb` | Simple Colab notebook |
| `archive/experimental_cuda_fix_2025-01-14/` | Old experimental files |

---

## ğŸ”¬ Model Architecture

```
Input: Antibody sequence + Antigen sequence
  â†“
IgT5 Encoder (frozen)  â†’  Mean pooling  â†’  [1280-dim]
ESM-2 Encoder (frozen) â†’  CLS token    â†’  [1280-dim]
  â†“
Concatenate [2560-dim]
  â†“
Regressor: 1024 â†’ 512 â†’ 256 â†’ 128 â†’ 1
  â†“
Output: pKd prediction
```

**Loss**: Focal MSE (gamma=2.0)
**Optimizer**: AdamW (fused, lr=4e-3, weight_decay=0.01)

---

## ğŸ’¾ Storage & Checkpoints

**Auto-saved files**:
- `checkpoint_latest.pth` - Latest state (auto-resume)
- `checkpoint_backup.pth` - Previous checkpoint
- `best_model.pth` - Best validation Spearman
- `checkpoint_epoch.pth` - End of epoch

**Disk management**:
- Auto cleanup every epoch
- Ultra-aggressive cleanup at 150GB
- Max 4 checkpoint files (~7.5GB total)

---

## âš™ï¸ Configuration (Working Settings)

**DO NOT CHANGE - This config is proven stable!**

```python
# Training
batch_size = 16
accumulation_steps = 3
epochs = 50
learning_rate = 4e-3

# Optimizations
use_compile = False          # âŒ DISABLED (prevents CUDA graphs errors)
use_checkpointing = True     # âœ… ENABLED (saves memory)
use_quantization = True      # âœ… ENABLED (INT8 for encoders)
use_bucketing = True         # âœ… ENABLED (efficient batching)
use_bfloat16 = True          # âœ… ENABLED (mixed precision)
use_fused_optimizer = True   # âœ… ENABLED (faster optimizer)
```

---

## ğŸ› Troubleshooting

### CUDA Graphs Error
**Fixed!** Current `train_ultra_speed_v26.py` has nuclear fix (lines 28-43):
- `torch.compiler.disable()` at import time
- `use_compile=False` in config
- No more crashes âœ…

### Out of Memory
Current config uses batch 16 with checkpointing.
If still OOM: Reduce to batch 12, accumulation 4

### Disk Space Full
Auto-cleanup triggers at 150GB.
Manual cleanup: See `WORKING_CONFIG.md`

---

## ğŸ“ˆ Expected Results

- **Validation Spearman**: ~0.7-0.8
- **Recall@pKdâ‰¥9**: ~70-80%
- **Training time**: ~21-22 hours
- **Model size**: ~2.6GB

---

## ğŸ“š Documentation

- `WORKING_CONFIG.md` - Detailed working config docs
- `START_HERE.md` - Getting started guide (if exists)
- `archive/experimental_cuda_fix_2025-01-14/` - Old experimental docs

---

## ğŸ“ Project Structure

```
AbAg_binding_prediction/
â”œâ”€â”€ train_ultra_speed_v26.py          # Main script âœ…
â”œâ”€â”€ WORKING_CONFIG.md                  # Config docs
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ data/
â”‚   â””â”€â”€ agab_phase2_full.csv          # Dataset (159k samples)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ colab_training_SIMPLE.ipynb
â”‚   â””â”€â”€ colab_training_ULTRA_SPEED_v26.ipynb
â”œâ”€â”€ src/
â”‚   â””â”€â”€ model_v3_full_dim.py          # Model source
â”œâ”€â”€ outputs_max_speed/                 # Checkpoints
â””â”€â”€ archive/
    â””â”€â”€ experimental_cuda_fix_2025-01-14/  # Old experiments
```

---

## ğŸ“ Citation

If you use this code:
- **IgT5**: [Exscientia/IgT5](https://huggingface.co/Exscientia/IgT5)
- **ESM-2**: [facebook/esm2_t33_650M_UR50D](https://huggingface.co/facebook/esm2_t33_650M_UR50D)

---

## âš ï¸ Critical Notes

1. **Current `train_ultra_speed_v26.py` is WORKING** at 4.45 it/s
2. **Do not modify config** unless you understand CUDA graphs issue
3. **All experiments archived** in `archive/experimental_cuda_fix_2025-01-14/`
4. **Nuclear fix applied** - torch.compile disabled globally

---

## ğŸš€ Version History

- **v2.6** (Current) - âœ… Stable, 17/19 optimizations, 4.45 it/s, no errors
- v2.5 - âŒ CUDA graphs errors with torch.compile + checkpointing
- v2.0 - Initial optimized version
- v1.0 - Baseline implementation

---

**Last Updated**: 2025-01-14
**Status**: âœ… WORKING - 4.45 it/s, no CUDA graphs errors
