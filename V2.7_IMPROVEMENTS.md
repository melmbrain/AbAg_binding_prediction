# Version 2.7 Improvements - Antibody-Antigen Binding Prediction

**Created**: 2025-01-XX
**Status**: In Development
**Based On**: v2.6 + CAFA6 Optimizations + 2024 Research

---

## Overview

Version 2.7 implements critical fixes based on:
1. **CAFA6 competition model optimizations** (proven techniques)
2. **2024 research-validated methods** (Multi-task Bioassay Pre-training, DualBind)
3. **Stability improvements** to fix recall oscillation

---

## Critical Issues Fixed

### 1. ❌ Unstable Soft Spearman Loss → ✅ Stable MSE Loss

**Problem in v2.6:**
```python
# O(n²) pairwise ranking - causes gradient instability
def soft_spearman_loss(self, pred, target):
    pred_diff = pred.unsqueeze(1) - pred.unsqueeze(0)  # [B, B]
    # ... unstable gradients
```

**Result**: Recall jumping 18% ↔ 100%

**Fixed in v2.7:**
```python
class StableCombinedLoss(nn.Module):
    """Research-validated (MBP 2024)"""
    def __init__(self, mse_weight=0.7, class_weight=0.3):
        self.mse = nn.MSELoss()  # Stable regression
        self.bce = nn.BCEWithLogitsLoss()  # Classification
```

**Source**: [Multi-task Bioassay Pre-training](https://pmc.ncbi.nlm.nih.gov/articles/PMC10783875/)

---

### 2. ❌ Unrealistic Predictions → ✅ Clamped to Physical Range

**Problem in v2.6:**
```
Pred range: [-2.48, 10.00]  ← Negative pKd is impossible!
```

**Fixed in v2.7:**
```python
pKd_pred = torch.clamp(pKd_pred, min=4.0, max=14.0)
```

Valid pKd range: 4-14 (physically realistic)

---

### 3. ❌ No NaN Detection → ✅ Early Failure Detection

**Added from CAFA6:**
```python
def check_loss_validity(loss, name="loss"):
    if torch.isnan(loss) or torch.isinf(loss):
        raise ValueError(f"{name} became {loss.item()}!")
    return True
```

Catches numerical issues before they corrupt training.

---

### 4. ❌ Incomplete Checkpointing → ✅ Full State Save

**Problem in v2.6:**
- Missing RNG states → Non-reproducible
- Missing scheduler state → Wrong LR on resume
- Drive sync issues → Lost checkpoints

**Fixed in v2.7:**
```python
checkpoint = {
    'epoch': epoch,
    'batch_idx': batch_idx,
    'global_step': global_step,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scheduler_state_dict': scheduler.state_dict(),
    # NEW: Full RNG state for reproducibility
    'rng_state': torch.get_rng_state(),
    'cuda_rng_state': torch.cuda.get_rng_state_all(),
    'numpy_rng_state': np.random.get_state(),
    'python_rng_state': random.getstate(),
    # NEW: Training metadata
    'best_spearman': best_spearman,
    'epochs_without_improvement': epochs_without_improvement,
    'training_history': history,
}
```

**Source**: CAFA6 Model Optimizations

---

### 5. ❌ No Overfitting Detection → ✅ Train/Val Monitoring

**Added from CAFA6:**
```python
overfit_ratio = val_loss / train_loss
print(f"  Overfit ratio: {overfit_ratio:.2f}x")
if overfit_ratio > 3.0:
    print("  ⚠️ WARNING: Overfitting detected!")
```

Helps diagnose recall instability.

---

## Hyperparameter Changes (Research-Validated)

### Learning Rate & Schedule

| Parameter | v2.6 (Old) | v2.7 (New) | Source |
|-----------|------------|------------|--------|
| Initial LR | 5e-4 | **1e-3** | MBP 2024 |
| Schedule | Cosine | **ReduceLROnPlateau** | MBP 2024 |
| LR Factor | - | **0.6** | MBP 2024 |
| LR Patience | - | **10 epochs** | MBP 2024 |
| Min LR | - | **1e-6** | MBP 2024 |

**Rationale**: Research shows ReduceLROnPlateau provides better stability than Cosine for binding affinity.

### Regularization

| Parameter | v2.6 (Old) | v2.7 (New) | Source |
|-----------|------------|------------|--------|
| Weight Decay | 0.01 | **1e-5** | MBP 2024 |
| Dropout | 0.3 | **0.1** | MBP 2024 |

**Rationale**: v2.6 was over-regularized (underfitting).

### Training Configuration

| Parameter | v2.6 (Old) | v2.7 (New) | Source |
|-----------|------------|------------|--------|
| Batch Size | 32 | **16** (physical) | Hardware limit |
| Gradient Accumulation | 4 | **8** | MBP 2024 |
| Effective Batch | 128 | **128** | MBP 2024 |
| Early Stop Patience | 10 | **15** | MBP 2024 |

**Rationale**: MBP research shows larger effective batch (128-256) improves stability.

### Loss Weights

| Component | v2.6 (Old) | v2.7 (New) | Change |
|-----------|------------|------------|--------|
| Huber Loss | 0.6 | **Removed** | Use MSE instead |
| Soft Spearman | 0.2 | **Removed** | Unstable |
| MSE Loss | - | **0.7** | Research-validated |
| Classification | 0.2 | **0.3** | Increased |

---

## New Features

### 1. Verified Checkpoint Saving

```python
def save_checkpoint_verified(checkpoint_data, save_path):
    """Save with verification"""
    temp_path = save_path + '.tmp'
    torch.save(checkpoint_data, temp_path)

    if os.path.getsize(temp_path) > 1e9:  # Must be > 1GB
        os.rename(temp_path, save_path)
        return True, os.path.getsize(save_path)
    return False, 0
```

Prevents corrupted checkpoints from Drive sync issues.

### 2. Reproducible Training

```python
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

# Reproducible dataloader
g = torch.Generator()
g.manual_seed(SEED)
train_loader = DataLoader(..., generator=g)
```

Exactly reproducible results every run.

### 3. Training History Tracking

```python
training_history.append({
    'epoch': epoch,
    'train_loss': train_loss,
    'val_spearman': val_spearman,
    'val_recall': val_recall,
    'overfit_ratio': overfit_ratio,
    'learning_rate': current_lr
})
```

Complete training logs for analysis.

---

## Expected Performance Improvements

| Metric | v2.6 (Current) | v2.7 (Expected) | Improvement |
|--------|----------------|-----------------|-------------|
| **Spearman** | 0.39 (oscillating) | **0.45-0.55** | +15-40% |
| **Recall@pKd9** | 18-100% (unstable) | **50-70%** (stable) | Stable |
| **RMSE** | ~1.8 | **1.2-1.5** | -20-30% |
| **Pred Range** | -2.48 to 10.0 | **4.0 to 14.0** | Realistic |
| **Training Stability** | Unstable | **Converges smoothly** | ✓ |

---

## Migration from v2.6 to v2.7

### For Fresh Training

1. Use new notebook cells (provided below)
2. Start from scratch with new hyperparameters
3. Expected training time: ~40-50 epochs until early stopping

### For Resuming from v2.6 Checkpoints

**Not recommended** - hyperparameters are too different. Better to:
1. Save v2.6 best model separately
2. Start v2.7 fresh
3. Compare final results

---

## File Structure

```
AbAg_binding_prediction/
├── notebooks/
│   ├── colab_training_OPTIMIZED_v2.ipynb  (v2.6 - old)
│   └── colab_training_OPTIMIZED_v2.7.ipynb (v2.7 - new)
├── V2.7_IMPROVEMENTS.md  (this file)
├── CHANGELOG.md  (updated)
└── models/
    ├── v2.6_best_model.pth  (Spearman 0.39)
    └── v2.7_best_model.pth  (Expected 0.45-0.55)
```

---

## Research Sources

1. **Multi-task Bioassay Pre-training (2024)**
   - URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC10783875/
   - Key: Learning rate 1e-3, ReduceLROnPlateau, dropout 0.1, weight decay 1e-5

2. **DualBind Framework (June 2024)**
   - URL: https://arxiv.org/html/2406.07770v1
   - Key: MSE loss for regression, combined with ranking loss

3. **CAFA6 Competition Optimizations**
   - Source: Internal `NOTEBOOK_OPTIMIZATIONS.md`
   - Key: RNG state saving, NaN detection, overfitting monitoring

4. **Antibody Design Deep Learning Review (2024)**
   - URL: https://pmc.ncbi.nlm.nih.gov/articles/PMC11221890/
   - Key: Attention mechanisms, multi-task learning

---

## Known Limitations

1. **ESM-2 3B model** - Large memory footprint (~20GB)
2. **IgT5 + ESM-2** - Encoders frozen (only train regressor head)
3. **Dataset imbalance** - ~10-20% strong binders (pKd ≥ 9)
4. **Single-task** - Only pKd prediction (no mutation effects)

---

## Next Steps (Future v2.8)

1. **Attention-based pooling** (from AttABseq 2024)
2. **Structure features** (if available)
3. **Data augmentation** (sequence mutations)
4. **Ensemble models** (combine multiple checkpoints)
5. **Test-time augmentation** (TTA)

---

## Changelog Entry

See `CHANGELOG.md` for detailed version history.

---

*Last updated: 2025-01-XX*
*Status: Ready for testing*
*Expected completion: After 40-50 epochs*
