{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Fast Model Training with Pre-computed Embeddings\n",
    "\n",
    "**Purpose**: Rapid experimentation with different fusion methods and model architectures.\n",
    "\n",
    "## Features:\n",
    "- âœ… **Auto-checkpoint**: Saves best model + progress every epoch\n",
    "- âœ… **Resume training**: Continue from last checkpoint if interrupted\n",
    "- âœ… **Live metrics**: Loss, RÂ², MAE, RMSE displayed per epoch\n",
    "- âœ… **Memory monitoring**: GPU/RAM/Disk usage tracked\n",
    "- âœ… **Optuna integration**: Hyperparameter optimization with checkpoint\n",
    "\n",
    "## Available Fusion Methods:\n",
    "1. **concat**: Simple concatenation\n",
    "2. **bilinear**: Pairwise feature interactions\n",
    "3. **cross_attention**: Attend between antibody and antigen\n",
    "4. **gated**: Learned importance weights\n",
    "5. **tensor**: Low-rank tensor product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONFIGURATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CONFIG = {\n",
    "    # ===== PATHS =====\n",
    "    'base_dir': '/content/drive/MyDrive/AbAg_Project',\n",
    "    'embeddings_dir': 'embeddings',\n",
    "    'output_dir': 'models',\n",
    "    'checkpoint_dir': 'checkpoints',\n",
    "    \n",
    "    # Embedding files (from Stage 1)\n",
    "    'train_embeddings': 'train_embeddings_IgT5_ProtT5-Half_lora_last.npz',\n",
    "    'val_embeddings': 'val_embeddings_IgT5_ProtT5-Half_lora_last.npz',\n",
    "    'test_embeddings': 'test_embeddings_IgT5_ProtT5-Half_lora_last.npz',\n",
    "    \n",
    "    # ===== MODEL ARCHITECTURE =====\n",
    "    'fusion_method': 'cross_attention',  # 'concat', 'bilinear', 'cross_attention', 'gated', 'tensor'\n",
    "    'head_type': 'residual_mlp',         # 'simple_mlp', 'residual_mlp', 'deep_mlp', 'multi_task'\n",
    "    \n",
    "    # Fusion settings\n",
    "    'fusion_hidden_dim': 512,\n",
    "    'fusion_num_heads': 8,\n",
    "    'fusion_dropout': 0.1,\n",
    "    \n",
    "    # Head settings\n",
    "    'head_hidden_dims': [512, 256, 128],\n",
    "    'head_dropout': 0.2,\n",
    "    'use_batch_norm': True,\n",
    "    \n",
    "    # ===== TRAINING =====\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'num_epochs': 100,\n",
    "    'early_stopping_patience': 15,\n",
    "    'scheduler': 'cosine',\n",
    "    \n",
    "    # Loss\n",
    "    'loss_type': 'mse',\n",
    "    'huber_delta': 1.0,\n",
    "    \n",
    "    # Multi-task (if head_type == 'multi_task')\n",
    "    'classification_weight': 0.3,\n",
    "    'regression_weight': 0.7,\n",
    "    \n",
    "    # ===== CHECKPOINTING =====\n",
    "    'checkpoint_every_n_epochs': 1,\n",
    "    'resume_from_checkpoint': True,\n",
    "    \n",
    "    # ===== OPTUNA =====\n",
    "    'use_optuna': False,\n",
    "    'optuna_trials': 50,\n",
    "    \n",
    "    # ===== MEMORY =====\n",
    "    'min_disk_space_gb': 2,\n",
    "    \n",
    "    # ===== EXPERIMENT =====\n",
    "    'experiment_name': 'stage2_experiment',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SETUP\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install -q optuna psutil\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, OneCycleLR\n",
    "import optuna\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from datetime import datetime\n",
    "import psutil\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = CONFIG['base_dir']\n",
    "EMBEDDINGS_DIR = os.path.join(BASE_DIR, CONFIG['embeddings_dir'])\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, CONFIG['output_dir'])\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, CONFIG['checkpoint_dir'])\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RESOURCE MONITOR\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class ResourceMonitor:\n",
    "    def __init__(self, min_disk_gb=2):\n",
    "        self.min_disk_gb = min_disk_gb\n",
    "    \n",
    "    def get_gpu_memory(self):\n",
    "        if torch.cuda.is_available():\n",
    "            return {\n",
    "                'allocated': torch.cuda.memory_allocated() / 1e9,\n",
    "                'total': torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            }\n",
    "        return {'allocated': 0, 'total': 0}\n",
    "    \n",
    "    def get_ram_usage(self):\n",
    "        mem = psutil.virtual_memory()\n",
    "        return {'used': mem.used / 1e9, 'total': mem.total / 1e9, 'percent': mem.percent}\n",
    "    \n",
    "    def get_disk_space(self, path='/content/drive'):\n",
    "        try:\n",
    "            usage = shutil.disk_usage(path)\n",
    "            return {'free': usage.free / 1e9, 'total': usage.total / 1e9}\n",
    "        except:\n",
    "            return {'free': 999, 'total': 999}\n",
    "    \n",
    "    def print_status(self, prefix=\"\"):\n",
    "        gpu = self.get_gpu_memory()\n",
    "        ram = self.get_ram_usage()\n",
    "        disk = self.get_disk_space()\n",
    "        print(f\"{prefix}GPU: {gpu['allocated']:.1f}/{gpu['total']:.1f}GB | \"\n",
    "              f\"RAM: {ram['used']:.1f}/{ram['total']:.1f}GB ({ram['percent']:.0f}%) | \"\n",
    "              f\"Disk: {disk['free']:.1f}GB free\")\n",
    "        if disk['free'] < self.min_disk_gb:\n",
    "            print(f\"âš ï¸ WARNING: Low disk space!\")\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "monitor = ResourceMonitor(CONFIG['min_disk_space_gb'])\n",
    "monitor.print_status(\"Initial: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# CHECKPOINT MANAGER\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nclass CheckpointManager:\n    def __init__(self, checkpoint_dir, experiment_name):\n        self.checkpoint_dir = checkpoint_dir\n        self.experiment_name = experiment_name\n    \n    def get_checkpoint_path(self):\n        return os.path.join(self.checkpoint_dir, f\"{self.experiment_name}_training.pth\")\n    \n    def get_best_model_path(self):\n        return os.path.join(self.checkpoint_dir, f\"{self.experiment_name}_best.pth\")\n    \n    def save_checkpoint(self, epoch, model, optimizer, scheduler, history, best_metrics, config):\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n            'history': history,\n            'best_metrics': best_metrics,\n            'config': config,\n            'timestamp': datetime.now().isoformat(),\n        }\n        torch.save(checkpoint, self.get_checkpoint_path())\n        print(f\"ğŸ’¾ Checkpoint saved: epoch {epoch+1}\")\n    \n    def save_best_model(self, model, config, metrics, ab_dim, ag_dim):\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'config': config,\n            'metrics': metrics,\n            'ab_dim': ab_dim,\n            'ag_dim': ag_dim,\n            'timestamp': datetime.now().isoformat(),\n        }, self.get_best_model_path())\n        print(f\"â­ Best model saved: RÂ²={metrics['r2']:.4f}\")\n    \n    def load_checkpoint(self, model, optimizer, scheduler=None):\n        path = self.get_checkpoint_path()\n        if os.path.exists(path):\n            # PyTorch 2.6+ requires weights_only=False for checkpoints with numpy/custom objects\n            checkpoint = torch.load(path, weights_only=False)\n            model.load_state_dict(checkpoint['model_state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            if scheduler and checkpoint['scheduler_state_dict']:\n                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n            print(f\"âœ… Resumed from epoch {checkpoint['epoch']+1}\")\n            return checkpoint\n        return None\n    \n    def clear(self):\n        for path in [self.get_checkpoint_path(), self.get_best_model_path()]:\n            if os.path.exists(path):\n                os.remove(path)\n                print(f\"ğŸ—‘ï¸ Removed: {os.path.basename(path)}\")\n\nckpt_manager = CheckpointManager(CHECKPOINT_DIR, CONFIG['experiment_name'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        data = np.load(path)\n",
    "        self.ab_emb = torch.tensor(data['antibody_embeddings'], dtype=torch.float32)\n",
    "        self.ag_emb = torch.tensor(data['antigen_embeddings'], dtype=torch.float32)\n",
    "        self.labels = torch.tensor(data['labels'], dtype=torch.float32)\n",
    "        self.class_labels = (self.labels > 7.0).long()\n",
    "        print(f\"  Loaded {len(self):,} samples | Ab dim: {self.ab_emb.shape[-1]} | Ag dim: {self.ag_emb.shape[-1]}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'antibody_emb': self.ab_emb[idx],\n",
    "            'antigen_emb': self.ag_emb[idx],\n",
    "            'label': self.labels[idx],\n",
    "            'class_label': self.class_labels[idx]\n",
    "        }\n",
    "\n",
    "print(\"Loading embeddings...\")\n",
    "print(\"Train:\")\n",
    "train_dataset = EmbeddingDataset(os.path.join(EMBEDDINGS_DIR, CONFIG['train_embeddings']))\n",
    "print(\"Val:\")\n",
    "val_dataset = EmbeddingDataset(os.path.join(EMBEDDINGS_DIR, CONFIG['val_embeddings']))\n",
    "print(\"Test:\")\n",
    "test_dataset = EmbeddingDataset(os.path.join(EMBEDDINGS_DIR, CONFIG['test_embeddings']))\n",
    "\n",
    "ab_dim = train_dataset.ab_emb.shape[-1]\n",
    "ag_dim = train_dataset.ag_emb.shape[-1]\n",
    "print(f\"\\nDimensions: Ab={ab_dim}, Ag={ag_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FUSION MODULES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class ConcatFusion(nn.Module):\n",
    "    def __init__(self, ab_dim, ag_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(ab_dim + ag_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.output_dim = hidden_dim\n",
    "    \n",
    "    def forward(self, ab, ag):\n",
    "        return self.proj(torch.cat([ab, ag], dim=-1))\n",
    "\n",
    "class BilinearFusion(nn.Module):\n",
    "    def __init__(self, ab_dim, ag_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ab_proj = nn.Linear(ab_dim, hidden_dim)\n",
    "        self.ag_proj = nn.Linear(ag_dim, hidden_dim)\n",
    "        self.bilinear = nn.Bilinear(hidden_dim, hidden_dim, hidden_dim)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_dim = hidden_dim\n",
    "    \n",
    "    def forward(self, ab, ag):\n",
    "        return self.dropout(F.gelu(self.norm(self.bilinear(self.ab_proj(ab), self.ag_proj(ag)))))\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, ab_dim, ag_dim, hidden_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ab_proj = nn.Linear(ab_dim, hidden_dim)\n",
    "        self.ag_proj = nn.Linear(ag_dim, hidden_dim)\n",
    "        self.cross_attn_ab = nn.MultiheadAttention(hidden_dim, num_heads, dropout, batch_first=True)\n",
    "        self.cross_attn_ag = nn.MultiheadAttention(hidden_dim, num_heads, dropout, batch_first=True)\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.output_dim = hidden_dim\n",
    "    \n",
    "    def forward(self, ab, ag):\n",
    "        ab_p = self.ab_proj(ab).unsqueeze(1)\n",
    "        ag_p = self.ag_proj(ag).unsqueeze(1)\n",
    "        ab_att, _ = self.cross_attn_ab(ab_p, ag_p, ag_p)\n",
    "        ag_att, _ = self.cross_attn_ag(ag_p, ab_p, ab_p)\n",
    "        return self.combine(torch.cat([ab_att.squeeze(1), ag_att.squeeze(1)], dim=-1))\n",
    "\n",
    "class GatedFusion(nn.Module):\n",
    "    def __init__(self, ab_dim, ag_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ab_proj = nn.Linear(ab_dim, hidden_dim)\n",
    "        self.ag_proj = nn.Linear(ag_dim, hidden_dim)\n",
    "        self.gate = nn.Sequential(nn.Linear(hidden_dim * 2, hidden_dim), nn.Sigmoid())\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_dim = hidden_dim\n",
    "    \n",
    "    def forward(self, ab, ag):\n",
    "        ab_p, ag_p = self.ab_proj(ab), self.ag_proj(ag)\n",
    "        g = self.gate(torch.cat([ab_p, ag_p], dim=-1))\n",
    "        return self.dropout(F.gelu(self.norm(g * ab_p + (1 - g) * ag_p)))\n",
    "\n",
    "class TensorFusion(nn.Module):\n",
    "    def __init__(self, ab_dim, ag_dim, hidden_dim, rank=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ab_factor = nn.Linear(ab_dim, rank)\n",
    "        self.ag_factor = nn.Linear(ag_dim, rank)\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(rank, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.output_dim = hidden_dim\n",
    "    \n",
    "    def forward(self, ab, ag):\n",
    "        return self.output_proj(self.ab_factor(ab) * self.ag_factor(ag))\n",
    "\n",
    "def get_fusion(fusion_type, ab_dim, ag_dim, config):\n",
    "    h = config.get('fusion_hidden_dim', 512)\n",
    "    d = config.get('fusion_dropout', 0.1)\n",
    "    n = config.get('fusion_num_heads', 8)\n",
    "    \n",
    "    if fusion_type == 'concat': return ConcatFusion(ab_dim, ag_dim, h, d)\n",
    "    elif fusion_type == 'bilinear': return BilinearFusion(ab_dim, ag_dim, h, d)\n",
    "    elif fusion_type == 'cross_attention': return CrossAttentionFusion(ab_dim, ag_dim, h, n, d)\n",
    "    elif fusion_type == 'gated': return GatedFusion(ab_dim, ag_dim, h, d)\n",
    "    elif fusion_type == 'tensor': return TensorFusion(ab_dim, ag_dim, h, dropout=d)\n",
    "    else: raise ValueError(f\"Unknown fusion: {fusion_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# HEAD MODULES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout=0.2, use_bn=False):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.extend([nn.Linear(prev, h), nn.BatchNorm1d(h) if use_bn else nn.Identity(), nn.GELU(), nn.Dropout(dropout)])\n",
    "            prev = h\n",
    "        layers.append(nn.Linear(prev, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "class ResidualMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout=0.2, use_bn=True):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.BatchNorm1d(hidden_dims[0]) if use_bn else nn.Identity(),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.blocks = nn.ModuleList()\n",
    "        prev = hidden_dims[0]\n",
    "        for h in hidden_dims[1:]:\n",
    "            self.blocks.append(nn.Sequential(\n",
    "                nn.Linear(prev, h),\n",
    "                nn.BatchNorm1d(h) if use_bn else nn.Identity(),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ))\n",
    "            prev = h\n",
    "        self.output = nn.Linear(prev, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.output(x).squeeze(-1)\n",
    "\n",
    "class MultiTaskHead(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout=0.2, use_bn=True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        for h in hidden_dims[:-1]:\n",
    "            layers.extend([nn.Linear(prev, h), nn.BatchNorm1d(h) if use_bn else nn.Identity(), nn.GELU(), nn.Dropout(dropout)])\n",
    "            prev = h\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "        self.reg_head = nn.Sequential(nn.Linear(prev, hidden_dims[-1]), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden_dims[-1], 1))\n",
    "        self.cls_head = nn.Sequential(nn.Linear(prev, hidden_dims[-1]), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden_dims[-1], 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        s = self.shared(x)\n",
    "        return self.reg_head(s).squeeze(-1), self.cls_head(s).squeeze(-1)\n",
    "\n",
    "def get_head(head_type, input_dim, config):\n",
    "    h = config.get('head_hidden_dims', [512, 256, 128])\n",
    "    d = config.get('head_dropout', 0.2)\n",
    "    bn = config.get('use_batch_norm', True)\n",
    "    \n",
    "    if head_type == 'simple_mlp': return SimpleMLP(input_dim, h, d, bn)\n",
    "    elif head_type == 'residual_mlp': return ResidualMLP(input_dim, h, d, bn)\n",
    "    elif head_type == 'deep_mlp': return SimpleMLP(input_dim, h + h, d, bn)\n",
    "    elif head_type == 'multi_task': return MultiTaskHead(input_dim, h, d, bn)\n",
    "    else: raise ValueError(f\"Unknown head: {head_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# COMPLETE MODEL\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class AffinityPredictor(nn.Module):\n",
    "    def __init__(self, ab_dim, ag_dim, config):\n",
    "        super().__init__()\n",
    "        self.fusion = get_fusion(config['fusion_method'], ab_dim, ag_dim, config)\n",
    "        self.head = get_head(config['head_type'], self.fusion.output_dim, config)\n",
    "        self.is_multi_task = config['head_type'] == 'multi_task'\n",
    "    \n",
    "    def forward(self, ab, ag):\n",
    "        return self.head(self.fusion(ab, ag))\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Create model\n",
    "model = AffinityPredictor(ab_dim, ag_dim, CONFIG).to(device)\n",
    "print(f\"\\nModel: {CONFIG['fusion_method']} + {CONFIG['head_type']}\")\n",
    "print(f\"Parameters: {count_params(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training with Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# TRAINING FUNCTIONS\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef get_loss_fn(config):\n    t = config.get('loss_type', 'mse')\n    if t == 'mse': return nn.MSELoss()\n    elif t == 'huber': return nn.HuberLoss(delta=config.get('huber_delta', 1.0))\n    else: \n        mse, mae = nn.MSELoss(), nn.L1Loss()\n        return lambda p, t: 0.5 * mse(p, t) + 0.5 * mae(p, t)\n\n@torch.no_grad()\ndef evaluate(model, loader, loss_fn, config, device):\n    model.eval()\n    total_loss, preds, labels = 0, [], []\n    is_mt = config['head_type'] == 'multi_task'\n    bce = nn.BCEWithLogitsLoss()\n    \n    for batch in loader:\n        ab, ag = batch['antibody_emb'].to(device), batch['antigen_emb'].to(device)\n        y, yc = batch['label'].to(device), batch['class_label'].float().to(device)\n        \n        out = model(ab, ag)\n        if is_mt:\n            reg, cls = out\n            loss = 0.7 * loss_fn(reg, y) + 0.3 * bce(cls, yc)\n            preds.extend(reg.cpu().numpy())\n        else:\n            loss = loss_fn(out, y)\n            preds.extend(out.cpu().numpy())\n        \n        labels.extend(y.cpu().numpy())\n        total_loss += loss.item()\n    \n    preds, labels = np.array(preds), np.array(labels)\n    return {\n        'loss': total_loss / len(loader),\n        'r2': r2_score(labels, preds),\n        'mae': mean_absolute_error(labels, preds),\n        'rmse': np.sqrt(np.mean((preds - labels) ** 2))\n    }\n\ndef train_with_checkpoints(model, train_loader, val_loader, config, device, ckpt_manager):\n    \"\"\"Training loop with checkpointing and resumption.\"\"\"\n    optimizer = AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n    \n    # Scheduler\n    if config['scheduler'] == 'cosine':\n        scheduler = CosineAnnealingLR(optimizer, T_max=config['num_epochs'])\n    elif config['scheduler'] == 'reduce_on_plateau':\n        scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n    else:\n        scheduler = None\n    \n    loss_fn = get_loss_fn(config)\n    is_mt = config['head_type'] == 'multi_task'\n    bce = nn.BCEWithLogitsLoss()\n    \n    # Initialize\n    history = {'train_loss': [], 'val_loss': [], 'val_r2': [], 'val_mae': []}\n    best_val_loss = float('inf')\n    best_metrics = None\n    patience_counter = 0\n    start_epoch = 0\n    \n    # Resume if exists\n    if config['resume_from_checkpoint']:\n        ckpt = ckpt_manager.load_checkpoint(model, optimizer, scheduler)\n        if ckpt:\n            start_epoch = ckpt['epoch'] + 1\n            history = ckpt['history']\n            best_metrics = ckpt['best_metrics']\n            if best_metrics:\n                best_val_loss = best_metrics.get('loss', float('inf'))\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"TRAINING: {config['num_epochs']} epochs (starting from {start_epoch+1})\")\n    print(f\"{'='*60}\")\n    \n    for epoch in range(start_epoch, config['num_epochs']):\n        model.train()\n        train_loss = 0\n        \n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']}\")\n        for batch in pbar:\n            ab, ag = batch['antibody_emb'].to(device), batch['antigen_emb'].to(device)\n            y, yc = batch['label'].to(device), batch['class_label'].float().to(device)\n            \n            optimizer.zero_grad()\n            out = model(ab, ag)\n            \n            if is_mt:\n                reg, cls = out\n                loss = 0.7 * loss_fn(reg, y) + 0.3 * bce(cls, yc)\n            else:\n                loss = loss_fn(out, y)\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            train_loss += loss.item()\n            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        train_loss /= len(train_loader)\n        \n        # Validation\n        val_metrics = evaluate(model, val_loader, loss_fn, config, device)\n        \n        # Update scheduler\n        if scheduler:\n            if config['scheduler'] == 'reduce_on_plateau':\n                scheduler.step(val_metrics['loss'])\n            else:\n                scheduler.step()\n        \n        # Record history\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_metrics['loss'])\n        history['val_r2'].append(val_metrics['r2'])\n        history['val_mae'].append(val_metrics['mae'])\n        \n        # Print epoch summary\n        print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_metrics['loss']:.4f} | \"\n              f\"RÂ²: {val_metrics['r2']:.4f} | MAE: {val_metrics['mae']:.4f}\")\n        \n        # Check for improvement\n        if val_metrics['loss'] < best_val_loss:\n            best_val_loss = val_metrics['loss']\n            best_metrics = val_metrics\n            patience_counter = 0\n            ckpt_manager.save_best_model(model, config, val_metrics, ab_dim, ag_dim)\n        else:\n            patience_counter += 1\n        \n        # Save checkpoint every N epochs\n        if (epoch + 1) % config['checkpoint_every_n_epochs'] == 0:\n            ckpt_manager.save_checkpoint(epoch, model, optimizer, scheduler, history, best_metrics, config)\n        \n        # Early stopping\n        if patience_counter >= config['early_stopping_patience']:\n            print(f\"\\nâ¹ï¸ Early stopping at epoch {epoch+1}\")\n            break\n        \n        monitor.clear_memory()\n    \n    # Load best model (PyTorch 2.6+ requires weights_only=False)\n    best_path = ckpt_manager.get_best_model_path()\n    if os.path.exists(best_path):\n        model.load_state_dict(torch.load(best_path, weights_only=False)['model_state_dict'])\n        print(f\"\\nâœ… Loaded best model (RÂ²: {best_metrics['r2']:.4f})\")\n    \n    return model, history, best_metrics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RUN TRAINING (without Optuna)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "if not CONFIG['use_optuna']:\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    # Train\n",
    "    model = AffinityPredictor(ab_dim, ag_dim, CONFIG).to(device)\n",
    "    model, history, best_metrics = train_with_checkpoints(\n",
    "        model, train_loader, val_loader, CONFIG, device, ckpt_manager\n",
    "    )\n",
    "    \n",
    "    # Test evaluation\n",
    "    loss_fn = get_loss_fn(CONFIG)\n",
    "    test_metrics = evaluate(model, test_loader, loss_fn, CONFIG, device)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Validation: RÂ²={best_metrics['r2']:.4f}, MAE={best_metrics['mae']:.4f}, RMSE={best_metrics['rmse']:.4f}\")\n",
    "    print(f\"Test:       RÂ²={test_metrics['r2']:.4f}, MAE={test_metrics['mae']:.4f}, RMSE={test_metrics['rmse']:.4f}\")\n",
    "    monitor.print_status(\"\\nFinal resources: \")\n",
    "else:\n",
    "    print(\"Optuna enabled. Run next cells for hyperparameter optimization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optuna Optimization (with checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# OPTUNA WITH CHECKPOINT\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def optuna_objective(trial, train_dataset, val_dataset, ab_dim, ag_dim, device):\n",
    "    config = {\n",
    "        'fusion_method': trial.suggest_categorical('fusion_method', ['concat', 'bilinear', 'cross_attention', 'gated', 'tensor']),\n",
    "        'fusion_hidden_dim': trial.suggest_categorical('fusion_hidden_dim', [256, 512, 768]),\n",
    "        'fusion_num_heads': trial.suggest_categorical('fusion_num_heads', [4, 8]),\n",
    "        'fusion_dropout': trial.suggest_float('fusion_dropout', 0.05, 0.3),\n",
    "        'head_type': trial.suggest_categorical('head_type', ['simple_mlp', 'residual_mlp', 'multi_task']),\n",
    "        'head_hidden_dims': trial.suggest_categorical('head_hidden_dims', [[512, 256, 128], [512, 256], [256, 128]]),\n",
    "        'head_dropout': trial.suggest_float('head_dropout', 0.1, 0.4),\n",
    "        'use_batch_norm': trial.suggest_categorical('use_batch_norm', [True, False]),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [128, 256, 512]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),\n",
    "        'scheduler': trial.suggest_categorical('scheduler', ['cosine', 'reduce_on_plateau']),\n",
    "        'loss_type': trial.suggest_categorical('loss_type', ['mse', 'huber']),\n",
    "        'huber_delta': 1.0,\n",
    "        'num_epochs': 30,\n",
    "        'early_stopping_patience': 8,\n",
    "        'checkpoint_every_n_epochs': 100,  # Don't checkpoint during trials\n",
    "        'resume_from_checkpoint': False,\n",
    "    }\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    model = AffinityPredictor(ab_dim, ag_dim, config).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config['num_epochs']) if config['scheduler'] == 'cosine' else None\n",
    "    loss_fn = get_loss_fn(config)\n",
    "    is_mt = config['head_type'] == 'multi_task'\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    best_r2 = -float('inf')\n",
    "    patience = 0\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            ab, ag = batch['antibody_emb'].to(device), batch['antigen_emb'].to(device)\n",
    "            y, yc = batch['label'].to(device), batch['class_label'].float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = model(ab, ag)\n",
    "            loss = 0.7 * loss_fn(out[0], y) + 0.3 * bce(out[1], yc) if is_mt else loss_fn(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if scheduler: scheduler.step()\n",
    "        \n",
    "        val_metrics = evaluate(model, val_loader, loss_fn, config, device)\n",
    "        trial.report(val_metrics['r2'], epoch)\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "        \n",
    "        if val_metrics['r2'] > best_r2:\n",
    "            best_r2 = val_metrics['r2']\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= config['early_stopping_patience']:\n",
    "                break\n",
    "        \n",
    "        monitor.clear_memory()\n",
    "    \n",
    "    return best_r2\n",
    "\n",
    "if CONFIG['use_optuna']:\n",
    "    print(f\"Starting Optuna: {CONFIG['optuna_trials']} trials\")\n",
    "    \n",
    "    # Create/load study\n",
    "    study_path = os.path.join(CHECKPOINT_DIR, f\"{CONFIG['experiment_name']}_optuna.pkl\")\n",
    "    if os.path.exists(study_path) and CONFIG['resume_from_checkpoint']:\n",
    "        with open(study_path, 'rb') as f:\n",
    "            study = pickle.load(f)\n",
    "        print(f\"âœ… Resumed study with {len(study.trials)} completed trials\")\n",
    "    else:\n",
    "        study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "    \n",
    "    # Run optimization\n",
    "    remaining_trials = CONFIG['optuna_trials'] - len(study.trials)\n",
    "    if remaining_trials > 0:\n",
    "        study.optimize(\n",
    "            lambda trial: optuna_objective(trial, train_dataset, val_dataset, ab_dim, ag_dim, device),\n",
    "            n_trials=remaining_trials,\n",
    "            show_progress_bar=True,\n",
    "            callbacks=[lambda study, trial: pickle.dump(study, open(study_path, 'wb'))]  # Save after each trial\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nBest RÂ²: {study.best_trial.value:.4f}\")\n",
    "    print(\"Best params:\")\n",
    "    for k, v in study.best_trial.params.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# TRAIN FINAL MODEL WITH BEST PARAMS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "if CONFIG['use_optuna']:\n",
    "    final_config = {\n",
    "        **study.best_trial.params,\n",
    "        'huber_delta': 1.0,\n",
    "        'num_epochs': 100,\n",
    "        'early_stopping_patience': 15,\n",
    "        'checkpoint_every_n_epochs': 1,\n",
    "        'resume_from_checkpoint': True,\n",
    "    }\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=final_config['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=final_config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=final_config['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    final_ckpt = CheckpointManager(CHECKPOINT_DIR, f\"{CONFIG['experiment_name']}_final\")\n",
    "    final_model = AffinityPredictor(ab_dim, ag_dim, final_config).to(device)\n",
    "    \n",
    "    final_model, history, best_metrics = train_with_checkpoints(\n",
    "        final_model, train_loader, val_loader, final_config, device, final_ckpt\n",
    "    )\n",
    "    \n",
    "    loss_fn = get_loss_fn(final_config)\n",
    "    test_metrics = evaluate(final_model, test_loader, loss_fn, final_config, device)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL MODEL RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Validation: RÂ²={best_metrics['r2']:.4f}, MAE={best_metrics['mae']:.4f}\")\n",
    "    print(f\"Test:       RÂ²={test_metrics['r2']:.4f}, MAE={test_metrics['mae']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save & Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SAVE FINAL MODEL\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_path = os.path.join(OUTPUT_DIR, f\"{CONFIG['experiment_name']}_{timestamp}.pth\")\n",
    "\n",
    "model_to_save = final_model if CONFIG['use_optuna'] else model\n",
    "config_to_save = final_config if CONFIG['use_optuna'] else CONFIG\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model_to_save.state_dict(),\n",
    "    'config': config_to_save,\n",
    "    'ab_dim': ab_dim,\n",
    "    'ag_dim': ag_dim,\n",
    "    'val_metrics': best_metrics,\n",
    "    'test_metrics': test_metrics,\n",
    "    'history': history,\n",
    "}, final_path)\n",
    "\n",
    "print(f\"âœ… Final model saved: {final_path}\")\n",
    "print(f\"   RÂ² (test): {test_metrics['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# VISUALIZATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['val_r2'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('RÂ²')\n",
    "axes[1].set_title(f'Validation RÂ² (best: {max(history[\"val_r2\"]):.4f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(history['val_mae'])\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('MAE')\n",
    "axes[2].set_title(f'Validation MAE (best: {min(history[\"val_mae\"]):.4f})')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, f\"{CONFIG['experiment_name']}_{timestamp}_curves.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š Training curves saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup checkpoints after successful completion\n",
    "cleanup = input(\"Clear training checkpoints? (y/n): \").lower() == 'y'\n",
    "if cleanup:\n",
    "    ckpt_manager.clear()\n",
    "    if CONFIG['use_optuna']:\n",
    "        final_ckpt.clear()\n",
    "    print(\"âœ… Checkpoints cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Checkpoint Features:\n",
    "- **Auto-save**: Best model saved whenever validation improves\n",
    "- **Epoch checkpoints**: Full training state saved every epoch\n",
    "- **Resume training**: If Colab disconnects, re-run cells to continue\n",
    "- **Optuna checkpoint**: Study saves after each trial\n",
    "\n",
    "### Files saved to Google Drive:\n",
    "```\n",
    "AbAg_Project/\n",
    "â”œâ”€â”€ checkpoints/\n",
    "â”‚   â”œâ”€â”€ stage2_experiment_training.pth   # Training checkpoint\n",
    "â”‚   â”œâ”€â”€ stage2_experiment_best.pth       # Best model\n",
    "â”‚   â””â”€â”€ stage2_experiment_optuna.pkl     # Optuna study\n",
    "â””â”€â”€ models/\n",
    "    â””â”€â”€ stage2_experiment_YYYYMMDD.pth   # Final model\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}