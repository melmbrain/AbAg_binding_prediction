{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Antibody-Antigen Binding Prediction - Optuna + ProtT5 v2.8 (OPTIMIZED)\n\n**IgT5 + ProtT5-XL with Maximum Speed Optimizations**\n\n## Speed Optimizations Applied:\n1. **Half-Precision Encoder** (`prot_t5_xl_half_uniref50-enc`) - 50% faster, uses only 8GB VRAM!\n2. **SDPA (Scaled Dot-Product Attention)** - Native PyTorch optimized attention\n3. **Gradient Checkpointing** - Trade 20% speed for 60% memory savings (optional)\n4. **torch.compile** - JIT compilation for faster execution (A100 optimized)\n5. **BetterTransformer** - Fused kernels via optimum library (optional)\n6. **bfloat16 Mixed Precision** - A100 native precision\n\n## Architecture (v2.8):\n- **Antibody encoder**: IgT5 (1024-dim)\n- **Antigen encoder**: ProtT5-XL-Half (1024-dim) - OPTIMIZED!\n- **Cross-attention fusion**: Bidirectional attention\n- **Multi-task output**: Regression (pKd) + Classification\n- **Loss**: MSE (0.7) + BCE (0.3)\n\n## Expected Speed on A100:\n| Optimization | Speed | Memory |\n|--------------|-------|--------|\n| Baseline ProtT5 | 1.0x | 40GB |\n| + Half-Precision | 1.5x | 20GB |\n| + SDPA | 1.8x | 18GB |\n| + torch.compile | 2.2x | 18GB |\n\n## Estimated Training Time (A100 40GB):\n- Optuna tuning (15 trials): ~20-30 min\n- Final training (50 epochs): ~3-5 hours"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "work_dir = '/content/drive/MyDrive/AbAg_Training'\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "%cd {work_dir}\n",
    "\n",
    "dataset_file = 'ab_ag_affinity_complete.csv'\n",
    "if os.path.exists(dataset_file):\n",
    "    print(f\"Dataset found: {dataset_file}\")\n",
    "    !wc -l {dataset_file}\n",
    "else:\n",
    "    print(f\"ERROR: Upload '{dataset_file}' to 'AbAg_Training' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies with optimization libraries\n!pip install -q transformers>=4.41.0 pandas scipy scikit-learn tqdm sentencepiece optuna\n!pip install -q optimum accelerate  # BetterTransformer + acceleration\n\nprint(\"=\"*60)\nprint(\"Dependencies installed!\")\nprint(\"=\"*60)\nprint(\"  transformers: Model loading + SDPA support\")\nprint(\"  optimum: BetterTransformer optimization\")\nprint(\"  accelerate: Mixed precision + device placement\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Detection & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GPU DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total,memory.free', \n",
    "                                '--format=csv,nounits,noheader'],\n",
    "                               capture_output=True, text=True, timeout=10)\n",
    "        parts = result.stdout.strip().split(',')\n",
    "        vram_total = int(parts[0].strip()) / 1024\n",
    "        vram_free = int(parts[1].strip()) / 1024\n",
    "        print(f\"VRAM: {vram_total:.1f} GB total, {vram_free:.1f} GB free\")\n",
    "    except:\n",
    "        vram_free = 40\n",
    "    \n",
    "    is_a100 = 'A100' in gpu_name\n",
    "    if is_a100:\n",
    "        print(\"\\nA100 DETECTED - Using optimized settings!\")\n",
    "        BATCH_OPTIONS = [24, 32, 48]  # A100 can handle larger batches\n",
    "        USE_QUANTIZATION = False\n",
    "    elif vram_free >= 20:\n",
    "        print(\"\\nHigh VRAM GPU detected\")\n",
    "        BATCH_OPTIONS = [16, 24, 32]\n",
    "        USE_QUANTIZATION = False\n",
    "    else:\n",
    "        print(\"\\nStandard GPU detected\")\n",
    "        BATCH_OPTIONS = [8, 12, 16]\n",
    "        USE_QUANTIZATION = True\n",
    "else:\n",
    "    print(\"No GPU - using CPU\")\n",
    "    BATCH_OPTIONS = [4, 8]\n",
    "    USE_QUANTIZATION = False\n",
    "\n",
    "print(f\"\\nBatch size options: {BATCH_OPTIONS}\")\n",
    "print(f\"Use quantization: {USE_QUANTIZATION}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === NARROWED CONFIGURATION (Research-Based for ProtT5 v2.8) ===\n#\n# These ranges are based on:\n# - Multi-task Bioassay Pre-training (MBP) 2024: lr=1e-3, dropout=0.1\n# - ProtT5 fine-tuning papers: lower dropout than ESM-2\n# - Your working v2.7 config (lr=1e-3, dropout=0.1, weight_decay=1e-5)\n#\n# Key differences from ESM-2:\n# - ProtT5 works better with LOWER dropout (0.1 vs 0.3)\n# - Same T5 architecture for both encoders -> more stable training\n\nCONFIG = {\n    'data_path': 'ab_ag_affinity_complete.csv',\n    'output_dir': 'outputs_optuna_v28',\n    \n    # Optuna Settings\n    'n_trials': 15,           # Fewer trials needed with narrow ranges\n    'optuna_epochs': 5,       # Quick evaluation per trial\n    'optuna_patience': 4,     # Stop if no improvement\n    \n    # Final Training\n    'final_epochs': 50,\n    'early_stopping_patience': 15,\n    \n    # ===== NARROWED SEARCH RANGES (ProtT5 optimized) =====\n    # Learning rate: 1e-3 to 5e-3 (MBP 2024 uses 1e-3)\n    'lr_range': (1e-3, 5e-3),\n    \n    # Dropout: 0.10 to 0.30 (ProtT5 works better with less dropout)\n    # MBP 2024 uses 0.1, your v2.7 uses 0.1\n    'dropout_range': (0.10, 0.30),\n    \n    # Weight decay: 1e-5 to 0.01 (tighter range)\n    'weight_decay_range': (1e-5, 0.01),\n    \n    # Batch size: A100 optimized options\n    'batch_size_options': BATCH_OPTIONS,\n    \n    # Cross-attention options\n    'use_cross_attention': True,\n    \n    # Loss weights (MSE + BCE)\n    'mse_weight': 0.7,\n    'class_weight': 0.3,\n}\n\nprint(\"NARROWED Configuration (ProtT5 v2.8):\")\nprint(f\"  Learning rate: {CONFIG['lr_range']} (centered on 1e-3)\")\nprint(f\"  Dropout: {CONFIG['dropout_range']} (lower for ProtT5)\")\nprint(f\"  Weight decay: {CONFIG['weight_decay_range']}\")\nprint(f\"  Batch sizes: {CONFIG['batch_size_options']}\")\nprint(f\"  Trials: {CONFIG['n_trials']}\")\nprint(f\"  Loss: MSE({CONFIG['mse_weight']}) + BCE({CONFIG['class_weight']})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport gc\nfrom transformers import T5EncoderModel, T5Tokenizer\nimport optuna\nfrom optuna.pruners import MedianPruner\nfrom optuna.samplers import TPESampler\nimport json\nimport os\n\n# ============================================================================\n# OPTIMIZATION SETTINGS\n# ============================================================================\nOPTIMIZATION_CONFIG = {\n    # Use half-precision encoder (50% faster, 50% less memory)\n    # Source: https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc\n    'use_half_precision_encoder': True,\n    \n    # Enable SDPA (Scaled Dot-Product Attention) - native PyTorch optimization\n    # Automatically uses FlashAttention-like kernels when available\n    'use_sdpa': True,\n    \n    # Enable gradient checkpointing (trades 20% speed for 60% memory)\n    # Useful if running out of memory\n    'use_gradient_checkpointing': False,\n    \n    # Enable torch.compile (JIT compilation, ~20% speedup)\n    # Set to False if you encounter CUDA graph errors\n    'use_torch_compile': True,\n    \n    # Enable BetterTransformer (fused kernels)\n    # Deprecated in favor of SDPA, but still useful for some models\n    'use_better_transformer': False,\n}\n\nprint(\"=\"*60)\nprint(\"PROTRANS OPTIMIZATION CONFIG\")\nprint(\"=\"*60)\nfor k, v in OPTIMIZATION_CONFIG.items():\n    status = \"ON\" if v else \"OFF\"\n    print(f\"  {k}: {status}\")\nprint(\"=\"*60)\n\n# A100/TF32 optimizations\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cudnn.benchmark = True\ntorch.set_float32_matmul_precision('high')\n\n# Disable torch.compile debug if not using it\nif not OPTIMIZATION_CONFIG['use_torch_compile']:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"\\nDevice: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# ============================================================================\n# CROSS-ATTENTION MODULE\n# ============================================================================\nclass CrossAttention(nn.Module):\n    \"\"\"Cross-attention between antibody and antigen embeddings\"\"\"\n    def __init__(self, dim, num_heads=8, dropout=0.1):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim * 4, dim),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, query, key_value):\n        attn_out, _ = self.attention(query, key_value, key_value)\n        query = self.norm1(query + attn_out)\n        ffn_out = self.ffn(query)\n        query = self.norm2(query + ffn_out)\n        return query\n\n# ============================================================================\n# OPTIMIZED MODEL - IgT5 + ProtT5-XL-Half with Speed Optimizations\n# ============================================================================\nclass IgT5ProtT5Model(nn.Module):\n    \"\"\"\n    OPTIMIZED Dual-encoder model for antibody-antigen binding prediction.\n    \n    Speed Optimizations:\n    1. prot_t5_xl_half_uniref50-enc: Half-precision encoder (50% faster)\n    2. SDPA: Scaled dot-product attention (native PyTorch)\n    3. Gradient checkpointing: Trade speed for memory (optional)\n    4. torch.compile: JIT compilation (A100 optimized)\n    5. BetterTransformer: Fused kernels (optional)\n    \n    Architecture:\n    - Antibody: IgT5 (1024-dim)\n    - Antigen: ProtT5-XL-Half (1024-dim) - OPTIMIZED\n    - Cross-attention + Multi-task output\n    \"\"\"\n    def __init__(self, dropout=0.1, hidden_dims=[512, 256, 128], use_cross_attention=True):\n        super().__init__()\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"BUILDING OPTIMIZED ProtT5 MODEL\")\n        print(\"=\"*60)\n        \n        # Determine which ProtT5 model to use\n        if OPTIMIZATION_CONFIG['use_half_precision_encoder']:\n            ag_model_name = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n            ag_dtype = torch.float16\n            print(\"  Antigen encoder: ProtT5-XL-Half (OPTIMIZED)\")\n            print(\"    - 50% less memory\")\n            print(\"    - 50% faster inference\")\n        else:\n            ag_model_name = \"Rostlab/prot_t5_xl_uniref50\"\n            ag_dtype = torch.float32\n            print(\"  Antigen encoder: ProtT5-XL (full precision)\")\n        \n        # Determine attention implementation\n        attn_impl = \"sdpa\" if OPTIMIZATION_CONFIG['use_sdpa'] else \"eager\"\n        print(f\"  Attention: {attn_impl.upper()}\")\n        \n        # IgT5 for antibody\n        print(\"\\n  Loading IgT5 for antibodies...\")\n        self.ab_tokenizer = T5Tokenizer.from_pretrained(\"Exscientia/IgT5\")\n        self.ab_model = T5EncoderModel.from_pretrained(\n            \"Exscientia/IgT5\",\n            attn_implementation=attn_impl,\n            torch_dtype=torch.float16 if OPTIMIZATION_CONFIG['use_half_precision_encoder'] else torch.float32\n        )\n        self.ab_dim = 1024\n        \n        # ProtT5 for antigen (OPTIMIZED)\n        print(f\"  Loading {ag_model_name}...\")\n        self.ag_tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n        self.ag_model = T5EncoderModel.from_pretrained(\n            ag_model_name,\n            attn_implementation=attn_impl,\n            torch_dtype=ag_dtype\n        )\n        self.ag_dim = 1024\n        \n        # Enable gradient checkpointing if requested\n        if OPTIMIZATION_CONFIG['use_gradient_checkpointing']:\n            self.ab_model.gradient_checkpointing_enable()\n            self.ag_model.gradient_checkpointing_enable()\n            print(\"  Gradient checkpointing: ENABLED (saves 60% memory)\")\n        \n        # Apply BetterTransformer if requested\n        if OPTIMIZATION_CONFIG['use_better_transformer']:\n            try:\n                from optimum.bettertransformer import BetterTransformer\n                self.ab_model = BetterTransformer.transform(self.ab_model)\n                self.ag_model = BetterTransformer.transform(self.ag_model)\n                print(\"  BetterTransformer: ENABLED (fused kernels)\")\n            except Exception as e:\n                print(f\"  BetterTransformer: FAILED ({e})\")\n        \n        # Freeze encoders (only train projection + attention + heads)\n        for p in self.ab_model.parameters(): p.requires_grad = False\n        for p in self.ag_model.parameters(): p.requires_grad = False\n        print(\"  Encoders: FROZEN (only train MLP heads)\")\n        \n        # Projection to common dimension\n        self.common_dim = 512\n        self.ab_proj = nn.Linear(self.ab_dim, self.common_dim)\n        self.ag_proj = nn.Linear(self.ag_dim, self.common_dim)\n        \n        # Cross-attention (optional)\n        self.use_cross_attention = use_cross_attention\n        if use_cross_attention:\n            self.cross_attn_ab = CrossAttention(self.common_dim, num_heads=8, dropout=dropout)\n            self.cross_attn_ag = CrossAttention(self.common_dim, num_heads=8, dropout=dropout)\n            print(\"  Cross-attention: ENABLED\")\n        \n        # Regression head with spectral normalization\n        self.regression_head = nn.Sequential(\n            nn.utils.spectral_norm(nn.Linear(self.common_dim * 2, hidden_dims[0])),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.LayerNorm(hidden_dims[0]),\n            \n            nn.utils.spectral_norm(nn.Linear(hidden_dims[0], hidden_dims[1])),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.LayerNorm(hidden_dims[1]),\n            \n            nn.utils.spectral_norm(nn.Linear(hidden_dims[1], hidden_dims[2])),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.LayerNorm(hidden_dims[2]),\n            \n            nn.Linear(hidden_dims[2], 1)\n        )\n        \n        # Classification head\n        self.classifier = nn.Linear(self.common_dim * 2, 1)\n        \n        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        total = sum(p.numel() for p in self.parameters())\n        print(f\"\\n  Trainable: {trainable/1e6:.1f}M | Total: {total/1e6:.1f}M\")\n        print(\"=\"*60)\n    \n    def forward(self, ab_seqs, ag_seqs, device):\n        # Tokenize antibodies\n        ab_tokens = self.ab_tokenizer(\n            ab_seqs, return_tensors='pt', padding=True,\n            truncation=True, max_length=512\n        ).to(device)\n        \n        # Tokenize antigens - ProtT5 expects SPACE-SEPARATED amino acids\n        ag_seqs_spaced = [\" \".join(list(seq)) for seq in ag_seqs]\n        ag_tokens = self.ag_tokenizer(\n            ag_seqs_spaced, return_tensors='pt', padding=True,\n            truncation=True, max_length=2048\n        ).to(device)\n        \n        # Encode (frozen encoders, no gradient)\n        with torch.no_grad():\n            ab_out = self.ab_model(**ab_tokens).last_hidden_state\n            ag_out = self.ag_model(**ag_tokens).last_hidden_state\n        \n        # Mean pooling\n        ab_emb = ab_out.mean(dim=1)\n        ag_emb = ag_out.mean(dim=1)\n        \n        # Cast to bfloat16 for trainable layers (A100 optimized)\n        ab_emb = ab_emb.to(torch.bfloat16)\n        ag_emb = ag_emb.to(torch.bfloat16)\n        \n        # Project to common dimension\n        ab_proj = self.ab_proj(ab_emb)\n        ag_proj = self.ag_proj(ag_emb)\n        \n        # Cross-attention\n        if self.use_cross_attention:\n            ab_proj = ab_proj.unsqueeze(1)\n            ag_proj = ag_proj.unsqueeze(1)\n            \n            ab_enhanced = self.cross_attn_ab(ab_proj, ag_proj).squeeze(1)\n            ag_enhanced = self.cross_attn_ag(ag_proj, ab_proj).squeeze(1)\n            \n            combined = torch.cat([ab_enhanced, ag_enhanced], dim=1)\n        else:\n            combined = torch.cat([ab_proj, ag_proj], dim=1)\n        \n        # Predictions\n        pKd_pred = self.regression_head(combined).squeeze(-1)\n        class_logits = self.classifier(combined).squeeze(-1)\n        \n        return pKd_pred.float(), class_logits.float()\n\n# ============================================================================\n# STABLE LOSS FUNCTION (MSE + BCE)\n# ============================================================================\nclass StableCombinedLoss(nn.Module):\n    \"\"\"MSE + BCE loss for multi-task learning\"\"\"\n    def __init__(self, mse_weight=0.7, class_weight=0.3):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.bce = nn.BCEWithLogitsLoss()\n        self.mse_weight = mse_weight\n        self.class_weight = class_weight\n    \n    def forward(self, pred, target, class_logits=None):\n        mse_loss = self.mse(pred, target)\n        loss = self.mse_weight * mse_loss\n        \n        if class_logits is not None:\n            class_target = (target >= 9.0).float()\n            class_loss = self.bce(class_logits, class_target)\n            loss += self.class_weight * class_loss\n        \n        return loss\n\n# ============================================================================\n# DATASET\n# ============================================================================\nclass AbAgDataset(Dataset):\n    def __init__(self, df):\n        self.df = df.reset_index(drop=True)\n    def __len__(self): return len(self.df)\n    def __getitem__(self, i):\n        return {'ab': self.df.iloc[i]['antibody_sequence'],\n                'ag': self.df.iloc[i]['antigen_sequence'],\n                'y': torch.tensor(self.df.iloc[i]['pKd'], dtype=torch.float32)}\n\ndef collate(batch):\n    return {'ab': [b['ab'] for b in batch], 'ag': [b['ag'] for b in batch],\n            'y': torch.stack([b['y'] for b in batch])}\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"OPTIMIZED ProtT5 v2.8 CODE LOADED\")\nprint(\"=\"*60)\nprint(\"Speed optimizations:\")\nprint(\"  1. Half-precision encoder (prot_t5_xl_half)\")\nprint(\"  2. SDPA attention (native PyTorch)\")\nprint(\"  3. Gradient checkpointing (optional)\")\nprint(\"  4. torch.compile (JIT, optional)\")\nprint(\"  5. BetterTransformer (fused kernels, optional)\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optuna Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Global model cache (avoid reloading frozen encoders during Optuna trials)\nMODEL_CACHE = None\nCOMPILED_FORWARD = None\n\ndef get_model(dropout, hidden_dims, use_cross_attention, use_cache=True):\n    \"\"\"Get model with optimizations, reusing frozen encoders from cache.\"\"\"\n    global MODEL_CACHE, COMPILED_FORWARD\n    \n    if use_cache and MODEL_CACHE is not None:\n        # Reuse frozen encoders, only rebuild trainable layers\n        m = MODEL_CACHE\n        \n        # Rebuild projection layers\n        m.ab_proj = nn.Linear(m.ab_dim, m.common_dim).to(device).to(torch.bfloat16)\n        m.ag_proj = nn.Linear(m.ag_dim, m.common_dim).to(device).to(torch.bfloat16)\n        \n        # Rebuild cross-attention\n        m.use_cross_attention = use_cross_attention\n        if use_cross_attention:\n            m.cross_attn_ab = CrossAttention(m.common_dim, num_heads=8, dropout=dropout).to(device).to(torch.bfloat16)\n            m.cross_attn_ag = CrossAttention(m.common_dim, num_heads=8, dropout=dropout).to(device).to(torch.bfloat16)\n        \n        # Rebuild regression head\n        m.regression_head = nn.Sequential(\n            nn.utils.spectral_norm(nn.Linear(m.common_dim * 2, hidden_dims[0])),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.LayerNorm(hidden_dims[0]),\n            \n            nn.utils.spectral_norm(nn.Linear(hidden_dims[0], hidden_dims[1])),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.LayerNorm(hidden_dims[1]),\n            \n            nn.utils.spectral_norm(nn.Linear(hidden_dims[1], hidden_dims[2])),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.LayerNorm(hidden_dims[2]),\n            \n            nn.Linear(hidden_dims[2], 1)\n        ).to(device).to(torch.bfloat16)\n        \n        # Rebuild classifier\n        m.classifier = nn.Linear(m.common_dim * 2, 1).to(device).to(torch.bfloat16)\n        \n        return m\n    \n    # First call: create full model\n    m = IgT5ProtT5Model(dropout, hidden_dims, use_cross_attention).to(device)\n    \n    # Cast trainable layers to bfloat16\n    m.ab_proj = m.ab_proj.to(torch.bfloat16)\n    m.ag_proj = m.ag_proj.to(torch.bfloat16)\n    if m.use_cross_attention:\n        m.cross_attn_ab = m.cross_attn_ab.to(torch.bfloat16)\n        m.cross_attn_ag = m.cross_attn_ag.to(torch.bfloat16)\n    m.regression_head = m.regression_head.to(torch.bfloat16)\n    m.classifier = m.classifier.to(torch.bfloat16)\n    \n    MODEL_CACHE = m\n    return m\n\ndef objective(trial, train_df, val_df):\n    \"\"\"Optuna objective with speed optimizations.\"\"\"\n    \n    # ===== NARROWED HYPERPARAMETERS =====\n    lr = trial.suggest_float('lr', CONFIG['lr_range'][0], CONFIG['lr_range'][1], log=True)\n    dropout = trial.suggest_float('dropout', CONFIG['dropout_range'][0], CONFIG['dropout_range'][1])\n    wd = trial.suggest_float('weight_decay', CONFIG['weight_decay_range'][0], CONFIG['weight_decay_range'][1], log=True)\n    bs = trial.suggest_categorical('batch_size', CONFIG['batch_size_options'])\n    \n    # Hidden layer config\n    hidden_cfg = trial.suggest_categorical('hidden', ['medium', 'large'])\n    hidden_map = {\n        'medium': [512, 256, 128],\n        'large': [512, 512, 256]\n    }\n    hidden_dims = hidden_map[hidden_cfg]\n    use_cross_attention = CONFIG['use_cross_attention']\n    \n    print(f\"\\nTrial {trial.number}: lr={lr:.4f}, dropout={dropout:.2f}, wd={wd:.5f}, bs={bs}, hidden={hidden_cfg}\")\n    \n    try:\n        model = get_model(dropout, hidden_dims, use_cross_attention)\n        \n        # Apply torch.compile for speed (A100 optimized)\n        if OPTIMIZATION_CONFIG['use_torch_compile']:\n            try:\n                model = torch.compile(model, mode='reduce-overhead')\n                print(\"  torch.compile: ENABLED\")\n            except Exception as e:\n                print(f\"  torch.compile: FAILED ({e})\")\n        \n        train_loader = DataLoader(AbAgDataset(train_df), batch_size=bs, shuffle=True, \n                                  num_workers=2, pin_memory=True, collate_fn=collate)\n        val_loader = DataLoader(AbAgDataset(val_df), batch_size=bs*2, shuffle=False, \n                                num_workers=2, pin_memory=True, collate_fn=collate)\n        \n        opt = torch.optim.AdamW(\n            [p for p in model.parameters() if p.requires_grad], \n            lr=lr, weight_decay=wd,\n            fused=True  # A100 fused optimizer\n        )\n        loss_fn = StableCombinedLoss(\n            mse_weight=CONFIG['mse_weight'], \n            class_weight=CONFIG['class_weight']\n        )\n        best = -1\n        \n        for epoch in range(CONFIG['optuna_epochs']):\n            model.train()\n            for batch in train_loader:\n                y = batch['y'].to(device)\n                \n                with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n                    pKd_pred, class_logits = model(batch['ab'], batch['ag'], device)\n                    loss = loss_fn(pKd_pred, y, class_logits)\n                \n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                opt.step()\n                opt.zero_grad(set_to_none=True)\n            \n            # Validate\n            model.eval()\n            preds, targs = [], []\n            with torch.no_grad():\n                for batch in val_loader:\n                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n                        pKd_pred, _ = model(batch['ab'], batch['ag'], device)\n                    preds.extend(pKd_pred.float().cpu().numpy())\n                    targs.extend(batch['y'].numpy())\n            \n            spearman = stats.spearmanr(targs, preds)[0]\n            if not np.isnan(spearman):\n                best = max(best, spearman)\n            \n            trial.report(spearman, epoch)\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n        \n        print(f\"  -> Spearman = {best:.4f}\")\n        del train_loader, val_loader, opt\n        gc.collect()\n        torch.cuda.empty_cache()\n        return best\n        \n    except Exception as e:\n        print(f\"  -> Failed: {e}\")\n        gc.collect()\n        torch.cuda.empty_cache()\n        return -1\n\nprint(\"Optuna objective ready with speed optimizations!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"OPTUNA HYPERPARAMETER SEARCH (ProtT5 v2.8)\")\nprint(\"=\"*60)\n\n# Load data\ndf = pd.read_csv(CONFIG['data_path'])\nprint(f\"Total samples: {len(df):,}\")\n\n# Filter invalid pKd values (critical for stable training!)\ndf = df[(df['pKd'] >= 4.0) & (df['pKd'] <= 14.0)].reset_index(drop=True)\nprint(f\"Valid samples (pKd 4-14): {len(df):,}\")\n\n# Subset for tuning (faster Optuna trials)\ntune_df = df.sample(n=min(50000, len(df)), random_state=42)\ntrain_tune, val_tune = train_test_split(tune_df, test_size=0.2, random_state=42)\nprint(f\"Tuning set: {len(train_tune):,} train, {len(val_tune):,} val\")\n\n# Print search space\nprint(f\"\\nSearch Space (NARROWED for ProtT5):\")\nprint(f\"  LR: {CONFIG['lr_range'][0]:.4f} - {CONFIG['lr_range'][1]:.4f}\")\nprint(f\"  Dropout: {CONFIG['dropout_range'][0]:.2f} - {CONFIG['dropout_range'][1]:.2f}\")\nprint(f\"  Weight Decay: {CONFIG['weight_decay_range'][0]:.5f} - {CONFIG['weight_decay_range'][1]:.4f}\")\nprint(f\"  Batch Size: {CONFIG['batch_size_options']}\")\nprint(f\"  Hidden: ['medium', 'large']\")\nprint(f\"  Loss: MSE({CONFIG['mse_weight']}) + BCE({CONFIG['class_weight']})\")\n\n# Early stopping for Optuna\nclass OptunaStop:\n    def __init__(self, patience=4):\n        self.patience = patience\n        self.best = -float('inf')\n        self.count = 0\n    def __call__(self, study, trial):\n        if trial.value and trial.value > self.best:\n            self.best = trial.value\n            self.count = 0\n            print(f\"\\n*** NEW BEST: Spearman = {trial.value:.4f} ***\")\n        else:\n            self.count += 1\n        if self.count >= self.patience:\n            print(f\"\\nStopping: no improvement for {self.patience} trials\")\n            study.stop()\n\n# Run Optuna\nprint(\"\\n\" + \"-\"*60)\nstudy = optuna.create_study(\n    direction='maximize', \n    sampler=TPESampler(seed=42),\n    pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=2)\n)\nstudy.optimize(\n    lambda t: objective(t, train_tune, val_tune), \n    n_trials=CONFIG['n_trials'],\n    callbacks=[OptunaStop(CONFIG['optuna_patience'])], \n    show_progress_bar=True\n)\n\n# Results\nprint(\"\\n\" + \"=\"*60)\nprint(\"BEST HYPERPARAMETERS FOUND\")\nprint(\"=\"*60)\nprint(f\"Best Spearman: {study.best_value:.4f}\")\nfor k, v in study.best_params.items():\n    if isinstance(v, float):\n        print(f\"  {k}: {v:.6f}\")\n    else:\n        print(f\"  {k}: {v}\")\n\nbest_params = study.best_params\nout_dir = Path(CONFIG['output_dir'])\nout_dir.mkdir(exist_ok=True)\nwith open(out_dir / 'best_params.json', 'w') as f:\n    json.dump(best_params, f, indent=2)\nprint(f\"\\nSaved to: {out_dir / 'best_params.json'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Training with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"FINAL TRAINING WITH BEST HYPERPARAMETERS (OPTIMIZED)\")\nprint(\"=\"*60)\n\n# Best params\nlr = best_params['lr']\ndropout = best_params['dropout']\nwd = best_params['weight_decay']\nbs = best_params['batch_size']\nhidden_map = {'medium': [512, 256, 128], 'large': [512, 512, 256]}\nhidden_dims = hidden_map[best_params['hidden']]\nuse_cross_attention = CONFIG['use_cross_attention']\n\nprint(f\"\\nHyperparameters:\")\nprint(f\"  Learning Rate: {lr:.6f}\")\nprint(f\"  Dropout: {dropout:.3f}\")\nprint(f\"  Weight Decay: {wd:.6f}\")\nprint(f\"  Batch Size: {bs}\")\nprint(f\"  Hidden: {hidden_dims}\")\nprint(f\"  Cross-Attention: {use_cross_attention}\")\n\nprint(f\"\\nOptimizations:\")\nfor k, v in OPTIMIZATION_CONFIG.items():\n    status = \"ON\" if v else \"OFF\"\n    print(f\"  {k}: {status}\")\n\n# Full data split\ntrain_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\nval_quick = val_df.sample(frac=0.1, random_state=42)\n\nprint(f\"\\nData Split:\")\nprint(f\"  Train: {len(train_df):,}\")\nprint(f\"  Val: {len(val_df):,}\")\nprint(f\"  Test: {len(test_df):,}\")\n\n# Fresh model with best params (clear cache)\nglobal MODEL_CACHE\nMODEL_CACHE = None\n\nprint(\"\\nLoading OPTIMIZED ProtT5 v2.8 model...\")\nmodel = IgT5ProtT5Model(dropout, hidden_dims, use_cross_attention).to(device)\n\n# Cast trainable layers to bfloat16\nmodel.ab_proj = model.ab_proj.to(torch.bfloat16)\nmodel.ag_proj = model.ag_proj.to(torch.bfloat16)\nif model.use_cross_attention:\n    model.cross_attn_ab = model.cross_attn_ab.to(torch.bfloat16)\n    model.cross_attn_ag = model.cross_attn_ag.to(torch.bfloat16)\nmodel.regression_head = model.regression_head.to(torch.bfloat16)\nmodel.classifier = model.classifier.to(torch.bfloat16)\n\n# Apply torch.compile for maximum speed\nif OPTIMIZATION_CONFIG['use_torch_compile']:\n    print(\"\\nApplying torch.compile (this may take a moment on first run)...\")\n    try:\n        model = torch.compile(model, mode='reduce-overhead')\n        print(\"  torch.compile: SUCCESS - ~20% speedup expected\")\n    except Exception as e:\n        print(f\"  torch.compile: FAILED ({e}) - continuing without JIT\")\n\n# Data loaders with optimized settings\ntrain_loader = DataLoader(\n    AbAgDataset(train_df), batch_size=bs, shuffle=True, \n    num_workers=4, pin_memory=True, collate_fn=collate,\n    prefetch_factor=2, persistent_workers=True\n)\nval_loader_q = DataLoader(\n    AbAgDataset(val_quick), batch_size=bs*2, shuffle=False, \n    num_workers=4, pin_memory=True, collate_fn=collate\n)\nval_loader_f = DataLoader(\n    AbAgDataset(val_df), batch_size=bs*2, shuffle=False, \n    num_workers=4, pin_memory=True, collate_fn=collate\n)\ntest_loader = DataLoader(\n    AbAgDataset(test_df), batch_size=bs*2, shuffle=False, \n    num_workers=4, pin_memory=True, collate_fn=collate\n)\n\n# Optimizer with A100 fused AdamW\nopt = torch.optim.AdamW(\n    [p for p in model.parameters() if p.requires_grad], \n    lr=lr, weight_decay=wd,\n    fused=True  # A100 fused optimizer (~10% speedup)\n)\n\n# Scheduler: ReduceLROnPlateau (MBP 2024)\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nsched = ReduceLROnPlateau(opt, mode='max', factor=0.6, patience=10, min_lr=1e-6)\n\n# Loss\nloss_fn = StableCombinedLoss(mse_weight=CONFIG['mse_weight'], class_weight=CONFIG['class_weight'])\n\n# Early stopping\nclass EarlyStop:\n    def __init__(self, patience):\n        self.patience = patience\n        self.best = -float('inf')\n        self.count = 0\n    def __call__(self, score):\n        if score > self.best:\n            self.best = score\n            self.count = 0\n            return False\n        self.count += 1\n        return self.count >= self.patience\n\nearly = EarlyStop(CONFIG['early_stopping_patience'])\nbest_spearman = -1\n\nprint(f\"\\nTraining for {CONFIG['final_epochs']} epochs...\")\nprint(\"(First epoch may be slower due to torch.compile warmup)\\n\")\n\nimport time\nepoch_times = []\n\nfor epoch in range(CONFIG['final_epochs']):\n    epoch_start = time.time()\n    \n    model.train()\n    total_loss = 0\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['final_epochs']}\")\n    \n    for batch in pbar:\n        y = batch['y'].to(device)\n        \n        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n            pKd_pred, class_logits = model(batch['ab'], batch['ag'], device)\n            loss = loss_fn(pKd_pred, y, class_logits)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opt.step()\n        opt.zero_grad(set_to_none=True)\n        \n        total_loss += loss.item()\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    # Quick validation\n    model.eval()\n    preds, targs = [], []\n    with torch.no_grad():\n        for batch in val_loader_q:\n            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n                pKd_pred, _ = model(batch['ab'], batch['ag'], device)\n            preds.extend(pKd_pred.float().cpu().numpy())\n            targs.extend(batch['y'].numpy())\n    \n    spearman = stats.spearmanr(targs, preds)[0]\n    current_lr = opt.param_groups[0]['lr']\n    epoch_time = time.time() - epoch_start\n    epoch_times.append(epoch_time)\n    \n    print(f\"  Loss: {total_loss/len(train_loader):.4f} | Spearman: {spearman:.4f} | LR: {current_lr:.2e} | Time: {epoch_time:.1f}s\")\n    \n    # Step scheduler\n    sched.step(spearman)\n    \n    # Save best\n    if spearman > best_spearman:\n        best_spearman = spearman\n        torch.save({\n            'epoch': epoch, \n            'model': model.state_dict(), \n            'spearman': spearman, \n            'params': best_params,\n            'config': CONFIG,\n            'optimizations': OPTIMIZATION_CONFIG\n        }, out_dir / 'best_model.pth')\n        print(f\"  *** Saved best model (Spearman: {spearman:.4f}) ***\")\n    \n    # Checkpoint\n    torch.save({\n        'epoch': epoch, \n        'model': model.state_dict(), \n        'opt': opt.state_dict(),\n        'best_spearman': best_spearman\n    }, out_dir / 'checkpoint.pth')\n    \n    if early(spearman):\n        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n        break\n\navg_time = sum(epoch_times[1:]) / len(epoch_times[1:]) if len(epoch_times) > 1 else epoch_times[0]\nprint(f\"\\n\" + \"=\"*60)\nprint(f\"Training complete!\")\nprint(f\"  Best Spearman: {best_spearman:.4f}\")\nprint(f\"  Avg epoch time: {avg_time:.1f}s (excluding warmup)\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load best model\nckpt = torch.load(out_dir / 'best_model.pth', map_location=device)\nmodel.load_state_dict(ckpt['model'])\nprint(f\"Loaded best model from epoch {ckpt['epoch']+1}\")\nprint(f\"Best Spearman during training: {ckpt['spearman']:.4f}\")\n\ndef evaluate(model, loader, name):\n    \"\"\"Evaluate ProtT5 v2.8 model with multi-task output.\"\"\"\n    model.eval()\n    preds, class_preds, targs = [], [], []\n    \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=name):\n            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n                pKd_pred, class_logits = model(batch['ab'], batch['ag'], device)\n            preds.extend(pKd_pred.float().cpu().numpy())\n            class_preds.extend(torch.sigmoid(class_logits).float().cpu().numpy())\n            targs.extend(batch['y'].numpy())\n    \n    t, p = np.array(targs), np.array(preds)\n    c = np.array(class_preds)\n    \n    # Classification metrics at pKd=9\n    strong = t >= 9.0\n    pred_strong = c >= 0.5\n    tp = np.sum(strong & pred_strong)\n    fn = np.sum(strong & ~pred_strong)\n    fp = np.sum(~strong & pred_strong)\n    \n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    \n    return {\n        'spearman': stats.spearmanr(t, p)[0],\n        'pearson': stats.pearsonr(t, p)[0],\n        'rmse': np.sqrt(mean_squared_error(t, p)),\n        'mae': mean_absolute_error(t, p),\n        'r2': r2_score(t, p),\n        'recall': recall * 100,\n        'precision': precision * 100\n    }, p, t\n\n# Validation\nprint(\"\\n\" + \"=\"*60)\nprint(\"VALIDATION SET RESULTS\")\nprint(\"=\"*60)\nval_m, val_p, val_t = evaluate(model, val_loader_f, \"Validation\")\nprint(f\"  Spearman:  {val_m['spearman']:.4f}\")\nprint(f\"  Pearson:   {val_m['pearson']:.4f}\")\nprint(f\"  RMSE:      {val_m['rmse']:.4f}\")\nprint(f\"  MAE:       {val_m['mae']:.4f}\")\nprint(f\"  R2:        {val_m['r2']:.4f}\")\nprint(f\"  Recall:    {val_m['recall']:.1f}%\")\nprint(f\"  Precision: {val_m['precision']:.1f}%\")\n\n# Test\nprint(\"\\n\" + \"=\"*60)\nprint(\"TEST SET RESULTS (UNSEEN DATA)\")\nprint(\"=\"*60)\ntest_m, test_p, test_t = evaluate(model, test_loader, \"Test\")\nprint(f\"  Spearman:  {test_m['spearman']:.4f}\")\nprint(f\"  Pearson:   {test_m['pearson']:.4f}\")\nprint(f\"  RMSE:      {test_m['rmse']:.4f}\")\nprint(f\"  MAE:       {test_m['mae']:.4f}\")\nprint(f\"  R2:        {test_m['r2']:.4f}\")\nprint(f\"  Recall:    {test_m['recall']:.1f}%\")\nprint(f\"  Precision: {test_m['precision']:.1f}%\")\n\n# Save predictions and results\npd.DataFrame({'true_pKd': val_t, 'pred_pKd': val_p}).to_csv(out_dir / 'val_predictions.csv', index=False)\npd.DataFrame({'true_pKd': test_t, 'pred_pKd': test_p}).to_csv(out_dir / 'test_predictions.csv', index=False)\n\nwith open(out_dir / 'final_results.json', 'w') as f:\n    json.dump({\n        'best_params': best_params,\n        'config': {\n            'mse_weight': CONFIG['mse_weight'],\n            'class_weight': CONFIG['class_weight'],\n            'use_cross_attention': CONFIG['use_cross_attention'],\n        },\n        'validation': {k: float(v) for k, v in val_m.items()},\n        'test': {k: float(v) for k, v in test_m.items()},\n        'architecture': 'IgT5 + ProtT5-XL (v2.8)'\n    }, f, indent=2)\n\nprint(f\"\\nAll results saved to: {out_dir}\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"DONE! ProtT5 v2.8 Training Complete\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}