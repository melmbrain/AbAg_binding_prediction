{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Generate & Cache Embeddings (Advanced)\n",
    "\n",
    "**Run this ONCE to pre-compute embeddings, then use Stage 2 for fast experimentation.**\n",
    "\n",
    "## Features:\n",
    "- âœ… **Auto-checkpoint**: Saves progress every N batches (resume if interrupted)\n",
    "- âœ… **Live metrics**: Loss, RÂ², MAE displayed during training\n",
    "- âœ… **Memory monitoring**: GPU/RAM/Disk usage tracked\n",
    "- âœ… **Multiple fine-tuning strategies**: Frozen, LoRA, partial unfreeze\n",
    "\n",
    "## Embedding Strategies:\n",
    "\n",
    "| Strategy | Trainable Params | Speed | Performance |\n",
    "|----------|------------------|-------|-------------|\n",
    "| **A. Frozen** | 0 | Fastest | Baseline |\n",
    "| **B. LoRA (r=4)** | ~3M (0.25%) | 4.5x faster than full | ~95% of full FT |\n",
    "| **C. Unfreeze Last 25%** | ~750M (25%) | 30% faster | = Full FT |\n",
    "| **D. Full Fine-tune** | 3B (100%) | Slowest | Best |\n",
    "\n",
    "## Time Estimate (A100 40GB, ~140k samples):\n",
    "- Frozen: ~30-60 min\n",
    "- LoRA: ~2-3 hours\n",
    "- Partial unfreeze: ~3-4 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# CONFIGURATION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nCONFIG = {\n    # ===== PATHS (MODIFY THESE) =====\n    'base_dir': '/content/drive/MyDrive/AbAg_Project',\n    'data_file': 'data/ab_ag_affinity_complete.csv',\n    'output_dir': 'embeddings',\n    'checkpoint_dir': 'checkpoints',\n    \n    # ===== ENCODER SELECTION =====\n    'antibody_encoder': 'IgT5',           # 'IgT5' or 'AbRoBERTa'\n    'antigen_encoder': 'ProtT5-Half',     # 'ProtT5-Half', 'ProtT5-Full', 'ESM2'\n    \n    # ===== FINE-TUNING STRATEGY =====\n    # Options: 'frozen', 'lora', 'unfreeze_last_25', 'unfreeze_last_50', 'full'\n    'finetune_strategy': 'lora',\n    \n    # ===== LORA SETTINGS =====\n    'lora_r': 4,\n    'lora_alpha': 8,\n    'lora_dropout': 0.1,\n    'lora_target_modules': ['q', 'k', 'v', 'o'],\n    \n    # ===== LAYER EXTRACTION =====\n    'layer_strategy': 'last',  # 'last', 'multi', 'weighted_all'\n    'multi_layers': [-1, -12, -18],\n    'pooling': 'mean',\n    \n    # ===== PROCESSING =====\n    'batch_size': 4,              # Reduced for fine-tuning (2 large models)\n    'gradient_accumulation': 4,   # Effective batch = 4 * 4 = 16\n    'max_ab_length': 256,         # Reduced (antibodies are ~150 AA)\n    'max_ag_length': 512,         # Reduced (saves memory)\n    \n    # ===== FINE-TUNING =====\n    'ft_epochs': 3,\n    'ft_lr': 1e-4,\n    'use_gradient_checkpointing': True,  # Saves memory\n    \n    # ===== CHECKPOINTING =====\n    'checkpoint_every_n_batches': 500,  # Save checkpoint every N batches\n    'checkpoint_every_n_epochs': 1,     # Save checkpoint every N epochs\n    'resume_from_checkpoint': True,     # Auto-resume if checkpoint exists\n    \n    # ===== MEMORY MANAGEMENT =====\n    'clear_cache_every_n_batches': 100,\n    'min_disk_space_gb': 5,             # Warn if disk space below this\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SETUP & MEMORY UTILITIES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Create directories\n",
    "BASE_DIR = CONFIG['base_dir']\n",
    "DATA_PATH = os.path.join(BASE_DIR, CONFIG['data_file'])\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, CONFIG['output_dir'])\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, CONFIG['checkpoint_dir'])\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"Checkpoint dir: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers>=4.41.0 pandas tqdm sentencepiece peft accelerate psutil\n",
    "import psutil\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RESOURCE MONITORING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class ResourceMonitor:\n",
    "    \"\"\"Monitor GPU, RAM, and Disk usage.\"\"\"\n",
    "    \n",
    "    def __init__(self, min_disk_gb=5):\n",
    "        self.min_disk_gb = min_disk_gb\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def get_gpu_memory(self):\n",
    "        \"\"\"Get GPU memory usage in GB.\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            allocated = torch.cuda.memory_allocated() / 1e9\n",
    "            reserved = torch.cuda.memory_reserved() / 1e9\n",
    "            total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            return {'allocated': allocated, 'reserved': reserved, 'total': total}\n",
    "        return {'allocated': 0, 'reserved': 0, 'total': 0}\n",
    "    \n",
    "    def get_ram_usage(self):\n",
    "        \"\"\"Get RAM usage in GB.\"\"\"\n",
    "        mem = psutil.virtual_memory()\n",
    "        return {\n",
    "            'used': mem.used / 1e9,\n",
    "            'total': mem.total / 1e9,\n",
    "            'percent': mem.percent\n",
    "        }\n",
    "    \n",
    "    def get_disk_space(self, path='/content/drive'):\n",
    "        \"\"\"Get disk space in GB.\"\"\"\n",
    "        try:\n",
    "            usage = shutil.disk_usage(path)\n",
    "            return {\n",
    "                'free': usage.free / 1e9,\n",
    "                'total': usage.total / 1e9,\n",
    "                'used': usage.used / 1e9\n",
    "            }\n",
    "        except:\n",
    "            return {'free': 999, 'total': 999, 'used': 0}\n",
    "    \n",
    "    def print_status(self, prefix=\"\"):\n",
    "        \"\"\"Print current resource status.\"\"\"\n",
    "        gpu = self.get_gpu_memory()\n",
    "        ram = self.get_ram_usage()\n",
    "        disk = self.get_disk_space()\n",
    "        \n",
    "        status = (\n",
    "            f\"{prefix}\"\n",
    "            f\"GPU: {gpu['allocated']:.1f}/{gpu['total']:.1f}GB | \"\n",
    "            f\"RAM: {ram['used']:.1f}/{ram['total']:.1f}GB ({ram['percent']:.0f}%) | \"\n",
    "            f\"Disk: {disk['free']:.1f}GB free\"\n",
    "        )\n",
    "        print(status)\n",
    "        \n",
    "        # Warning if low disk space\n",
    "        if disk['free'] < self.min_disk_gb:\n",
    "            print(f\"âš ï¸  WARNING: Low disk space! Only {disk['free']:.1f}GB remaining\")\n",
    "        \n",
    "        return gpu, ram, disk\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear GPU and RAM caches.\"\"\"\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = ResourceMonitor(min_disk_gb=CONFIG['min_disk_space_gb'])\n",
    "\n",
    "# Show initial status\n",
    "print(\"=\"*70)\n",
    "print(\"SYSTEM RESOURCES\")\n",
    "print(\"=\"*70)\n",
    "monitor.print_status()\n",
    "\n",
    "# GPU info\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Enable optimizations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# CHECKPOINT MANAGER\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nclass CheckpointManager:\n    \"\"\"Manage checkpoints for resumable training and embedding generation.\"\"\"\n    \n    def __init__(self, checkpoint_dir, config):\n        self.checkpoint_dir = checkpoint_dir\n        self.config = config\n        self.config_str = self._get_config_str()\n    \n    def _get_config_str(self):\n        \"\"\"Create unique identifier for this configuration.\"\"\"\n        return (\n            f\"{self.config['antibody_encoder']}_\"\n            f\"{self.config['antigen_encoder']}_\"\n            f\"{self.config['finetune_strategy']}_\"\n            f\"{self.config['layer_strategy']}\"\n        )\n    \n    def get_training_checkpoint_path(self):\n        return os.path.join(self.checkpoint_dir, f\"train_ckpt_{self.config_str}.pth\")\n    \n    def get_embedding_checkpoint_path(self):\n        return os.path.join(self.checkpoint_dir, f\"embed_ckpt_{self.config_str}.npz\")\n    \n    def save_training_checkpoint(self, epoch, batch_idx, ab_model, ag_model, optimizer, \n                                  train_history, best_metrics):\n        \"\"\"Save training checkpoint.\"\"\"\n        checkpoint = {\n            'epoch': epoch,\n            'batch_idx': batch_idx,\n            'ab_model_state': ab_model.state_dict(),\n            'ag_model_state': ag_model.state_dict(),\n            'optimizer_state': optimizer.state_dict(),\n            'train_history': train_history,\n            'best_metrics': best_metrics,\n            'config': self.config,\n            'timestamp': datetime.now().isoformat(),\n        }\n        path = self.get_training_checkpoint_path()\n        torch.save(checkpoint, path)\n        print(f\"ğŸ’¾ Checkpoint saved: epoch {epoch+1}, batch {batch_idx}\")\n        return path\n    \n    def load_training_checkpoint(self, ab_model, ag_model, optimizer):\n        \"\"\"Load training checkpoint if exists.\"\"\"\n        path = self.get_training_checkpoint_path()\n        if os.path.exists(path):\n            # PyTorch 2.6+ requires weights_only=False for checkpoints with numpy/custom objects\n            checkpoint = torch.load(path, weights_only=False)\n            ab_model.load_state_dict(checkpoint['ab_model_state'])\n            ag_model.load_state_dict(checkpoint['ag_model_state'])\n            optimizer.load_state_dict(checkpoint['optimizer_state'])\n            print(f\"âœ… Resumed from checkpoint: epoch {checkpoint['epoch']+1}, batch {checkpoint['batch_idx']}\")\n            return checkpoint\n        return None\n    \n    def save_embedding_checkpoint(self, processed_idx, ab_embeddings, ag_embeddings, labels):\n        \"\"\"Save embedding generation progress.\"\"\"\n        path = self.get_embedding_checkpoint_path()\n        np.savez_compressed(\n            path,\n            processed_idx=processed_idx,\n            ab_embeddings=ab_embeddings[:processed_idx],\n            ag_embeddings=ag_embeddings[:processed_idx],\n            labels=labels[:processed_idx],\n        )\n        print(f\"ğŸ’¾ Embedding checkpoint saved: {processed_idx:,} samples\")\n    \n    def load_embedding_checkpoint(self):\n        \"\"\"Load embedding checkpoint if exists.\"\"\"\n        path = self.get_embedding_checkpoint_path()\n        if os.path.exists(path):\n            data = np.load(path)\n            print(f\"âœ… Resumed embedding from checkpoint: {data['processed_idx']:,} samples done\")\n            return {\n                'processed_idx': int(data['processed_idx']),\n                'ab_embeddings': data['ab_embeddings'],\n                'ag_embeddings': data['ag_embeddings'],\n                'labels': data['labels'],\n            }\n        return None\n    \n    def clear_checkpoints(self):\n        \"\"\"Remove all checkpoints for this config.\"\"\"\n        for path in [self.get_training_checkpoint_path(), self.get_embedding_checkpoint_path()]:\n            if os.path.exists(path):\n                os.remove(path)\n                print(f\"ğŸ—‘ï¸  Removed: {path}\")\n\n\n# Initialize checkpoint manager\nckpt_manager = CheckpointManager(CHECKPOINT_DIR, CONFIG)\nprint(f\"Checkpoint config: {ckpt_manager.config_str}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# TRAINING LOGGER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class TrainingLogger:\n",
    "    \"\"\"Track and display training metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_r2': [],\n",
    "            'val_mae': [],\n",
    "            'epoch_times': [],\n",
    "        }\n",
    "        self.current_epoch_losses = []\n",
    "        self.epoch_start_time = None\n",
    "    \n",
    "    def start_epoch(self):\n",
    "        self.current_epoch_losses = []\n",
    "        self.epoch_start_time = time.time()\n",
    "    \n",
    "    def log_batch(self, loss):\n",
    "        self.current_epoch_losses.append(loss)\n",
    "    \n",
    "    def end_epoch(self, val_loss=None, val_r2=None, val_mae=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        train_loss = np.mean(self.current_epoch_losses) if self.current_epoch_losses else 0\n",
    "        \n",
    "        self.history['train_loss'].append(train_loss)\n",
    "        self.history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        if val_loss is not None:\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_r2'].append(val_r2)\n",
    "            self.history['val_mae'].append(val_mae)\n",
    "        \n",
    "        return train_loss, epoch_time\n",
    "    \n",
    "    def get_progress_str(self, epoch, total_epochs, batch_idx=None, total_batches=None):\n",
    "        \"\"\"Get formatted progress string.\"\"\"\n",
    "        if self.current_epoch_losses:\n",
    "            recent_loss = np.mean(self.current_epoch_losses[-10:])  # Last 10 batches\n",
    "        else:\n",
    "            recent_loss = 0\n",
    "        \n",
    "        parts = [f\"Epoch {epoch+1}/{total_epochs}\"]\n",
    "        if batch_idx is not None:\n",
    "            parts.append(f\"Batch {batch_idx}/{total_batches}\")\n",
    "        parts.append(f\"Loss: {recent_loss:.4f}\")\n",
    "        \n",
    "        if self.history['val_r2']:\n",
    "            parts.append(f\"Val RÂ²: {self.history['val_r2'][-1]:.4f}\")\n",
    "        \n",
    "        return \" | \".join(parts)\n",
    "    \n",
    "    def print_epoch_summary(self, epoch):\n",
    "        \"\"\"Print summary for completed epoch.\"\"\"\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(f\"EPOCH {epoch+1} SUMMARY\")\n",
    "        print(\"-\"*60)\n",
    "        print(f\"  Train Loss: {self.history['train_loss'][-1]:.4f}\")\n",
    "        if self.history['val_loss']:\n",
    "            print(f\"  Val Loss:   {self.history['val_loss'][-1]:.4f}\")\n",
    "            print(f\"  Val RÂ²:     {self.history['val_r2'][-1]:.4f}\")\n",
    "            print(f\"  Val MAE:    {self.history['val_mae'][-1]:.4f}\")\n",
    "        print(f\"  Time:       {self.history['epoch_times'][-1]/60:.1f} min\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "\n",
    "print(\"Training logger ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Loaded {len(df):,} samples\")\n",
    "\n",
    "# Filter valid pKd range\n",
    "df = df[(df['pKd'] >= 4.0) & (df['pKd'] <= 14.0)].reset_index(drop=True)\n",
    "print(f\"After filtering (pKd 4-14): {len(df):,} samples\")\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\npKd statistics:\")\n",
    "print(f\"  Min: {df['pKd'].min():.2f}\")\n",
    "print(f\"  Max: {df['pKd'].max():.2f}\")\n",
    "print(f\"  Mean: {df['pKd'].mean():.2f}\")\n",
    "print(f\"  Std: {df['pKd'].std():.2f}\")\n",
    "\n",
    "# Split for fine-tuning\n",
    "if CONFIG['finetune_strategy'] != 'frozen':\n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "    print(f\"\\nTrain/Val split: {len(train_df):,} / {len(val_df):,}\")\n",
    "else:\n",
    "    train_df = df\n",
    "    val_df = None\n",
    "    print(\"\\nStrategy is FROZEN - no train/val split needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FINE-TUNING UTILITIES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def apply_lora(model, config):\n",
    "    \"\"\"Apply LoRA adapters to model.\"\"\"\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=config['lora_r'],\n",
    "        lora_alpha=config['lora_alpha'],\n",
    "        lora_dropout=config['lora_dropout'],\n",
    "        target_modules=config['lora_target_modules'],\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.FEATURE_EXTRACTION,\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "\n",
    "def unfreeze_last_n_percent(model, percent=25):\n",
    "    \"\"\"Unfreeze last N% of encoder blocks.\"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    if hasattr(model, 'encoder') and hasattr(model.encoder, 'block'):\n",
    "        blocks = model.encoder.block\n",
    "        total_blocks = len(blocks)\n",
    "        n_unfreeze = max(1, int(total_blocks * percent / 100))\n",
    "        \n",
    "        print(f\"  Total blocks: {total_blocks}, Unfreezing last {n_unfreeze} ({percent}%)\")\n",
    "        \n",
    "        for i in range(total_blocks - n_unfreeze, total_blocks):\n",
    "            for param in blocks[i].parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        if hasattr(model.encoder, 'final_layer_norm'):\n",
    "            for param in model.encoder.final_layer_norm.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Trainable: {trainable/1e6:.1f}M / {total/1e6:.1f}M ({100*trainable/total:.1f}%)\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def apply_finetune_strategy(model, strategy, config):\n",
    "    \"\"\"Apply fine-tuning strategy.\"\"\"\n",
    "    print(f\"  Strategy: {strategy.upper()}\")\n",
    "    \n",
    "    if strategy == 'frozen':\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    elif strategy == 'lora':\n",
    "        model = apply_lora(model, config)\n",
    "    elif strategy == 'unfreeze_last_25':\n",
    "        model = unfreeze_last_n_percent(model, 25)\n",
    "    elif strategy == 'unfreeze_last_50':\n",
    "        model = unfreeze_last_n_percent(model, 50)\n",
    "    elif strategy == 'full':\n",
    "        trainable = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"  Trainable: {trainable/1e6:.1f}M (100%)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Layer extraction functions\n",
    "def extract_embeddings_multilayer(hidden_states, attention_mask, layer_indices, pooling='mean'):\n",
    "    embeddings_list = []\n",
    "    for idx in layer_indices:\n",
    "        hidden = hidden_states[idx]\n",
    "        if pooling == 'mean':\n",
    "            mask = attention_mask.unsqueeze(-1).float()\n",
    "            emb = (hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)\n",
    "        elif pooling == 'cls':\n",
    "            emb = hidden[:, 0, :]\n",
    "        elif pooling == 'max':\n",
    "            emb = hidden.max(dim=1).values\n",
    "        embeddings_list.append(emb)\n",
    "    return torch.cat(embeddings_list, dim=-1)\n",
    "\n",
    "\n",
    "def extract_embeddings_weighted(hidden_states, attention_mask, pooling='mean'):\n",
    "    n_layers = len(hidden_states)\n",
    "    weights = torch.arange(1, n_layers + 1, dtype=torch.float32, device=hidden_states[0].device)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    weighted_sum = None\n",
    "    for i, hidden in enumerate(hidden_states):\n",
    "        if pooling == 'mean':\n",
    "            mask = attention_mask.unsqueeze(-1).float()\n",
    "            emb = (hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)\n",
    "        elif pooling == 'cls':\n",
    "            emb = hidden[:, 0, :]\n",
    "        else:\n",
    "            emb = hidden.max(dim=1).values\n",
    "        \n",
    "        if weighted_sum is None:\n",
    "            weighted_sum = weights[i] * emb\n",
    "        else:\n",
    "            weighted_sum += weights[i] * emb\n",
    "    return weighted_sum\n",
    "\n",
    "\n",
    "print(\"Fine-tuning utilities ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import T5EncoderModel, T5Tokenizer\n\nENCODER_MODELS = {\n    'IgT5': 'Exscientia/IgT5',\n    'AbRoBERTa': 'mogam-ai/Ab-RoBERTa',\n    'ProtT5-Half': 'Rostlab/prot_t5_xl_half_uniref50-enc',\n    'ProtT5-Full': 'Rostlab/prot_t5_xl_uniref50',\n    'ESM2': 'facebook/esm2_t33_650M_UR50D',\n}\n\nprint(\"=\"*70)\nprint(\"LOADING ENCODERS\")\nprint(\"=\"*70)\n\n# Antibody encoder\nab_model_name = ENCODER_MODELS[CONFIG['antibody_encoder']]\nprint(f\"\\n[Antibody] {ab_model_name}\")\nab_tokenizer = T5Tokenizer.from_pretrained(ab_model_name)\nab_model = T5EncoderModel.from_pretrained(\n    ab_model_name,\n    torch_dtype=torch.float16,\n    output_hidden_states=True\n)\n\n# Enable gradient checkpointing to save memory\nif CONFIG.get('use_gradient_checkpointing', False) and CONFIG['finetune_strategy'] != 'frozen':\n    ab_model.gradient_checkpointing_enable()\n    print(\"  âœ“ Gradient checkpointing enabled\")\n\nab_model = apply_finetune_strategy(ab_model, CONFIG['finetune_strategy'], CONFIG)\nab_model = ab_model.to(device)\n\n# Antigen encoder\nag_model_name = ENCODER_MODELS[CONFIG['antigen_encoder']]\nprint(f\"\\n[Antigen] {ag_model_name}\")\nag_tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_uniref50')\nag_model = T5EncoderModel.from_pretrained(\n    ag_model_name,\n    torch_dtype=torch.float16,\n    output_hidden_states=True\n)\n\n# Enable gradient checkpointing to save memory\nif CONFIG.get('use_gradient_checkpointing', False) and CONFIG['finetune_strategy'] != 'frozen':\n    ag_model.gradient_checkpointing_enable()\n    print(\"  âœ“ Gradient checkpointing enabled\")\n\nag_model = apply_finetune_strategy(ag_model, CONFIG['finetune_strategy'], CONFIG)\nag_model = ag_model.to(device)\n\n# Calculate dimensions\nab_dim = 1024\nag_dim = 1024\nif CONFIG['layer_strategy'] == 'multi':\n    n_layers = len(CONFIG['multi_layers'])\n    ab_output_dim = ab_dim * n_layers\n    ag_output_dim = ag_dim * n_layers\nelse:\n    ab_output_dim = ab_dim\n    ag_output_dim = ag_dim\n\nprint(f\"\\nOutput dimensions: Ab={ab_output_dim}, Ag={ag_output_dim}, Total={ab_output_dim + ag_output_dim}\")\nmonitor.print_status(\"\\nAfter loading: \")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning (with checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# FINE-TUNING WITH CHECKPOINTS & GRADIENT ACCUMULATION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nif CONFIG['finetune_strategy'] != 'frozen':\n    print(\"=\"*70)\n    print(\"FINE-TUNING ENCODERS\")\n    print(\"=\"*70)\n    \n    grad_accum = CONFIG.get('gradient_accumulation', 1)\n    effective_batch = CONFIG['batch_size'] * grad_accum\n    print(f\"Batch size: {CONFIG['batch_size']} x {grad_accum} accumulation = {effective_batch} effective\")\n    \n    # Dataset\n    class FTDataset(Dataset):\n        def __init__(self, dataframe):\n            self.data = dataframe.reset_index(drop=True)\n        def __len__(self):\n            return len(self.data)\n        def __getitem__(self, idx):\n            row = self.data.iloc[idx]\n            return {\n                'ab_seq': row['antibody_sequence'],\n                'ag_seq': row['antigen_sequence'],\n                'pKd': torch.tensor(row['pKd'], dtype=torch.float32)\n            }\n    \n    def ft_collate(batch):\n        return {\n            'ab_seq': [b['ab_seq'] for b in batch],\n            'ag_seq': [b['ag_seq'] for b in batch],\n            'pKd': torch.stack([b['pKd'] for b in batch])\n        }\n    \n    # Simple head\n    class SimpleHead(nn.Module):\n        def __init__(self, input_dim):\n            super().__init__()\n            self.head = nn.Sequential(\n                nn.Linear(input_dim, 256),\n                nn.ReLU(),\n                nn.Dropout(0.1),\n                nn.Linear(256, 1)\n            )\n        def forward(self, x):\n            return self.head(x).squeeze(-1)\n    \n    head = SimpleHead(ab_output_dim + ag_output_dim).to(device).to(torch.float32)\n    \n    # Optimizer\n    params = [\n        {'params': [p for p in ab_model.parameters() if p.requires_grad], 'lr': CONFIG['ft_lr']},\n        {'params': [p for p in ag_model.parameters() if p.requires_grad], 'lr': CONFIG['ft_lr']},\n        {'params': head.parameters(), 'lr': CONFIG['ft_lr'] * 10},\n    ]\n    optimizer = torch.optim.AdamW(params, weight_decay=0.01)\n    criterion = nn.MSELoss()\n    \n    # DataLoaders\n    train_loader = DataLoader(FTDataset(train_df), batch_size=CONFIG['batch_size'],\n                              shuffle=True, collate_fn=ft_collate, num_workers=2, pin_memory=True)\n    val_loader = DataLoader(FTDataset(val_df), batch_size=CONFIG['batch_size'],\n                            shuffle=False, collate_fn=ft_collate, num_workers=2, pin_memory=True)\n    \n    # Logger\n    logger = TrainingLogger()\n    \n    # Try to resume\n    start_epoch = 0\n    start_batch = 0\n    if CONFIG['resume_from_checkpoint']:\n        ckpt = ckpt_manager.load_training_checkpoint(ab_model, ag_model, optimizer)\n        if ckpt:\n            start_epoch = ckpt['epoch']\n            start_batch = ckpt['batch_idx']\n            logger.history = ckpt['train_history']\n    \n    # Training loop\n    print(f\"\\nTraining for {CONFIG['ft_epochs']} epochs...\")\n    \n    for epoch in range(start_epoch, CONFIG['ft_epochs']):\n        ab_model.train()\n        ag_model.train()\n        head.train()\n        logger.start_epoch()\n        \n        optimizer.zero_grad()  # Zero at start of accumulation\n        \n        pbar = tqdm(enumerate(train_loader), total=len(train_loader), \n                    desc=f\"Epoch {epoch+1}/{CONFIG['ft_epochs']}\")\n        \n        for batch_idx, batch in pbar:\n            # Skip if resuming\n            if epoch == start_epoch and batch_idx < start_batch:\n                continue\n            \n            # Tokenize\n            ab_seqs = [\" \".join(list(s)) for s in batch['ab_seq']]\n            ag_seqs = [\" \".join(list(s)) for s in batch['ag_seq']]\n            \n            ab_tokens = ab_tokenizer(ab_seqs, return_tensors='pt', padding=True,\n                                     truncation=True, max_length=CONFIG['max_ab_length']).to(device)\n            ag_tokens = ag_tokenizer(ag_seqs, return_tensors='pt', padding=True,\n                                     truncation=True, max_length=CONFIG['max_ag_length']).to(device)\n            targets = batch['pKd'].to(device)\n            \n            # Forward\n            with torch.amp.autocast('cuda', dtype=torch.float16):\n                ab_out = ab_model(**ab_tokens)\n                ag_out = ag_model(**ag_tokens)\n                \n                if CONFIG['layer_strategy'] == 'multi':\n                    ab_emb = extract_embeddings_multilayer(ab_out.hidden_states, ab_tokens['attention_mask'],\n                                                          CONFIG['multi_layers'], CONFIG['pooling'])\n                    ag_emb = extract_embeddings_multilayer(ag_out.hidden_states, ag_tokens['attention_mask'],\n                                                          CONFIG['multi_layers'], CONFIG['pooling'])\n                elif CONFIG['layer_strategy'] == 'weighted_all':\n                    ab_emb = extract_embeddings_weighted(ab_out.hidden_states, ab_tokens['attention_mask'], CONFIG['pooling'])\n                    ag_emb = extract_embeddings_weighted(ag_out.hidden_states, ag_tokens['attention_mask'], CONFIG['pooling'])\n                else:\n                    ab_hidden = ab_out.last_hidden_state\n                    ag_hidden = ag_out.last_hidden_state\n                    mask_ab = ab_tokens['attention_mask'].unsqueeze(-1).float()\n                    mask_ag = ag_tokens['attention_mask'].unsqueeze(-1).float()\n                    ab_emb = (ab_hidden * mask_ab).sum(1) / mask_ab.sum(1)\n                    ag_emb = (ag_hidden * mask_ag).sum(1) / mask_ag.sum(1)\n                \n                combined = torch.cat([ab_emb, ag_emb], dim=-1).float()\n                pred = head(combined)\n                loss = criterion(pred, targets) / grad_accum  # Scale loss\n            \n            loss.backward()\n            logger.log_batch(loss.item() * grad_accum)  # Log unscaled loss\n            \n            # Gradient accumulation step\n            if (batch_idx + 1) % grad_accum == 0:\n                torch.nn.utils.clip_grad_norm_(\n                    list(ab_model.parameters()) + list(ag_model.parameters()) + list(head.parameters()), 1.0\n                )\n                optimizer.step()\n                optimizer.zero_grad()\n            \n            # Update progress\n            pbar.set_postfix({\n                'loss': f'{loss.item() * grad_accum:.4f}', \n                'avg': f'{np.mean(logger.current_epoch_losses[-50:]):.4f}'\n            })\n            \n            # Checkpoint\n            if (batch_idx + 1) % CONFIG['checkpoint_every_n_batches'] == 0:\n                ckpt_manager.save_training_checkpoint(\n                    epoch, batch_idx, ab_model, ag_model, optimizer, \n                    logger.history, {'train_loss': np.mean(logger.current_epoch_losses)}\n                )\n            \n            # Memory management\n            if (batch_idx + 1) % CONFIG['clear_cache_every_n_batches'] == 0:\n                monitor.clear_memory()\n        \n        # Final optimizer step if not aligned with accumulation\n        if len(train_loader) % grad_accum != 0:\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        # Validation\n        ab_model.eval()\n        ag_model.eval()\n        head.eval()\n        \n        val_preds, val_labels = [], []\n        val_loss_sum = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=\"Validating\", leave=False):\n                ab_seqs = [\" \".join(list(s)) for s in batch['ab_seq']]\n                ag_seqs = [\" \".join(list(s)) for s in batch['ag_seq']]\n                \n                ab_tokens = ab_tokenizer(ab_seqs, return_tensors='pt', padding=True,\n                                         truncation=True, max_length=CONFIG['max_ab_length']).to(device)\n                ag_tokens = ag_tokenizer(ag_seqs, return_tensors='pt', padding=True,\n                                         truncation=True, max_length=CONFIG['max_ag_length']).to(device)\n                targets = batch['pKd'].to(device)\n                \n                with torch.amp.autocast('cuda', dtype=torch.float16):\n                    ab_out = ab_model(**ab_tokens)\n                    ag_out = ag_model(**ag_tokens)\n                    \n                    if CONFIG['layer_strategy'] == 'multi':\n                        ab_emb = extract_embeddings_multilayer(ab_out.hidden_states, ab_tokens['attention_mask'],\n                                                              CONFIG['multi_layers'], CONFIG['pooling'])\n                        ag_emb = extract_embeddings_multilayer(ag_out.hidden_states, ag_tokens['attention_mask'],\n                                                              CONFIG['multi_layers'], CONFIG['pooling'])\n                    elif CONFIG['layer_strategy'] == 'weighted_all':\n                        ab_emb = extract_embeddings_weighted(ab_out.hidden_states, ab_tokens['attention_mask'], CONFIG['pooling'])\n                        ag_emb = extract_embeddings_weighted(ag_out.hidden_states, ag_tokens['attention_mask'], CONFIG['pooling'])\n                    else:\n                        ab_hidden = ab_out.last_hidden_state\n                        ag_hidden = ag_out.last_hidden_state\n                        mask_ab = ab_tokens['attention_mask'].unsqueeze(-1).float()\n                        mask_ag = ag_tokens['attention_mask'].unsqueeze(-1).float()\n                        ab_emb = (ab_hidden * mask_ab).sum(1) / mask_ab.sum(1)\n                        ag_emb = (ag_hidden * mask_ag).sum(1) / mask_ag.sum(1)\n                    \n                    combined = torch.cat([ab_emb, ag_emb], dim=-1).float()\n                    pred = head(combined)\n                    val_loss_sum += criterion(pred, targets).item()\n                \n                val_preds.extend(pred.cpu().numpy())\n                val_labels.extend(targets.cpu().numpy())\n        \n        val_preds = np.array(val_preds)\n        val_labels = np.array(val_labels)\n        val_loss = val_loss_sum / len(val_loader)\n        val_r2 = r2_score(val_labels, val_preds)\n        val_mae = mean_absolute_error(val_labels, val_preds)\n        \n        logger.end_epoch(val_loss, val_r2, val_mae)\n        logger.print_epoch_summary(epoch)\n        monitor.print_status(\"Resources: \")\n        \n        # Epoch checkpoint\n        if (epoch + 1) % CONFIG['checkpoint_every_n_epochs'] == 0:\n            ckpt_manager.save_training_checkpoint(\n                epoch + 1, 0, ab_model, ag_model, optimizer,\n                logger.history, {'val_loss': val_loss, 'val_r2': val_r2, 'val_mae': val_mae}\n            )\n        \n        start_batch = 0  # Reset for next epoch\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"FINE-TUNING COMPLETE!\")\n    print(\"=\"*70)\n    print(f\"Best Val RÂ²: {max(logger.history['val_r2']):.4f}\")\n    print(f\"Best Val MAE: {min(logger.history['val_mae']):.4f}\")\n    \n    # Cleanup\n    del head, optimizer\n    monitor.clear_memory()\n    \nelse:\n    print(\"Strategy is FROZEN - skipping fine-tuning\")\n\n# Set to eval mode\nab_model.eval()\nag_model.eval()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings (with checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# GENERATE EMBEDDINGS WITH CHECKPOINTS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING EMBEDDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_samples = len(df)\n",
    "batch_size = CONFIG['batch_size'] * 2  # Larger for inference\n",
    "labels = df['pKd'].values.astype(np.float32)\n",
    "\n",
    "print(f\"Samples: {n_samples:,}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Layer strategy: {CONFIG['layer_strategy']}\")\n",
    "\n",
    "# Try to resume\n",
    "start_idx = 0\n",
    "if CONFIG['resume_from_checkpoint']:\n",
    "    ckpt = ckpt_manager.load_embedding_checkpoint()\n",
    "    if ckpt:\n",
    "        start_idx = ckpt['processed_idx']\n",
    "        ab_embeddings = np.zeros((n_samples, ab_output_dim), dtype=np.float32)\n",
    "        ag_embeddings = np.zeros((n_samples, ag_output_dim), dtype=np.float32)\n",
    "        ab_embeddings[:start_idx] = ckpt['ab_embeddings']\n",
    "        ag_embeddings[:start_idx] = ckpt['ag_embeddings']\n",
    "    else:\n",
    "        ab_embeddings = np.zeros((n_samples, ab_output_dim), dtype=np.float32)\n",
    "        ag_embeddings = np.zeros((n_samples, ag_output_dim), dtype=np.float32)\n",
    "else:\n",
    "    ab_embeddings = np.zeros((n_samples, ab_output_dim), dtype=np.float32)\n",
    "    ag_embeddings = np.zeros((n_samples, ag_output_dim), dtype=np.float32)\n",
    "\n",
    "print(f\"Starting from index: {start_idx:,}\")\n",
    "\n",
    "start_time = time.time()\n",
    "batches_since_checkpoint = 0\n",
    "\n",
    "pbar = tqdm(range(start_idx, n_samples, batch_size), desc=\"Encoding\", \n",
    "            initial=start_idx//batch_size, total=(n_samples + batch_size - 1)//batch_size)\n",
    "\n",
    "for i in pbar:\n",
    "    batch_end = min(i + batch_size, n_samples)\n",
    "    batch_df = df.iloc[i:batch_end]\n",
    "    \n",
    "    # Prepare sequences\n",
    "    ab_seqs = [\" \".join(list(s)) for s in batch_df['antibody_sequence'].tolist()]\n",
    "    ag_seqs = [\" \".join(list(s)) for s in batch_df['antigen_sequence'].tolist()]\n",
    "    \n",
    "    # Tokenize\n",
    "    ab_tokens = ab_tokenizer(ab_seqs, return_tensors='pt', padding=True,\n",
    "                             truncation=True, max_length=CONFIG['max_ab_length']).to(device)\n",
    "    ag_tokens = ag_tokenizer(ag_seqs, return_tensors='pt', padding=True,\n",
    "                             truncation=True, max_length=CONFIG['max_ag_length']).to(device)\n",
    "    \n",
    "    # Encode\n",
    "    with torch.no_grad(), torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "        ab_out = ab_model(**ab_tokens)\n",
    "        ag_out = ag_model(**ag_tokens)\n",
    "        \n",
    "        if CONFIG['layer_strategy'] == 'multi':\n",
    "            ab_emb = extract_embeddings_multilayer(ab_out.hidden_states, ab_tokens['attention_mask'],\n",
    "                                                  CONFIG['multi_layers'], CONFIG['pooling'])\n",
    "            ag_emb = extract_embeddings_multilayer(ag_out.hidden_states, ag_tokens['attention_mask'],\n",
    "                                                  CONFIG['multi_layers'], CONFIG['pooling'])\n",
    "        elif CONFIG['layer_strategy'] == 'weighted_all':\n",
    "            ab_emb = extract_embeddings_weighted(ab_out.hidden_states, ab_tokens['attention_mask'], CONFIG['pooling'])\n",
    "            ag_emb = extract_embeddings_weighted(ag_out.hidden_states, ag_tokens['attention_mask'], CONFIG['pooling'])\n",
    "        else:\n",
    "            ab_hidden = ab_out.last_hidden_state\n",
    "            ag_hidden = ag_out.last_hidden_state\n",
    "            mask_ab = ab_tokens['attention_mask'].unsqueeze(-1).float()\n",
    "            mask_ag = ag_tokens['attention_mask'].unsqueeze(-1).float()\n",
    "            ab_emb = (ab_hidden * mask_ab).sum(1) / mask_ab.sum(1)\n",
    "            ag_emb = (ag_hidden * mask_ag).sum(1) / mask_ag.sum(1)\n",
    "    \n",
    "    ab_embeddings[i:batch_end] = ab_emb.float().cpu().numpy()\n",
    "    ag_embeddings[i:batch_end] = ag_emb.float().cpu().numpy()\n",
    "    \n",
    "    batches_since_checkpoint += 1\n",
    "    \n",
    "    # Update progress\n",
    "    elapsed = time.time() - start_time\n",
    "    speed = (batch_end - start_idx) / elapsed if elapsed > 0 else 0\n",
    "    eta = (n_samples - batch_end) / speed / 60 if speed > 0 else 0\n",
    "    pbar.set_postfix({'speed': f'{speed:.0f}/s', 'ETA': f'{eta:.1f}min'})\n",
    "    \n",
    "    # Checkpoint\n",
    "    if batches_since_checkpoint >= CONFIG['checkpoint_every_n_batches']:\n",
    "        ckpt_manager.save_embedding_checkpoint(batch_end, ab_embeddings, ag_embeddings, labels)\n",
    "        batches_since_checkpoint = 0\n",
    "    \n",
    "    # Memory management\n",
    "    if (i // batch_size) % CONFIG['clear_cache_every_n_batches'] == 0:\n",
    "        monitor.clear_memory()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nCompleted in {elapsed/60:.1f} minutes\")\n",
    "print(f\"Speed: {(n_samples - start_idx)/elapsed:.1f} samples/sec\")\n",
    "monitor.print_status(\"Final: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SAVE EMBEDDINGS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING EMBEDDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split into train/val/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "indices = np.arange(n_samples)\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_idx):,} | Val: {len(val_idx):,} | Test: {len(test_idx):,}\")\n",
    "\n",
    "# Config string for filename\n",
    "config_str = ckpt_manager.config_str\n",
    "\n",
    "# Save train\n",
    "train_file = os.path.join(OUTPUT_DIR, f\"train_embeddings_{config_str}.npz\")\n",
    "np.savez_compressed(\n",
    "    train_file,\n",
    "    antibody_embeddings=ab_embeddings[train_idx],\n",
    "    antigen_embeddings=ag_embeddings[train_idx],\n",
    "    labels=labels[train_idx],\n",
    ")\n",
    "print(f\"âœ… Train saved: {train_file}\")\n",
    "\n",
    "# Save val\n",
    "val_file = os.path.join(OUTPUT_DIR, f\"val_embeddings_{config_str}.npz\")\n",
    "np.savez_compressed(\n",
    "    val_file,\n",
    "    antibody_embeddings=ab_embeddings[val_idx],\n",
    "    antigen_embeddings=ag_embeddings[val_idx],\n",
    "    labels=labels[val_idx],\n",
    ")\n",
    "print(f\"âœ… Val saved: {val_file}\")\n",
    "\n",
    "# Save test\n",
    "test_file = os.path.join(OUTPUT_DIR, f\"test_embeddings_{config_str}.npz\")\n",
    "np.savez_compressed(\n",
    "    test_file,\n",
    "    antibody_embeddings=ab_embeddings[test_idx],\n",
    "    antigen_embeddings=ag_embeddings[test_idx],\n",
    "    labels=labels[test_idx],\n",
    ")\n",
    "print(f\"âœ… Test saved: {test_file}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'config': CONFIG,\n",
    "    'n_samples': n_samples,\n",
    "    'n_train': len(train_idx),\n",
    "    'n_val': len(val_idx),\n",
    "    'n_test': len(test_idx),\n",
    "    'ab_dim': ab_output_dim,\n",
    "    'ag_dim': ag_output_dim,\n",
    "    'total_dim': ab_output_dim + ag_output_dim,\n",
    "    'generation_time_minutes': elapsed / 60,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "metadata_file = os.path.join(OUTPUT_DIR, f\"metadata_{config_str}.json\")\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "print(f\"âœ… Metadata saved: {metadata_file}\")\n",
    "\n",
    "# Save LoRA weights if applicable\n",
    "if CONFIG['finetune_strategy'] == 'lora':\n",
    "    lora_dir = os.path.join(OUTPUT_DIR, f\"lora_weights_{config_str}\")\n",
    "    os.makedirs(lora_dir, exist_ok=True)\n",
    "    ab_model.save_pretrained(os.path.join(lora_dir, 'antibody'))\n",
    "    ag_model.save_pretrained(os.path.join(lora_dir, 'antigen'))\n",
    "    print(f\"âœ… LoRA weights saved: {lora_dir}\")\n",
    "\n",
    "# File sizes\n",
    "print(f\"\\nFile sizes:\")\n",
    "for f in [train_file, val_file, test_file]:\n",
    "    size = os.path.getsize(f) / (1024**3)\n",
    "    print(f\"  {os.path.basename(f)}: {size:.2f} GB\")\n",
    "\n",
    "# Clear checkpoints (completed successfully)\n",
    "print(\"\\nClearing checkpoints...\")\n",
    "ckpt_manager.clear_checkpoints()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STAGE 1 COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nNext: Open Stage2_Fast_Training.ipynb\")\n",
    "print(f\"Set embeddings_dir to: {OUTPUT_DIR}\")\n",
    "print(f\"Use files with suffix: {config_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del ab_model, ag_model, ab_embeddings, ag_embeddings\n",
    "monitor.clear_memory()\n",
    "print(\"Memory cleaned up!\")\n",
    "monitor.print_status()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}