{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antibody-Antigen Binding Affinity Prediction - Training\n",
    "\n",
    "**Ultra-Speed Training v2.6** - IgT5 + ESM-2 Dual Encoder Model\n",
    "\n",
    "## Instructions:\n",
    "1. Upload your CSV file (`ab_ag_affinity_complete.csv`) to Google Drive in folder `AbAg_Training`\n",
    "2. Run all cells in order\n",
    "3. Training will auto-save checkpoints - you can resume if disconnected\n",
    "\n",
    "**Expected time:** ~21-22 hours for 50 epochs on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create working directory\n",
    "import os\n",
    "work_dir = '/content/drive/MyDrive/AbAg_Training'\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "%cd {work_dir}\n",
    "\n",
    "# Check if dataset exists\n",
    "dataset_file = 'ab_ag_affinity_complete.csv'\n",
    "if os.path.exists(dataset_file):\n",
    "    print(f\"Dataset found: {dataset_file}\")\n",
    "    !wc -l {dataset_file}\n",
    "else:\n",
    "    print(f\"ERROR: Please upload '{dataset_file}' to Google Drive folder 'AbAg_Training'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers pandas scipy scikit-learn tqdm sentencepiece faesm bitsandbytes accelerate\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Adjust these settings if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "# You can modify these settings\n",
    "\n",
    "CONFIG = {\n",
    "    'data_path': 'ab_ag_affinity_complete.csv',  # Your dataset file\n",
    "    'output_dir': 'outputs',                      # Where to save models\n",
    "    'epochs': 50,                                 # Number of training epochs\n",
    "    'batch_size': 16,                             # Batch size (16 works well on T4)\n",
    "    'accumulation_steps': 3,                      # Gradient accumulation (effective batch = 48)\n",
    "    'learning_rate': 3e-3,                        # Learning rate\n",
    "    'weight_decay': 0.02,                         # L2 regularization\n",
    "    'dropout': 0.35,                              # Dropout rate\n",
    "    'early_stopping_patience': 10,                # Stop if no improvement for N epochs\n",
    "    'validation_frequency': 1,                    # Validate every N epochs\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Code\n",
    "\n",
    "This cell contains all the training code. Just run it - no modifications needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ULTRA SPEED Training v2.6 - All Advanced Optimizations\n",
    "IgT5 + ESM-2 Dual Encoder for Antibody-Antigen Binding Prediction\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import shutil\n",
    "import gc\n",
    "import random\n",
    "from transformers import T5EncoderModel, T5Tokenizer, AutoTokenizer, AutoModel, BitsAndBytesConfig\n",
    "import threading\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# NUCLEAR FIX: Force disable torch.compile globally\n",
    "# ============================================================================\n",
    "import torch._dynamo\n",
    "import torch.compiler\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch.compiler.disable()\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
    "os.environ['TORCH_CUDAGRAPH_DISABLE'] = '1'\n",
    "\n",
    "print(\"torch.compile DISABLED (prevents CUDA graphs errors)\")\n",
    "\n",
    "# Try to import FAESM for FlashAttention\n",
    "try:\n",
    "    from faesm.esm import FAEsmForMaskedLM\n",
    "    FLASH_ATTN_AVAILABLE = True\n",
    "    print(\"FlashAttention (FAESM) available\")\n",
    "except ImportError:\n",
    "    FLASH_ATTN_AVAILABLE = False\n",
    "    print(\"Using standard ESM-2 (no FlashAttention)\")\n",
    "\n",
    "# Enable backend optimizations\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# ============================================================================\n",
    "# DISK CLEANUP\n",
    "# ============================================================================\n",
    "def cleanup_disk_space():\n",
    "    \"\"\"Clean up disk space\"\"\"\n",
    "    try:\n",
    "        subprocess.run(['pip', 'cache', 'purge'], capture_output=True)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ============================================================================\n",
    "# SEQUENCE BUCKETING\n",
    "# ============================================================================\n",
    "class BucketBatchSampler(Sampler):\n",
    "    \"\"\"Group sequences by similar lengths to minimize padding\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size, drop_last=True, buckets=[256, 384, 512]):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.buckets = sorted(buckets)\n",
    "        self.bucket_indices = {b: [] for b in self.buckets}\n",
    "        \n",
    "        for idx in range(len(dataset)):\n",
    "            item = dataset[idx]\n",
    "            seq_len = len(item['antibody_sequence'])\n",
    "            bucket = min([b for b in self.buckets if b >= seq_len], default=self.buckets[-1])\n",
    "            self.bucket_indices[bucket].append(idx)\n",
    "        \n",
    "        print(f\"\\nBucket Distribution:\")\n",
    "        for bucket in self.buckets:\n",
    "            count = len(self.bucket_indices[bucket])\n",
    "            print(f\"  <={bucket}: {count:,} samples ({count/len(dataset)*100:.1f}%)\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        bucket_order = list(self.buckets)\n",
    "        random.shuffle(bucket_order)\n",
    "        \n",
    "        for bucket in bucket_order:\n",
    "            indices = self.bucket_indices[bucket].copy()\n",
    "            random.shuffle(indices)\n",
    "            \n",
    "            for i in range(0, len(indices), self.batch_size):\n",
    "                batch = indices[i:i+self.batch_size]\n",
    "                if len(batch) == self.batch_size or not self.drop_last:\n",
    "                    yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        count = 0\n",
    "        for bucket in self.buckets:\n",
    "            n = len(self.bucket_indices[bucket])\n",
    "            count += n // self.batch_size\n",
    "            if not self.drop_last and n % self.batch_size > 0:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL\n",
    "# ============================================================================\n",
    "class IgT5ESM2Model(nn.Module):\n",
    "    \"\"\"Dual-encoder model: IgT5 for antibody + ESM-2 for antigen\"\"\"\n",
    "    \n",
    "    def __init__(self, dropout=0.3, use_quantization=True, use_checkpointing=True):\n",
    "        super().__init__()\n",
    "        self.use_checkpointing = use_checkpointing\n",
    "        \n",
    "        print(\"Loading models...\")\n",
    "        \n",
    "        # Quantization config\n",
    "        quantization_config = None\n",
    "        if use_quantization:\n",
    "            try:\n",
    "                quantization_config = BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,\n",
    "                    llm_int8_threshold=6.0,\n",
    "                    llm_int8_has_fp16_weight=False\n",
    "                )\n",
    "                print(\"  Using INT8 quantization\")\n",
    "            except:\n",
    "                print(\"  Quantization not available, using BFloat16\")\n",
    "        \n",
    "        # Load IgT5\n",
    "        print(\"  Loading IgT5 for antibody...\")\n",
    "        self.igt5_tokenizer = T5Tokenizer.from_pretrained(\n",
    "            \"Exscientia/IgT5\", do_lower_case=False, use_fast=True\n",
    "        )\n",
    "        if quantization_config:\n",
    "            self.igt5_model = T5EncoderModel.from_pretrained(\n",
    "                \"Exscientia/IgT5\", quantization_config=quantization_config, device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            self.igt5_model = T5EncoderModel.from_pretrained(\"Exscientia/IgT5\")\n",
    "        \n",
    "        # Load ESM-2\n",
    "        print(\"  Loading ESM-2 for antigen...\")\n",
    "        self.esm2_tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"facebook/esm2_t33_650M_UR50D\", use_fast=True\n",
    "        )\n",
    "        if FLASH_ATTN_AVAILABLE:\n",
    "            self.esm2_model = FAEsmForMaskedLM.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "        else:\n",
    "            self.esm2_model = AutoModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "        \n",
    "        # Freeze encoders\n",
    "        for param in self.igt5_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.esm2_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Regressor\n",
    "        igt5_dim = self.igt5_model.config.d_model\n",
    "        esm2_dim = self.esm2_model.config.hidden_size\n",
    "        combined_dim = igt5_dim + esm2_dim\n",
    "        \n",
    "        self.regressor_block1 = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 1024), nn.GELU(), nn.Dropout(dropout), nn.LayerNorm(1024)\n",
    "        )\n",
    "        self.regressor_block2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512), nn.GELU(), nn.Dropout(dropout), nn.LayerNorm(512)\n",
    "        )\n",
    "        self.regressor_block3 = nn.Sequential(\n",
    "            nn.Linear(512, 256), nn.GELU(), nn.Dropout(dropout), nn.LayerNorm(256)\n",
    "        )\n",
    "        self.regressor_block4 = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.GELU(), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.regressor_final = nn.Linear(128, 1)\n",
    "        \n",
    "        print(\"  Model ready!\")\n",
    "    \n",
    "    def get_batch_embeddings(self, sequences, model, tokenizer, device, pooling='mean'):\n",
    "        inputs = tokenizer(\n",
    "            sequences, return_tensors='pt', padding=True, truncation=True, max_length=512\n",
    "        ).to(device, non_blocking=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            if pooling == 'mean':\n",
    "                embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            else:\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        return embeddings\n",
    "    \n",
    "    def forward(self, antibody_seqs, antigen_seqs, device):\n",
    "        ab_embeddings = self.get_batch_embeddings(\n",
    "            antibody_seqs, self.igt5_model, self.igt5_tokenizer, device, pooling='mean'\n",
    "        )\n",
    "        ag_embeddings = self.get_batch_embeddings(\n",
    "            antigen_seqs, self.esm2_model, self.esm2_tokenizer, device, pooling='cls'\n",
    "        )\n",
    "        \n",
    "        combined = torch.cat([ab_embeddings, ag_embeddings], dim=1)\n",
    "        \n",
    "        if self.use_checkpointing and self.training:\n",
    "            x = checkpoint(self.regressor_block1, combined, use_reentrant=False)\n",
    "            x = checkpoint(self.regressor_block2, x, use_reentrant=False)\n",
    "            x = checkpoint(self.regressor_block3, x, use_reentrant=False)\n",
    "            x = checkpoint(self.regressor_block4, x, use_reentrant=False)\n",
    "        else:\n",
    "            x = self.regressor_block1(combined)\n",
    "            x = self.regressor_block2(x)\n",
    "            x = self.regressor_block3(x)\n",
    "            x = self.regressor_block4(x)\n",
    "        \n",
    "        return self.regressor_final(x).squeeze(-1)\n",
    "\n",
    "# ============================================================================\n",
    "# LOSS & UTILITIES\n",
    "# ============================================================================\n",
    "class FocalMSELoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        if self.label_smoothing > 0:\n",
    "            target_mean = target.mean()\n",
    "            target = (1 - self.label_smoothing) * target + self.label_smoothing * target_mean\n",
    "        mse = (pred - target) ** 2\n",
    "        focal_weight = (1 + mse) ** self.gamma\n",
    "        return (focal_weight * mse).mean()\n",
    "\n",
    "class AbAgDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'antibody_sequence': self.df.iloc[idx]['antibody_sequence'],\n",
    "            'antigen_sequence': self.df.iloc[idx]['antigen_sequence'],\n",
    "            'pKd': torch.tensor(self.df.iloc[idx]['pKd'], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'antibody_seqs': [item['antibody_sequence'] for item in batch],\n",
    "        'antigen_seqs': [item['antigen_sequence'] for item in batch],\n",
    "        'pKd': torch.stack([item['pKd'] for item in batch])\n",
    "    }\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.0001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    def __call__(self, score, epoch):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            return False\n",
    "        \n",
    "        if score > self.best_score + self.min_delta:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(f\"  EarlyStopping: {self.counter}/{self.patience} (best: {self.best_score:.4f} @ epoch {self.best_epoch+1})\")\n",
    "            return self.counter >= self.patience\n",
    "\n",
    "def compute_metrics(targets, predictions):\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    spearman, _ = stats.spearmanr(targets, predictions)\n",
    "    pearson, _ = stats.pearsonr(targets, predictions)\n",
    "    \n",
    "    # High-affinity metrics (pKd >= 9)\n",
    "    strong = targets >= 9.0\n",
    "    pred_strong = predictions >= 9.0\n",
    "    if strong.sum() > 0:\n",
    "        tp = (strong & pred_strong).sum()\n",
    "        fn = (strong & ~pred_strong).sum()\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    else:\n",
    "        recall = 0\n",
    "    \n",
    "    return {\n",
    "        'rmse': rmse, 'mae': mae, 'r2': r2,\n",
    "        'spearman': spearman, 'pearson': pearson,\n",
    "        'recall_pkd9': recall * 100\n",
    "    }\n",
    "\n",
    "def quick_eval(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions, targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if i >= 50:  # Quick eval on 50 batches\n",
    "                break\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                preds = model(batch['antibody_seqs'], batch['antigen_seqs'], device)\n",
    "            predictions.extend(preds.float().cpu().numpy())\n",
    "            targets.extend(batch['pKd'].numpy())\n",
    "    \n",
    "    return compute_metrics(np.array(targets), np.array(predictions))\n",
    "\n",
    "def full_eval(model, loader, device, desc=\"Eval\"):\n",
    "    model.eval()\n",
    "    predictions, targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=desc):\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                preds = model(batch['antibody_seqs'], batch['antigen_seqs'], device)\n",
    "            predictions.extend(preds.float().cpu().numpy())\n",
    "            targets.extend(batch['pKd'].numpy())\n",
    "    \n",
    "    return compute_metrics(np.array(targets), np.array(predictions)), np.array(predictions), np.array(targets)\n",
    "\n",
    "print(\"Training code loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ANTIBODY-ANTIGEN BINDING PREDICTION - TRAINING\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n",
    "# Load data\n",
    "print(f\"\\nLoading data from: {CONFIG['data_path']}\")\n",
    "df = pd.read_csv(CONFIG['data_path'])\n",
    "print(f\"Loaded {len(df):,} samples\")\n",
    "\n",
    "# Split data\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "val_df_quick = val_df.sample(frac=0.05, random_state=42)\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Train: {len(train_df):,} (70%)\")\n",
    "print(f\"  Val:   {len(val_df):,} (15%)\")\n",
    "print(f\"  Test:  {len(test_df):,} (15%)\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AbAgDataset(train_df)\n",
    "val_dataset_quick = AbAgDataset(val_df_quick)\n",
    "val_dataset_full = AbAgDataset(val_df)\n",
    "test_dataset = AbAgDataset(test_df)\n",
    "\n",
    "# Create data loaders with bucketing\n",
    "print(\"\\nCreating data loaders...\")\n",
    "train_sampler = BucketBatchSampler(train_dataset, CONFIG['batch_size'], drop_last=True)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_sampler=train_sampler, num_workers=4,\n",
    "    prefetch_factor=4, pin_memory=True, persistent_workers=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader_quick = DataLoader(\n",
    "    val_dataset_quick, batch_size=CONFIG['batch_size']*2, shuffle=False,\n",
    "    num_workers=4, pin_memory=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader_full = DataLoader(\n",
    "    val_dataset_full, batch_size=CONFIG['batch_size']*2, shuffle=False,\n",
    "    num_workers=4, pin_memory=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=CONFIG['batch_size']*2, shuffle=False,\n",
    "    num_workers=4, pin_memory=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nInitializing model...\")\n",
    "model = IgT5ESM2Model(\n",
    "    dropout=CONFIG['dropout'], use_quantization=True, use_checkpointing=True\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = FocalMSELoss(gamma=2.0, label_smoothing=0.05)\n",
    "try:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay'], fused=True\n",
    "    )\n",
    "    print(\"Using fused optimizer\")\n",
    "except:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "\n",
    "# LR scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epochs'])\n",
    "\n",
    "# Setup\n",
    "output_dir = Path(CONFIG['output_dir'])\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "early_stopping = EarlyStopping(patience=CONFIG['early_stopping_patience'])\n",
    "\n",
    "start_epoch = 0\n",
    "best_spearman = -1\n",
    "\n",
    "# Try to resume from checkpoint\n",
    "checkpoint_path = output_dir / 'checkpoint_latest.pth'\n",
    "if checkpoint_path.exists():\n",
    "    print(f\"\\nResuming from checkpoint...\")\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    try:\n",
    "        model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        start_epoch = ckpt['epoch'] + 1\n",
    "        best_spearman = ckpt.get('best_val_spearman', -1)\n",
    "        print(f\"Resumed from epoch {start_epoch}, best Spearman: {best_spearman:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load checkpoint: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Starting training for {CONFIG['epochs']} epochs...\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, CONFIG['epochs']):\n",
    "    print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Cleanup\n",
    "    if epoch > 0:\n",
    "        cleanup_disk_space()\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, batch in pbar:\n",
    "        targets = batch['pKd'].to(device, non_blocking=True)\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            predictions = model(batch['antibody_seqs'], batch['antigen_seqs'], device)\n",
    "            loss = criterion(predictions, targets) / CONFIG['accumulation_steps']\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if (batch_idx + 1) % CONFIG['accumulation_steps'] == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        total_loss += loss.item() * CONFIG['accumulation_steps']\n",
    "        pbar.set_postfix({'loss': f'{loss.item() * CONFIG[\"accumulation_steps\"]:.4f}'})\n",
    "        \n",
    "        # Save checkpoint every 500 batches\n",
    "        if (batch_idx + 1) % 500 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch, 'batch_idx': batch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_spearman': best_spearman\n",
    "            }, output_dir / 'checkpoint_latest.pth')\n",
    "    \n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    if (epoch + 1) % CONFIG['validation_frequency'] == 0:\n",
    "        print(\"\\nValidating...\")\n",
    "        val_metrics = quick_eval(model, val_loader_quick, device)\n",
    "        print(f\"Val Spearman: {val_metrics['spearman']:.4f} | Recall@pKd>=9: {val_metrics['recall_pkd9']:.1f}%\")\n",
    "        \n",
    "        if val_metrics['spearman'] > best_spearman:\n",
    "            best_spearman = val_metrics['spearman']\n",
    "            torch.save({\n",
    "                'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "                'best_val_spearman': best_spearman\n",
    "            }, output_dir / 'best_model.pth')\n",
    "            print(f\"Saved best model (Spearman: {best_spearman:.4f})\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping(val_metrics['spearman'], epoch):\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(f\"Train Loss: {train_loss:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save epoch checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_spearman': best_spearman\n",
    "    }, output_dir / 'checkpoint_latest.pth')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(f\"Best Validation Spearman: {best_spearman:.4f}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate\n",
    "print(\"\\nLoading best model for final evaluation...\")\n",
    "best_model_path = output_dir / 'best_model.pth'\n",
    "if best_model_path.exists():\n",
    "    ckpt = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {ckpt.get('epoch', 'unknown')+1}\")\n",
    "\n",
    "# Full validation evaluation\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FULL VALIDATION SET EVALUATION\")\n",
    "print(f\"{'='*70}\")\n",
    "val_metrics, val_preds, val_targets = full_eval(model, val_loader_full, device, \"Validation\")\n",
    "print(f\"\\nValidation Results ({len(val_targets):,} samples):\")\n",
    "print(f\"  Spearman: {val_metrics['spearman']:.4f}\")\n",
    "print(f\"  Pearson:  {val_metrics['pearson']:.4f}\")\n",
    "print(f\"  RMSE:     {val_metrics['rmse']:.4f}\")\n",
    "print(f\"  MAE:      {val_metrics['mae']:.4f}\")\n",
    "print(f\"  R2:       {val_metrics['r2']:.4f}\")\n",
    "print(f\"  Recall@pKd>=9: {val_metrics['recall_pkd9']:.1f}%\")\n",
    "\n",
    "# Test set evaluation\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TEST SET EVALUATION (UNSEEN DATA)\")\n",
    "print(f\"{'='*70}\")\n",
    "test_metrics, test_preds, test_targets = full_eval(model, test_loader, device, \"Test\")\n",
    "print(f\"\\nTest Results ({len(test_targets):,} samples):\")\n",
    "print(f\"  Spearman: {test_metrics['spearman']:.4f}  <- TRUE PERFORMANCE\")\n",
    "print(f\"  Pearson:  {test_metrics['pearson']:.4f}\")\n",
    "print(f\"  RMSE:     {test_metrics['rmse']:.4f}\")\n",
    "print(f\"  MAE:      {test_metrics['mae']:.4f}\")\n",
    "print(f\"  R2:       {test_metrics['r2']:.4f}\")\n",
    "print(f\"  Recall@pKd>=9: {test_metrics['recall_pkd9']:.1f}%\")\n",
    "\n",
    "# Save predictions\n",
    "pd.DataFrame({'true_pKd': val_targets, 'pred_pKd': val_preds}).to_csv(output_dir / 'val_predictions.csv', index=False)\n",
    "pd.DataFrame({'true_pKd': test_targets, 'pred_pKd': test_preds}).to_csv(output_dir / 'test_predictions.csv', index=False)\n",
    "\n",
    "# Save metrics\n",
    "with open(output_dir / 'final_metrics.json', 'w') as f:\n",
    "    json.dump({'validation': val_metrics, 'test': test_metrics}, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(f\"Results saved to: {output_dir}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Results (Optional)\n",
    "\n",
    "Run this cell to download the trained model and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all output files\n",
    "print(\"Output files:\")\n",
    "for f in sorted(output_dir.glob('*')):\n",
    "    size_mb = f.stat().st_size / 1e6\n",
    "    print(f\"  {f.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nFiles are saved in Google Drive: {output_dir}\")\n",
    "print(\"You can download them directly from Drive or use:\")\n",
    "print(\"  from google.colab import files\")\n",
    "print(\"  files.download('outputs/best_model.pth')\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
