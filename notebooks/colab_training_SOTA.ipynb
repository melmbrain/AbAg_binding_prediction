{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-of-the-Art: IgT5 + ESM-2 for Antibody-Antigen Binding\n",
    "\n",
    "**Training time**: ~4-5 days on Colab T4 GPU\n",
    "\n",
    "**Architecture** (Based on latest 2024-2025 research):\n",
    "- **Antibody**: IgT5 embeddings (512-dim)\n",
    "  - Published: December 2024 (Kenlay et al., PLOS Computational Biology)\n",
    "  - Best binding affinity prediction: RÂ² 0.297-0.306\n",
    "  - Trained on 2 billion unpaired + 2 million paired sequences\n",
    "  \n",
    "- **Antigen**: ESM-2 embeddings (1280-dim)\n",
    "  - Best epitope prediction: AUC 0.76-0.789\n",
    "  - Standard choice in 2024-2025 papers\n",
    "  - Proven performance on binding tasks\n",
    "\n",
    "**Expected Performance**:\n",
    "- Spearman: 0.60-0.70\n",
    "- Recall@pKdâ‰¥9: 40-60% (vs 14% baseline)\n",
    "- RMSE: 1.25-1.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/AbAg_Training')\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.57.1 torch pandas scipy scikit-learn tqdm sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_file = 'agab_phase2_full.csv'\n",
    "\n",
    "if Path(data_file).exists():\n",
    "    size_mb = Path(data_file).stat().st_size / (1024 * 1024)\n",
    "    print(f\"âœ“ Data file found: {data_file}\")\n",
    "    print(f\"  Size: {size_mb:.1f} MB\")\n",
    "    \n",
    "    # Check first few rows\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(data_file, nrows=5)\n",
    "    print(f\"\\nâœ“ Data preview:\")\n",
    "    print(df[['antibody_sequence', 'antigen_sequence', 'pKd']].head())\n",
    "else:\n",
    "    print(f\"âœ— Data file not found: {data_file}\")\n",
    "    print(f\"  Please upload {data_file} to Google Drive/AbAg_Training/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile model_igt5_esm2.py\n\"\"\"\nState-of-the-Art: IgT5 + ESM-2 Hybrid Model\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom transformers import T5EncoderModel, T5Tokenizer, AutoModel, AutoTokenizer\n\n\nclass IgT5ESM2Model(nn.Module):\n    \"\"\"\n    State-of-the-art hybrid:\n    - IgT5 for antibody (auto-detect dimension)\n    - ESM-2 for antigen (1280-dim)\n    \"\"\"\n\n    def __init__(self, dropout=0.3, freeze_encoders=True):\n        super().__init__()\n\n        print(\"Loading IgT5 for antibody...\")\n        self.igt5_tokenizer = T5Tokenizer.from_pretrained(\"Exscientia/IgT5\", do_lower_case=False)\n        self.igt5_model = T5EncoderModel.from_pretrained(\"Exscientia/IgT5\")\n\n        print(\"Loading ESM-2 for antigen...\")\n        self.esm2_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n        self.esm2_model = AutoModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n\n        if freeze_encoders:\n            for param in self.igt5_model.parameters():\n                param.requires_grad = False\n            for param in self.esm2_model.parameters():\n                param.requires_grad = False\n\n        # Auto-detect actual dimensions from model configs\n        igt5_dim = self.igt5_model.config.d_model  # Get actual IgT5 dimension\n        esm2_dim = self.esm2_model.config.hidden_size  # 1280 for t33_650M\n        combined_dim = igt5_dim + esm2_dim\n\n        print(f\"\\nArchitecture: Antibody {igt5_dim}D + Antigen {esm2_dim}D = {combined_dim}D\\n\")\n\n        self.regressor = nn.Sequential(\n            nn.Linear(combined_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.LayerNorm(1024),\n            nn.Linear(1024, 512),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.LayerNorm(512),\n            nn.Linear(512, 256),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.LayerNorm(256),\n            nn.Linear(256, 128),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1)\n        )\n\n    def get_antibody_embedding(self, antibody_seq, device):\n        inputs = self.igt5_tokenizer(\n            antibody_seq,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=512\n        ).to(device)\n\n        with torch.no_grad():\n            outputs = self.igt5_model(**inputs)\n            ab_emb = outputs.last_hidden_state.mean(dim=1)\n\n        return ab_emb.squeeze(0)\n\n    def get_antigen_embedding(self, antigen_seq, device):\n        inputs = self.esm2_tokenizer(\n            antigen_seq,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=512\n        ).to(device)\n\n        with torch.no_grad():\n            outputs = self.esm2_model(**inputs)\n            ag_emb = outputs.last_hidden_state[:, 0, :]\n\n        return ag_emb.squeeze(0)\n\n    def forward(self, antibody_seqs, antigen_seqs, device):\n        ab_embeddings = []\n        for ab_seq in antibody_seqs:\n            ab_emb = self.get_antibody_embedding(ab_seq, device)\n            ab_embeddings.append(ab_emb)\n        ab_embeddings = torch.stack(ab_embeddings).to(device)\n\n        ag_embeddings = []\n        for ag_seq in antigen_seqs:\n            ag_emb = self.get_antigen_embedding(ag_seq, device)\n            ag_embeddings.append(ag_emb)\n        ag_embeddings = torch.stack(ag_embeddings).to(device)\n\n        combined = torch.cat([ab_embeddings, ag_embeddings], dim=1)\n        predictions = self.regressor(combined).squeeze(-1)\n\n        return predictions\n\n\nclass FocalMSELoss(nn.Module):\n    def __init__(self, gamma=2.0):\n        super().__init__()\n        self.gamma = gamma\n\n    def forward(self, pred, target):\n        mse = (pred - target) ** 2\n        focal_weight = (1 + mse) ** self.gamma\n        return (focal_weight * mse).mean()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_colab.py\n",
    "\"\"\"\n",
    "Training script for IgT5 + ESM-2\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "from model_igt5_esm2 import IgT5ESM2Model, FocalMSELoss\n",
    "\n",
    "\n",
    "class AbAgDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'antibody_sequence': self.df.iloc[idx]['antibody_sequence'],\n",
    "            'antigen_sequence': self.df.iloc[idx]['antigen_sequence'],\n",
    "            'pKd': torch.tensor(self.df.iloc[idx]['pKd'], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    antibody_seqs = [item['antibody_sequence'] for item in batch]\n",
    "    antigen_seqs = [item['antigen_sequence'] for item in batch]\n",
    "    pKds = torch.stack([item['pKd'] for item in batch])\n",
    "    return {'antibody_seqs': antibody_seqs, 'antigen_seqs': antigen_seqs, 'pKd': pKds}\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        antibody_seqs = batch['antibody_seqs']\n",
    "        antigen_seqs = batch['antigen_seqs']\n",
    "        targets = batch['pKd'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            predictions = model(antibody_seqs, antigen_seqs, device)\n",
    "            loss = criterion(predictions, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.2e}'})\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            antibody_seqs = batch['antibody_seqs']\n",
    "            antigen_seqs = batch['antigen_seqs']\n",
    "            batch_targets = batch['pKd'].to(device)\n",
    "\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                batch_predictions = model(antibody_seqs, antigen_seqs, device)\n",
    "\n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "            targets.extend(batch_targets.cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(targets, predictions))\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    spearman = stats.spearmanr(targets, predictions)[0]\n",
    "\n",
    "    strong_binders = targets >= 9.0\n",
    "    predicted_strong = predictions >= 9.0\n",
    "    recall_pkd9 = (strong_binders & predicted_strong).sum() / strong_binders.sum() if strong_binders.sum() > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'spearman': spearman,\n",
    "        'recall_pkd9': recall_pkd9 * 100\n",
    "    }\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nDevice: {device}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\\n\")\n",
    "\n",
    "    df = pd.read_csv(args.data)\n",
    "    print(f\"Loaded {len(df):,} samples\\n\")\n",
    "\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "    print(f\"Train: {len(train_df):,} | Val: {len(val_df):,} | Test: {len(test_df):,}\\n\")\n",
    "\n",
    "    train_dataset = AbAgDataset(train_df)\n",
    "    val_dataset = AbAgDataset(val_df)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n",
    "                             num_workers=2, collate_fn=collate_fn, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n",
    "                           num_workers=2, collate_fn=collate_fn, pin_memory=True)\n",
    "\n",
    "    model = IgT5ESM2Model(dropout=args.dropout, freeze_encoders=True).to(device)\n",
    "    criterion = FocalMSELoss(gamma=args.focal_gamma)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    start_epoch = 0\n",
    "    best_spearman = -1\n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    if args.resume and Path(args.resume).exists():\n",
    "        checkpoint = torch.load(args.resume, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_spearman = checkpoint.get('best_val_spearman', -1)\n",
    "        print(f\"Resuming from epoch {start_epoch}, Spearman: {best_spearman:.4f}\\n\")\n",
    "\n",
    "    print(f\"Starting training for {args.epochs} epochs...\\n\")\n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val RMSE: {val_metrics['rmse']:.4f} | Spearman: {val_metrics['spearman']:.4f}\")\n",
    "        print(f\"Val Recall@pKdâ‰¥9: {val_metrics['recall_pkd9']:.2f}%\")\n",
    "\n",
    "        if val_metrics['spearman'] > best_spearman:\n",
    "            best_spearman = val_metrics['spearman']\n",
    "            torch.save(model.state_dict(), output_dir / 'best_model.pth')\n",
    "            print(\"âœ“ Saved best model\")\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'best_val_spearman': best_spearman,\n",
    "            'val_metrics': val_metrics\n",
    "        }\n",
    "        torch.save(checkpoint, output_dir / 'checkpoint_latest.pth')\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training complete! Best Spearman: {best_spearman:.4f}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', type=str, required=True)\n",
    "    parser.add_argument('--output_dir', type=str, default='outputs_sota')\n",
    "    parser.add_argument('--resume', type=str, default=None)\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--batch_size', type=int, default=8)\n",
    "    parser.add_argument('--lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.01)\n",
    "    parser.add_argument('--dropout', type=float, default=0.3)\n",
    "    parser.add_argument('--focal_gamma', type=float, default=2.0)\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Start Training ðŸš€\n",
    "\n",
    "**Expected**: 4-5 days on Colab T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_colab.py \\\n",
    "  --data agab_phase2_full.csv \\\n",
    "  --epochs 50 \\\n",
    "  --batch_size 8 \\\n",
    "  --focal_gamma 2.0 \\\n",
    "  --output_dir outputs_sota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current progress\n",
    "import torch\n",
    "\n",
    "checkpoint = torch.load('outputs_sota/checkpoint_latest.pth', map_location='cpu')\n",
    "print(f\"Epoch: {checkpoint['epoch'] + 1}/50\")\n",
    "print(f\"Best Spearman: {checkpoint['best_val_spearman']:.4f}\")\n",
    "print(f\"Current Recall@pKdâ‰¥9: {checkpoint['val_metrics']['recall_pkd9']:.2f}%\")\n",
    "print(f\"\\nTarget: Spearman 0.60-0.70, Recall 40-60%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download trained models\n",
    "from google.colab import files\n",
    "\n",
    "files.download('outputs_sota/best_model.pth')\n",
    "files.download('outputs_sota/checkpoint_latest.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Why This Architecture?\n",
    "\n",
    "**IgT5 for Antibody** (December 2024):\n",
    "- Trained on 2 billion antibody sequences\n",
    "- Best binding affinity prediction: RÂ² 0.297-0.306\n",
    "- Paired heavy+light training captures chain interactions\n",
    "- Outperforms AntiBERTy, AbLang, IgFold\n",
    "\n",
    "**ESM-2 for Antigen** (2024-2025):\n",
    "- State-of-the-art epitope prediction: AUC 0.76-0.789\n",
    "- Standard choice in recent papers (EpiGraph, CALIBER, EPP)\n",
    "- Rich 1280-dim embeddings capture evolutionary features\n",
    "- Proven performance on antibody-antigen binding tasks\n",
    "\n",
    "**Combined**: State-of-the-art architecture based on latest research!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}