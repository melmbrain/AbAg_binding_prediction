{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antibody-Antigen Binding Prediction Training on Colab\n",
    "\n",
    "**Training time**: ~3-4 days (vs 36 days on RTX 2060)\n",
    "\n",
    "**Setup**: Follow instructions below to connect Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/AbAg_Training')\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.57.1 torch pandas scipy scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Tokenization Cache (One-time, ~10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_tokenization_cache.py\n",
    "\"\"\"\n",
    "SQLite-Based Tokenization Cache\n",
    "\"\"\"\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def create_sequence_hash(sequence):\n",
    "    \"\"\"Create hash for sequence to use as key\"\"\"\n",
    "    return hashlib.md5(sequence.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def create_tokenization_db(db_path):\n",
    "    \"\"\"Create SQLite database for tokenized sequences\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS sequences (\n",
    "            seq_hash TEXT PRIMARY KEY,\n",
    "            input_ids BLOB,\n",
    "            attention_mask BLOB\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_hash ON sequences(seq_hash)')\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "\n",
    "def tokenize_and_cache(csv_path, db_path, max_length=512, batch_size=100):\n",
    "    \"\"\"Tokenize all sequences and cache to SQLite\"\"\"\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Total samples: {len(df):,}\")\n",
    "    \n",
    "    print(\"\\nInitializing tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "    \n",
    "    print(\"\\nCreating cache database...\")\n",
    "    conn = create_tokenization_db(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get unique sequences\n",
    "    unique_sequences = set()\n",
    "    for seq in df['antibody_sequence']:\n",
    "        unique_sequences.add(seq)\n",
    "    for seq in df['antigen_sequence']:\n",
    "        unique_sequences.add(seq)\n",
    "    \n",
    "    unique_sequences = list(unique_sequences)\n",
    "    print(f\"\\nUnique sequences to tokenize: {len(unique_sequences):,}\")\n",
    "    \n",
    "    # Tokenize in batches\n",
    "    print(\"\\nTokenizing and caching...\")\n",
    "    for i in tqdm(range(0, len(unique_sequences), batch_size)):\n",
    "        batch = unique_sequences[i:i+batch_size]\n",
    "        \n",
    "        for seq in batch:\n",
    "            seq_hash = create_sequence_hash(seq)\n",
    "            \n",
    "            # Check if already cached\n",
    "            cursor.execute('SELECT 1 FROM sequences WHERE seq_hash = ?', (seq_hash,))\n",
    "            if cursor.fetchone():\n",
    "                continue\n",
    "            \n",
    "            # Tokenize\n",
    "            tokens = tokenizer(\n",
    "                seq,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Convert to numpy for storage\n",
    "            input_ids = tokens['input_ids'].squeeze(0).numpy()\n",
    "            attention_mask = tokens['attention_mask'].squeeze(0).numpy()\n",
    "            \n",
    "            # Store as bytes\n",
    "            input_ids_blob = input_ids.tobytes()\n",
    "            attention_mask_blob = attention_mask.tobytes()\n",
    "            \n",
    "            cursor.execute('''\n",
    "                INSERT INTO sequences (seq_hash, input_ids, attention_mask)\n",
    "                VALUES (?, ?, ?)\n",
    "            ''', (seq_hash, input_ids_blob, attention_mask_blob))\n",
    "        \n",
    "        if (i + batch_size) % 1000 == 0:\n",
    "            conn.commit()\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Tokenization cache created: {db_path}\")\n",
    "    print(f\"   Size: {Path(db_path).stat().st_size / (1024*1024):.1f} MB\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', type=str, required=True)\n",
    "    parser.add_argument('--output', type=str, default='tokenization_cache.db')\n",
    "    parser.add_argument('--max_length', type=int, default=512)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    tokenize_and_cache(args.data, args.output, args.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cache creation (only need to do this once!)\n",
    "!python create_tokenization_cache.py \\\n",
    "  --data agab_phase2_full.csv \\\n",
    "  --output tokenization_cache.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile train_colab.py\n\"\"\"\nColab Training Script with SQLite Cache\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModel\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom tqdm import tqdm\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nimport sqlite3\nimport hashlib\n\n\nclass FocalMSELoss(nn.Module):\n    def __init__(self, gamma=2.0):\n        super().__init__()\n        self.gamma = gamma\n\n    def forward(self, pred, target):\n        mse = (pred - target) ** 2\n        focal_weight = (1 + mse) ** self.gamma\n        return (focal_weight * mse).mean()\n\n\nclass CachedAbAgDataset(Dataset):\n    def __init__(self, df, cache_db_path, max_length=512):\n        self.df = df.reset_index(drop=True)\n        self.cache_db_path = cache_db_path\n        self.max_length = max_length\n        self.conn = None\n        print(f\"Dataset: {len(df)} samples (SQLite cache)\")\n\n    def _get_connection(self):\n        if self.conn is None:\n            self.conn = sqlite3.connect(self.cache_db_path, check_same_thread=False)\n        return self.conn\n\n    def _get_tokens(self, sequence):\n        seq_hash = hashlib.md5(sequence.encode()).hexdigest()\n        conn = self._get_connection()\n        cursor = conn.cursor()\n        cursor.execute('SELECT input_ids, attention_mask FROM sequences WHERE seq_hash = ?', (seq_hash,))\n        result = cursor.fetchone()\n        \n        if result is None:\n            raise ValueError(f\"Sequence not found in cache: {seq_hash}\")\n        \n        input_ids = np.frombuffer(result[0], dtype=np.int64)\n        attention_mask = np.frombuffer(result[1], dtype=np.int64)\n        \n        return torch.from_numpy(input_ids.copy()), torch.from_numpy(attention_mask.copy())\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        ab_seq = self.df.iloc[idx]['antibody_sequence']\n        ag_seq = self.df.iloc[idx]['antigen_sequence']\n        \n        ab_input_ids, ab_attention_mask = self._get_tokens(ab_seq)\n        ag_input_ids, ag_attention_mask = self._get_tokens(ag_seq)\n        \n        return {\n            'antibody_input_ids': ab_input_ids,\n            'antibody_attention_mask': ab_attention_mask,\n            'antigen_input_ids': ag_input_ids,\n            'antigen_attention_mask': ag_attention_mask,\n            'pKd': torch.tensor(self.df.iloc[idx]['pKd'], dtype=torch.float32)\n        }\n\n\nclass AbAgModel(nn.Module):\n    def __init__(self, model_name=\"facebook/esm2_t33_650M_UR50D\", dropout=0.2):\n        super().__init__()\n        self.esm = AutoModel.from_pretrained(model_name)\n        \n        for param in self.esm.parameters():\n            param.requires_grad = False\n        \n        hidden_size = self.esm.config.hidden_size\n        self.regressor = nn.Sequential(\n            nn.Linear(hidden_size * 2, 512),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, ab_input_ids, ab_attention_mask, ag_input_ids, ag_attention_mask):\n        ab_emb = self.esm(input_ids=ab_input_ids, attention_mask=ab_attention_mask).last_hidden_state[:, 0, :]\n        ag_emb = self.esm(input_ids=ag_input_ids, attention_mask=ag_attention_mask).last_hidden_state[:, 0, :]\n        combined = torch.cat([ab_emb, ag_emb], dim=1)\n        return self.regressor(combined).squeeze(-1)\n\n\ndef train_epoch(model, loader, optimizer, criterion, device, scaler):\n    model.train()\n    total_loss = 0\n    \n    pbar = tqdm(loader, desc=\"Training\")\n    for batch in pbar:\n        antibody_input_ids = batch['antibody_input_ids'].to(device)\n        antibody_attention_mask = batch['antibody_attention_mask'].to(device)\n        antigen_input_ids = batch['antigen_input_ids'].to(device)\n        antigen_attention_mask = batch['antigen_attention_mask'].to(device)\n        targets = batch['pKd'].to(device)\n        \n        optimizer.zero_grad()\n        \n        with torch.amp.autocast('cuda'):\n            predictions = model(antibody_input_ids, antibody_attention_mask,\n                              antigen_input_ids, antigen_attention_mask)\n            loss = criterion(predictions, targets)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        total_loss += loss.item()\n        pbar.set_postfix({'loss': f'{loss.item():.2e}'})\n    \n    return total_loss / len(loader)\n\n\ndef evaluate(model, loader, device):\n    model.eval()\n    predictions = []\n    targets = []\n    \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Evaluating\"):\n            antibody_input_ids = batch['antibody_input_ids'].to(device)\n            antibody_attention_mask = batch['antibody_attention_mask'].to(device)\n            antigen_input_ids = batch['antigen_input_ids'].to(device)\n            antigen_attention_mask = batch['antigen_attention_mask'].to(device)\n            batch_targets = batch['pKd'].to(device)\n            \n            with torch.amp.autocast('cuda'):\n                batch_predictions = model(antibody_input_ids, antibody_attention_mask,\n                                        antigen_input_ids, antigen_attention_mask)\n            \n            predictions.extend(batch_predictions.cpu().numpy())\n            targets.extend(batch_targets.cpu().numpy())\n    \n    predictions = np.array(predictions)\n    targets = np.array(targets)\n    \n    rmse = np.sqrt(mean_squared_error(targets, predictions))\n    mae = mean_absolute_error(targets, predictions)\n    r2 = r2_score(targets, predictions)\n    spearman = stats.spearmanr(targets, predictions)[0]\n    \n    # Recall for strong binders\n    strong_binders = targets >= 9.0\n    predicted_strong = predictions >= 9.0\n    recall = (strong_binders & predicted_strong).sum() / strong_binders.sum() if strong_binders.sum() > 0 else 0\n    \n    return {\n        'rmse': rmse,\n        'mae': mae,\n        'r2': r2,\n        'spearman': spearman,\n        'recall_pkd9': recall * 100,\n        'predictions': predictions,\n        'targets': targets\n    }\n\n\ndef main(args):\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Device: {device}\")\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    \n    df = pd.read_csv(args.data)\n    print(f\"\\nLoaded {len(df):,} samples\")\n    \n    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n    \n    print(f\"Train: {len(train_df):,} | Val: {len(val_df):,} | Test: {len(test_df):,}\")\n    \n    train_dataset = CachedAbAgDataset(train_df, args.cache_db)\n    val_dataset = CachedAbAgDataset(val_df, args.cache_db)\n    \n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, \n                             num_workers=2, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=args.batch_size * 2, shuffle=False,\n                           num_workers=2, pin_memory=True)\n    \n    model = AbAgModel(dropout=args.dropout).to(device)\n    criterion = FocalMSELoss(gamma=args.focal_gamma)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n    scaler = torch.amp.GradScaler('cuda')\n    \n    start_epoch = 0\n    best_spearman = -1\n    output_dir = Path(args.output_dir)\n    output_dir.mkdir(exist_ok=True)\n    \n    # Resume from checkpoint if provided\n    if args.resume and Path(args.resume).exists():\n        print(f\"\\nüîÑ Resuming from checkpoint: {args.resume}\")\n        checkpoint = torch.load(args.resume, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        if 'scheduler_state_dict' in checkpoint:\n            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_spearman = checkpoint.get('best_val_spearman', -1)\n        print(f\"‚úì Resuming from epoch {start_epoch}\")\n        print(f\"‚úì Best Spearman: {best_spearman:.4f}\")\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Starting training for {args.epochs} epochs (from epoch {start_epoch})\")\n    print(f\"{'='*70}\\n\")\n    \n    for epoch in range(start_epoch, args.epochs):\n        print(f\"\\nEpoch {epoch+1}/{args.epochs}\")\n        \n        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, scaler)\n        val_metrics = evaluate(model, val_loader, device)\n        scheduler.step()\n        \n        print(f\"Train Loss: {train_loss:.4f}\")\n        print(f\"Val RMSE: {val_metrics['rmse']:.4f} | MAE: {val_metrics['mae']:.4f}\")\n        print(f\"Val Spearman: {val_metrics['spearman']:.4f} | R¬≤: {val_metrics['r2']:.4f}\")\n        print(f\"Val Recall@pKd‚â•9: {val_metrics['recall_pkd9']:.2f}%\")\n        \n        if val_metrics['spearman'] > best_spearman:\n            best_spearman = val_metrics['spearman']\n            torch.save(model.state_dict(), output_dir / 'best_model.pth')\n            print(\"‚úì Saved best model\")\n        \n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'best_val_spearman': best_spearman,\n            'val_metrics': val_metrics\n        }\n        torch.save(checkpoint, output_dir / 'checkpoint_latest.pth')\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Training complete! Best Spearman: {best_spearman:.4f}\")\n    print(f\"{'='*70}\")\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data', type=str, required=True)\n    parser.add_argument('--cache_db', type=str, required=True)\n    parser.add_argument('--output_dir', type=str, default='outputs_colab')\n    parser.add_argument('--resume', type=str, default=None, help='Path to checkpoint to resume from')\n    parser.add_argument('--epochs', type=int, default=50)\n    parser.add_argument('--batch_size', type=int, default=32)\n    parser.add_argument('--lr', type=float, default=1e-3)\n    parser.add_argument('--weight_decay', type=float, default=0.01)\n    parser.add_argument('--dropout', type=float, default=0.2)\n    parser.add_argument('--focal_gamma', type=float, default=2.0)\n    args = parser.parse_args()\n    \n    main(args)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Start Training üöÄ\n\n**Option A: Start from scratch**\n**Option B: Resume from your local checkpoint (epoch 5)**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Option A: Start from scratch (epoch 1)\n!python train_colab.py \\\n  --data agab_phase2_full.csv \\\n  --cache_db tokenization_cache.db \\\n  --epochs 50 \\\n  --batch_size 32 \\\n  --focal_gamma 2.0 \\\n  --output_dir outputs_colab"
  },
  {
   "cell_type": "markdown",
   "source": "## Verify Files Before Training",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Check file sizes and integrity\nimport os\nfrom pathlib import Path\n\nfiles_to_check = {\n    'agab_phase2_full.csv': 127 * 1024 * 1024,  # ~127 MB\n    'checkpoint_latest.pth': 2500 * 1024 * 1024,  # ~2.5 GB\n}\n\nprint(\"File verification:\")\nprint(\"-\" * 60)\nfor filename, expected_size in files_to_check.items():\n    if Path(filename).exists():\n        actual_size = Path(filename).stat().st_size\n        size_mb = actual_size / (1024 * 1024)\n        expected_mb = expected_size / (1024 * 1024)\n        status = \"‚úì\" if actual_size > expected_size * 0.95 else \"‚ö†Ô∏è SIZE MISMATCH\"\n        print(f\"{status} {filename}: {size_mb:.1f} MB (expected: {expected_mb:.1f} MB)\")\n        \n        # For checkpoint, try to load it\n        if filename.endswith('.pth') and actual_size > 1024:\n            try:\n                import torch\n                checkpoint = torch.load(filename, map_location='cpu', weights_only=False)\n                print(f\"   ‚úì Valid checkpoint - Epoch {checkpoint['epoch'] + 1}, Spearman: {checkpoint['best_val_spearman']:.4f}\")\n            except Exception as e:\n                print(f\"   ‚úó CORRUPTED: {str(e)[:100]}\")\n                print(f\"   ‚Üí Recommendation: Delete and re-upload, OR use Option A (start fresh)\")\n    else:\n        print(f\"‚úó {filename}: NOT FOUND\")\n        if filename == 'agab_phase2_full.csv':\n            print(f\"   ‚Üí REQUIRED: Please upload this file to continue!\")\n\nprint(\"-\" * 60)\nprint(\"\\nRecommendation:\")\nif not Path('agab_phase2_full.csv').exists():\n    print(\"‚ùå Cannot proceed - missing data file!\")\nelif not Path('checkpoint_latest.pth').exists():\n    print(\"‚úì Use Option A (start from scratch)\")\nelse:\n    # Check if checkpoint loads\n    try:\n        import torch\n        torch.load('checkpoint_latest.pth', map_location='cpu', weights_only=False)\n        print(\"‚úì Use Option B (resume from epoch 6)\")\n    except:\n        print(\"‚ö†Ô∏è  Checkpoint corrupted - Use Option A (start from scratch)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check checkpoint\n",
    "import torch\n",
    "checkpoint = torch.load('outputs_colab/checkpoint_latest.pth', map_location='cpu')\n",
    "print(f\"Epoch: {checkpoint['epoch'] + 1}\")\n",
    "print(f\"Best Spearman: {checkpoint['best_val_spearman']:.4f}\")\n",
    "print(f\"Latest metrics: {checkpoint['val_metrics']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download best model\n",
    "from google.colab import files\n",
    "files.download('outputs_colab/best_model.pth')\n",
    "files.download('outputs_colab/checkpoint_latest.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}