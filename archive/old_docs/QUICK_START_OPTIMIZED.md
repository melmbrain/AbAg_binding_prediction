# Quick Start - Optimized Training

## What's New?

Based on 2024-2025 research, I've created a **much faster and more accurate** training approach:

### Speed Improvements:
- âš¡ **3-10x faster** with FlashAttention
- âš¡ **1.5-2x faster** with mixed precision (bfloat16)
- âš¡ **Combined: 4-6 hour training** (vs 15-20 hours)

### Accuracy Improvements:
- ðŸ“ˆ **Focal MSE Loss** - better on extreme values
- ðŸ“ˆ **Stratified sampling** - oversample strong binders
- ðŸ“ˆ **Better architecture** - LayerNorm + GELU
- ðŸ“ˆ **Expected: 30-40% recall** on strong binders (vs 17%)

---

## Installation

```bash
# Install required packages
pip install torch transformers pandas scipy scikit-learn tqdm

# Install FlashAttention (for speed boost)
pip install flash-attn --no-build-isolation
```

If FlashAttention installation fails, the script will still work (just slower).

---

## Usage

### Basic Usage (Default Settings)

```bash
python train_optimized_v1.py \
  --data /mnt/c/Users/401-24/Desktop/Ab_Ag_dataset/data/agab/agab_phase2_full.csv \
  --epochs 50 \
  --batch_size 16
```

### With Stratified Sampling (Recommended)

```bash
python train_optimized_v1.py \
  --data /mnt/c/Users/401-24/Desktop/Ab_Ag_dataset/data/agab/agab_phase2_full.csv \
  --epochs 50 \
  --batch_size 16 \
  --use_stratified_sampling \
  --focal_gamma 2.0
```

### Optimize for Your GPU

**If you have 16GB+ VRAM:**
```bash
python train_optimized_v1.py \
  --data /path/to/data.csv \
  --epochs 50 \
  --batch_size 32 \    # Larger batch
  --use_stratified_sampling
```

**If you have 8-12GB VRAM:**
```bash
python train_optimized_v1.py \
  --data /path/to/data.csv \
  --epochs 50 \
  --batch_size 8 \     # Smaller batch
  --use_stratified_sampling
```

---

## Expected Timeline

### For 159K samples:

| Step | Time | What's happening |
|------|------|------------------|
| Data loading | 1-2 min | Reading CSV, splitting data |
| Model loading | 1-2 min | Loading ESM-2 650M |
| **Training (50 epochs)** | **4-6 hours** | Main training loop |
| Evaluation | 5 min | Final test metrics |
| **TOTAL** | **~4-6 hours** | vs 15-20 hours before |

---

## What You'll Get

After training completes:

```
outputs_optimized_v1/
â”œâ”€â”€ best_model.pth           # Best model checkpoint
â”œâ”€â”€ results.json             # Performance metrics
â””â”€â”€ test_predictions.csv     # Predictions on test set
```

### Expected Performance:

| Metric | Current (49K) | Phase 1 (159K) | Target |
|--------|--------------|----------------|--------|
| **Spearman** | 0.49 | 0.55-0.60 | > 0.5 âœ“ |
| **RMSE** | 1.40 | 1.30-1.35 | < 1.4 âœ“ |
| **Recall@pKdâ‰¥9** | 17% | 30-40% | > 30% âœ“ |

**Improvement**: 2x better recall on strong binders!

---

## For Google Colab

Create a notebook with:

```python
# Cell 1: Install dependencies
!pip install flash-attn --no-build-isolation

# Cell 2: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Cell 3: Clone repository or upload script
# Upload train_optimized_v1.py to Colab

# Cell 4: Train
!python train_optimized_v1.py \
  --data /content/drive/MyDrive/AbAg_data/agab_phase2_full.csv \
  --epochs 50 \
  --batch_size 32 \
  --use_stratified_sampling \
  --output_dir /content/drive/MyDrive/AbAg_outputs_v1

# Cell 5: Check results
import json
with open('/content/drive/MyDrive/AbAg_outputs_v1/results.json') as f:
    results = json.load(f)
print(json.dumps(results, indent=2))
```

**Expected time on Colab T4**: 4-6 hours (fits in free tier 12h limit!)

---

## Troubleshooting

### "FlashAttention not available"
**Don't worry!** The script will automatically fall back to standard attention. You'll still get:
- Mixed precision speedup (1.5-2x)
- Focal loss improvements
- Better architecture

**To enable**: `pip install flash-attn --no-build-isolation` (requires CUDA)

### "Out of memory"
**Solution**: Reduce batch size
```bash
--batch_size 8  # or even 4
```

### "Training is slow"
**Check**:
1. GPU is being used: Look for "Device: cuda" in output
2. FlashAttention is enabled: Look for "âœ“ FlashAttention enabled"
3. Batch size is optimal: Try increasing if you have memory

### "Low recall on strong binders"
**Solutions**:
1. Use stratified sampling: `--use_stratified_sampling`
2. Increase focal gamma: `--focal_gamma 3.0`
3. Train longer: `--epochs 100`

---

## Comparison with Previous Approach

### Old Approach (COMPLETE_COLAB_TRAINING.py):
- âŒ 10-12 hours embedding generation
- âŒ 3-5 hours training
- âŒ Standard MSE loss
- âŒ No stratified sampling
- â±ï¸ **Total: 15-20 hours**

### New Approach (train_optimized_v1.py):
- âœ… FlashAttention (3-10x faster)
- âœ… Mixed precision (1.5-2x faster)
- âœ… Focal MSE loss (better extremes)
- âœ… Stratified sampling (better balance)
- âœ… End-to-end training (no separate embedding step)
- â±ï¸ **Total: 4-6 hours**

---

## Next Steps

### After Phase 1 completes:

**If results are good enough** (Recall > 40%, Spearman > 0.60):
- âœ“ You're done! Use the model.

**If you want even better results**:
- Try **Phase 2: LoRA Fine-Tuning** (see `MODERN_TRAINING_STRATEGY.md`)
  - 2-4 hours training
  - 50-70% recall on strong binders
  - Requires more coding

---

## Understanding the Results

After training, check `results.json`:

```json
{
  "test_rmse": 1.32,           // Lower is better (< 1.4 is good)
  "test_spearman": 0.58,       // Higher is better (> 0.5 is good)
  "test_recall_strong": 0.35,  // 35% recall on pKdâ‰¥9 (vs 17% before!)
  "training_time_hours": 4.5   // Much faster than before!
}
```

### Check Predictions by Range:

```python
import pandas as pd

pred_df = pd.read_csv('outputs_optimized_v1/test_predictions.csv')

# Strong binders
strong = pred_df[pred_df['true_pKd'] >= 9]
print(f"Strong binders: {len(strong)}")
print(f"Mean error: {strong['residual'].abs().mean():.3f}")

# Weak binders
weak = pred_df[pred_df['true_pKd'] == 6.0]
print(f"Weak binders: {len(weak)}")
print(f"Mean error: {weak['residual'].abs().mean():.3f}")
```

---

## FAQ

**Q: Do I need to generate embeddings separately?**
A: No! This script does everything end-to-end.

**Q: Can I use the 49K dataset first to test?**
A: Yes! It will finish in ~1 hour.

**Q: What if I don't have a GPU?**
A: Use Google Colab (free T4 GPU). Local CPU training would take days.

**Q: How is this different from the old approach?**
A: See comparison table above. Much faster and more accurate.

**Q: Can I resume if training is interrupted?**
A: Not in Phase 1. Use checkpoints in Phase 2.

**Q: What's the minimum GPU memory needed?**
A: 8GB (batch_size=8), but 16GB+ is better (batch_size=32)

---

## Ready to Start?

```bash
# Quick test on sample data (~30 min)
python train_optimized_v1.py \
  --data /path/to/sample.csv \
  --epochs 20 \
  --batch_size 16

# Full training on 159K dataset (~4-6 hours)
python train_optimized_v1.py \
  --data /path/to/agab_phase2_full.csv \
  --epochs 50 \
  --batch_size 16 \
  --use_stratified_sampling
```

**Good luck! ðŸš€**
