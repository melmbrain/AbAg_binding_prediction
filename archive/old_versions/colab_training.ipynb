{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# AbAg Binding Affinity Prediction - Colab Training\n",
    "\n",
    "**Fast GPU training on Google Colab**\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Enable GPU**: Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU (T4)\n",
    "2. **Upload data**: Upload `merged_with_all_features.csv` to your Google Drive\n",
    "3. **Run all cells**: Runtime â†’ Run all\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset Info:**\n",
    "- Total samples: 390,757\n",
    "- Samples with features: 330,762 (84.65%)\n",
    "- Very strong binders: 230\n",
    "- File size: 883 MB\n",
    "\n",
    "**Expected Training Time:**\n",
    "- 20 epochs: ~1.5-2 hours on T4 GPU\n",
    "- 100 epochs: ~7-10 hours on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Check GPU and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: No GPU detected! Enable GPU in Runtime settings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers scikit-learn pandas numpy tqdm matplotlib seaborn\n",
    "print(\"âœ… All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount_drive"
   },
   "source": [
    "## 2. Mount Google Drive and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"âœ… Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_paths"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up paths - MODIFY THIS to match your Google Drive location\n",
    "DRIVE_DATA_PATH = \"/content/drive/MyDrive/AbAg_data/merged_with_all_features.csv\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/AbAg_data/models\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Alternative: Copy data to Colab local storage for faster I/O\n",
    "LOCAL_DATA_PATH = \"/content/merged_with_all_features.csv\"\n",
    "\n",
    "print(f\"Data path: {DRIVE_DATA_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "if os.path.exists(DRIVE_DATA_PATH):\n",
    "    print(f\"âœ… Data file found! Size: {os.path.getsize(DRIVE_DATA_PATH) / 1e6:.1f} MB\")\n",
    "    \n",
    "    # Copy to local for faster training\n",
    "    print(\"Copying data to local storage for faster I/O...\")\n",
    "    !cp \"{DRIVE_DATA_PATH}\" \"{LOCAL_DATA_PATH}\"\n",
    "    print(\"âœ… Data copied to local storage!\")\n",
    "    \n",
    "    DATA_PATH = LOCAL_DATA_PATH\n",
    "else:\n",
    "    print(f\"âŒ Data file not found at: {DRIVE_DATA_PATH}\")\n",
    "    print(\"\\nPlease upload 'merged_with_all_features.csv' to your Google Drive and update DRIVE_DATA_PATH above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_code"
   },
   "source": [
    "## 3. Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset_class"
   },
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class AffinityDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "print(\"âœ… Dataset class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_class"
   },
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "class AffinityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=150, hidden_dims=[256, 128], dropout=0.3):\n",
    "        super(AffinityPredictor, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "print(\"âœ… Model class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weighted_mse"
   },
   "outputs": [],
   "source": "# Weighted MSE Loss\nclass WeightedMSELoss(nn.Module):\n    def __init__(self, bin_weights, bins_edges):\n        super().__init__()\n        self.bin_weights = bin_weights\n        self.bins = bins_edges\n    \n    def forward(self, predictions, targets):\n        # Calculate weights for each sample\n        weights = torch.ones_like(targets)\n        for i, (low, high) in enumerate(zip(self.bins[:-1], self.bins[1:])):\n            mask = (targets >= low) & (targets < high)\n            weights[mask] = self.bin_weights[i]\n        \n        # Weighted MSE\n        mse = (predictions - targets) ** 2\n        weighted_mse = mse * weights\n        return weighted_mse.mean()\n\nprint(\"âœ… Weighted MSE loss defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_data"
   },
   "source": [
    "## 4. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dataset"
   },
   "outputs": [],
   "source": "print(\"Loading dataset...\")\ndf = pd.read_csv(DATA_PATH, low_memory=False)\nprint(f\"âœ… Loaded {len(df):,} samples\")\n\n# Filter samples with features\npca_cols = [f'esm2_pca_{i}' for i in range(150)]\ndf_with_features = df[df[pca_cols[0]].notna()].copy()\nprint(f\"âœ… Samples with features: {len(df_with_features):,}\")\n\n# Create affinity bins\nBINS = [0, 5, 7, 9, 11, 16]\nBIN_LABELS = ['very_weak', 'weak', 'moderate', 'strong', 'very_strong']\ndf_with_features['affinity_bin'] = pd.cut(\n    df_with_features['pKd'], bins=BINS, labels=BIN_LABELS, include_lowest=True\n)\n\n# Show distribution\nprint(\"\\nAffinity Distribution:\")\nfor label in BIN_LABELS:\n    count = (df_with_features['affinity_bin'] == label).sum()\n    pct = count / len(df_with_features) * 100\n    print(f\"  {label:<15}: {count:6,} ({pct:5.2f}%)\")\n\nprint(f\"\\nTotal: {len(df_with_features):,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_data"
   },
   "outputs": [],
   "source": "# Extract features and labels\nX = df_with_features[pca_cols].values\ny = df_with_features['pKd'].values\n\n# Train/val/test split\nX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15/0.85, random_state=42)\n\nprint(f\"Train set: {len(X_train):,}\")\nprint(f\"Val set: {len(X_val):,}\")\nprint(f\"Test set: {len(X_test):,}\")\n\n# Calculate class weights\ny_train_binned = pd.cut(y_train, bins=BINS, labels=BIN_LABELS, include_lowest=True)\nbin_counts = y_train_binned.value_counts().sort_index()\ntotal_samples = len(y_train)\nbin_weights = {}\nfor label in BIN_LABELS:\n    count = bin_counts.get(label, 1)\n    bin_weights[label] = total_samples / (len(BIN_LABELS) * count)\n\nprint(\"\\nClass weights:\")\nfor label, weight in bin_weights.items():\n    print(f\"  {label:<15}: {weight:.2f}\")\n\n# Convert to tensor\nbin_weights_tensor = torch.FloatTensor([bin_weights[l] for l in BIN_LABELS]).cuda()\n\n# Create datasets\ntrain_dataset = AffinityDataset(X_train, y_train)\nval_dataset = AffinityDataset(X_val, y_val)\ntest_dataset = AffinityDataset(X_test, y_test)\n\nprint(\"\\nâœ… Data preparation complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_config"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 100  # Change to 20 for quick test\n",
    "BATCH_SIZE = 128  # Can use larger batch size on Colab\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataloaders"
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"âœ… Data loaders created!\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initialize_model"
   },
   "outputs": [],
   "source": "# Initialize model\nmodel = AffinityPredictor(input_dim=150, hidden_dims=[256, 128], dropout=0.3)\nmodel = model.to(DEVICE)\n\n# Loss and optimizer\ncriterion = WeightedMSELoss(bin_weights_tensor, BINS)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"âœ… Model initialized!\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\nprint(f\"\\nModel architecture:\")\nprint(model)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_loop"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_start = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
    "    \n",
    "    for features, labels in train_pbar:\n",
    "        features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(features)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_pbar:\n",
    "            features, labels = features.to(DEVICE), labels.to(DEVICE)\n",
    "            predictions = model(features)\n",
    "            loss = criterion(predictions, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "        }, f'{OUTPUT_DIR}/best_model.pth')\n",
    "        print(f\"  âœ… New best model saved! (val_loss: {val_loss:.4f})\")\n",
    "    \n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "        }, f'{OUTPUT_DIR}/checkpoint_epoch_{epoch+1}.pth')\n",
    "        print(f\"  ðŸ’¾ Checkpoint saved!\")\n",
    "\n",
    "total_time = time.time() - train_start\n",
    "print(f\"\\nâœ… Training complete! Total time: {total_time/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_losses"
   },
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'{OUTPUT_DIR}/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Training curves saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_best_model"
   },
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(f'{OUTPUT_DIR}/best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"âœ… Best model loaded from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"   Val loss: {checkpoint['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_evaluation"
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        features = features.to(DEVICE)\n",
    "        predictions = model(features)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_targets.extend(labels.numpy())\n",
    "\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_targets = np.array(test_targets)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(test_targets, test_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(test_targets, test_predictions)\n",
    "spearman = spearmanr(test_targets, test_predictions)[0]\n",
    "pearson = pearsonr(test_targets, test_predictions)[0]\n",
    "r2 = 1 - (np.sum((test_targets - test_predictions)**2) / np.sum((test_targets - test_targets.mean())**2))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RMSE:        {rmse:.4f}\")\n",
    "print(f\"MAE:         {mae:.4f}\")\n",
    "print(f\"Spearman Ï:  {spearman:.4f}\")\n",
    "print(f\"Pearson r:   {pearson:.4f}\")\n",
    "print(f\"RÂ²:          {r2:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "per_bin_metrics"
   },
   "outputs": [],
   "source": "# Per-bin metrics\ntest_df = pd.DataFrame({\n    'target': test_targets,\n    'prediction': test_predictions\n})\ntest_df['affinity_bin'] = pd.cut(test_df['target'], bins=BINS, labels=BIN_LABELS, include_lowest=True)\n\nprint(\"\\nPER-BIN PERFORMANCE:\")\nprint(\"=\"*60)\nprint(f\"{'Bin':<15} | {'Count':<8} | {'RMSE':<8} | {'MAE':<8}\")\nprint(\"-\"*60)\n\nfor label in BIN_LABELS:\n    bin_data = test_df[test_df['affinity_bin'] == label]\n    if len(bin_data) > 0:\n        bin_rmse = np.sqrt(mean_squared_error(bin_data['target'], bin_data['prediction']))\n        bin_mae = mean_absolute_error(bin_data['target'], bin_data['prediction'])\n        print(f\"{label:<15} | {len(bin_data):<8} | {bin_rmse:<8.4f} | {bin_mae:<8.4f}\")\n\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_predictions"
   },
   "outputs": [],
   "source": [
    "# Plot predictions vs targets\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(test_targets, test_predictions, alpha=0.3, s=10)\n",
    "plt.plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], 'r--', lw=2)\n",
    "plt.xlabel('True pKd')\n",
    "plt.ylabel('Predicted pKd')\n",
    "plt.title(f'Test Set Predictions (Spearman Ï = {spearman:.4f}, RMSE = {rmse:.4f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.savefig(f'{OUTPUT_DIR}/predictions_vs_targets.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Prediction plot saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "residuals_plot"
   },
   "outputs": [],
   "source": [
    "# Residuals plot\n",
    "residuals = test_predictions - test_targets\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Residuals vs predictions\n",
    "axes[0].scatter(test_predictions, residuals, alpha=0.3, s=10)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[0].set_xlabel('Predicted pKd')\n",
    "axes[0].set_ylabel('Residuals (Predicted - True)')\n",
    "axes[0].set_title('Residuals vs Predictions')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals distribution\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black')\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Residuals')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title(f'Residuals Distribution (Mean = {residuals.mean():.4f}, Std = {residuals.std():.4f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/residuals_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Residuals analysis saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_results"
   },
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_metrics"
   },
   "outputs": [],
   "source": "# Save metrics to file\nmetrics_summary = f\"\"\"\nAbAg Binding Affinity Prediction - Training Results\n{'='*60}\n\nTraining Configuration:\n  - Epochs: {EPOCHS}\n  - Batch size: {BATCH_SIZE}\n  - Learning rate: {LEARNING_RATE}\n  - Training samples: {len(X_train):,}\n  - Validation samples: {len(X_val):,}\n  - Test samples: {len(X_test):,}\n  - Total training time: {total_time/3600:.2f} hours\n\nTest Set Performance:\n  - RMSE: {rmse:.4f}\n  - MAE: {mae:.4f}\n  - Spearman Ï: {spearman:.4f}\n  - Pearson r: {pearson:.4f}\n  - RÂ²: {r2:.4f}\n\nPer-Bin Performance:\n\"\"\"\n\nfor label in BIN_LABELS:\n    bin_data = test_df[test_df['affinity_bin'] == label]\n    if len(bin_data) > 0:\n        bin_rmse = np.sqrt(mean_squared_error(bin_data['target'], bin_data['prediction']))\n        bin_mae = mean_absolute_error(bin_data['target'], bin_data['prediction'])\n        metrics_summary += f\"  - {label}: RMSE={bin_rmse:.4f}, MAE={bin_mae:.4f}, N={len(bin_data)}\\n\"\n\nmetrics_summary += f\"\\n{'='*60}\\n\"\n\n# Save to file\nwith open(f'{OUTPUT_DIR}/training_results.txt', 'w') as f:\n    f.write(metrics_summary)\n\nprint(metrics_summary)\nprint(f\"âœ… Results saved to {OUTPUT_DIR}/training_results.txt\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final_save"
   },
   "outputs": [],
   "source": [
    "# Save final model with metadata\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'metrics': {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'spearman': spearman,\n",
    "        'pearson': pearson,\n",
    "        'r2': r2\n",
    "    },\n",
    "    'config': {\n",
    "        'input_dim': 150,\n",
    "        'hidden_dims': [256, 128],\n",
    "        'dropout': 0.3,\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE\n",
    "    }\n",
    "}, f'{OUTPUT_DIR}/final_model.pth')\n",
    "\n",
    "print(f\"âœ… Final model saved to {OUTPUT_DIR}/final_model.pth\")\n",
    "print(f\"\\nAll files saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nYou can now download the trained model from Google Drive!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}