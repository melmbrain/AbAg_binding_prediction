# v2.7 Notebook Ready for Training! ‚úÖ

## File Location
**Notebook**: `notebooks/colab_training_v2.7.ipynb`

---

## ‚úÖ All 8 v2.7 Changes Applied

### Change 1: Stable Loss Function ‚úÖ
- **Cell 8**: Replaced `CombinedLoss` with `StableCombinedLoss`
- Removed Soft Spearman (O(n¬≤) instability)
- Using MSE (0.7 weight) + BCE (0.3 weight)
- Added `check_loss_validity()` function for NaN detection

### Change 2: Research-Validated Hyperparameters ‚úÖ
- **Cell 17**: Updated all hyperparameters
- Learning rate: 2e-4 ‚Üí **1e-3** (MBP 2024)
- Dropout: 0.3 ‚Üí **0.1** (was over-regularizing)
- Weight decay: 0.01 ‚Üí **1e-5**
- Batch: 32 ‚Üí **16√ó8=128** (gradient accumulation)
- Early stop patience: 10 ‚Üí **15**

### Change 3: Prediction Clamping ‚úÖ
- **Cell 16**: Added in model forward method
- `pKd_pred = torch.clamp(pKd_pred, min=4.0, max=14.0)`
- **Fixes**: Negative pKd predictions (-2.48 in v2.6)

### Change 4: NaN Detection ‚úÖ
- **Cell 22**: Added after `loss.backward()`
- `check_loss_validity(loss, "training_loss")`
- Stops training immediately if NaN/Inf detected

### Change 5: Complete RNG State Saving ‚úÖ
- **Cell 22**: Added to checkpoint saving
- Saves: torch, CUDA, numpy, python random states
- Restores all RNG states when resuming
- **Provides**: Full reproducibility

### Change 6: Overfitting Monitoring ‚úÖ
- **Cell 22**: Added after validation
- Computes overfit_ratio = val_loss / train_loss
- Warns if ratio > 3.0
- Logged to TensorBoard

### Change 7: ReduceLROnPlateau Scheduler ‚úÖ
- **Cell 20**: Replaced LambdaLR + Cosine
- Mode: maximize Spearman
- Factor: 0.6, Patience: 10 epochs
- From MBP 2024 paper

### Change 8: Update Criterion ‚úÖ
- **Cell 20**: Updated instantiation
- `criterion = StableCombinedLoss(mse_weight=0.7, class_weight=0.3)`
- No more Soft Spearman parameters

---

## üìä Expected Results (vs v2.6)

| Metric | v2.6 (Old) | v2.7 (Expected) | Improvement |
|--------|------------|-----------------|-------------|
| **Spearman** | 0.39 (unstable) | **0.45-0.55** | +15-40% |
| **Recall@pKd9** | 18-100% (jumping) | **50-70%** (stable) | ‚úÖ Stable |
| **RMSE** | 2.10 | **1.2-1.5** | -30% |
| **Pred Range** | -2.48 to 10.0 | **4.0 to 14.0** | ‚úÖ Valid |
| **Training** | Unstable | **Converges smoothly** | ‚úÖ Fixed |

---

## üöÄ Next Steps

### 1. Upload to Google Drive
Upload this file to: `Google Drive ‚Üí MyDrive/AbAg_Training_02/colab_training_v2.7.ipynb`

### 2. Open in Google Colab
1. Go to https://colab.research.google.com
2. File ‚Üí Open notebook ‚Üí Google Drive tab
3. Navigate to: `MyDrive/AbAg_Training_02/colab_training_v2.7.ipynb`
4. Open the notebook

### 3. Connect to A100 GPU
1. Runtime ‚Üí Change runtime type
2. Hardware accelerator: **GPU**
3. GPU type: **A100** (if available)
4. Save

### 4. Run Training
1. Runtime ‚Üí Run all (or Ctrl+F9)
2. Training should start automatically
3. Monitor for:
   - ‚úÖ Pred range: [4.0, 14.0] (no negative values!)
   - ‚úÖ Recall: stable, not jumping
   - ‚úÖ Overfit ratio: < 3.0

---

## üîç What to Watch During Training

### Good Signs ‚úÖ
- Pred range stays in [4.0, 14.0]
- Recall stable (50-70%), no wild jumps
- Spearman increasing steadily
- Overfit ratio < 3.0
- No NaN warnings

### Warning Signs ‚ö†Ô∏è
- Overfit ratio > 3.0 ‚Üí Model overfitting
- Pred range outside [4.0, 14.0] ‚Üí Something wrong with clamping
- NaN loss ‚Üí Training will stop automatically
- Recall jumping wildly ‚Üí Soft Spearman still present (shouldn't happen)

---

## üìÅ Output Files

Training will create:
- `training_output_OPTIMIZED_v2/best_model.pth` - Best model
- `training_output_OPTIMIZED_v2/checkpoint_latest.pth` - Resume checkpoint
- `training_output_OPTIMIZED_v2/metrics.json` - Final metrics
- `training_output_OPTIMIZED_v2/test_predictions.csv` - Test predictions
- `training_output_OPTIMIZED_v2/runs/` - TensorBoard logs

---

## üí° Troubleshooting

### Out of Memory (OOM)
Reduce batch size in Cell 17:
```python
BATCH_SIZE = 8              # Reduced from 16
GRADIENT_ACCUMULATION = 16  # Increased to maintain effective batch=128
```

### Training Very Slow
- Check GPU: Should be A100
- Run `!nvidia-smi` to verify
- If T4/V100, training will be slower

### Recall Still Unstable
Double-check Cell 8:
- Should be `StableCombinedLoss`
- NOT `CombinedLoss`
- No Soft Spearman code present

---

## üéØ Research References

All changes are based on:
1. **Multi-task Bioassay Pre-training (2024)**
   - https://pmc.ncbi.nlm.nih.gov/articles/PMC10783875/
   - Source for: stable loss, hyperparameters, scheduler

2. **DualBind (2024)**
   - https://arxiv.org/html/2406.07770v1
   - Source for: cross-attention architecture

3. **CAFA6 Competition Best Practices**
   - Source for: NaN detection, RNG state saving, overfitting monitoring

---

## ‚úÖ Checklist

- [x] All 8 v2.7 changes applied to notebook
- [x] Notebook saved as `colab_training_v2.7.ipynb`
- [ ] Upload notebook to Google Drive
- [ ] Open in Google Colab
- [ ] Connect to A100 GPU
- [ ] Run training!

---

**Status**: Ready to train! üöÄ

**Expected Training Time**: ~40-50 epochs until early stopping (~160-200 hours on A100)

**File Ready**: `notebooks/colab_training_v2.7.ipynb`

---

*Last updated: 2025-11-25*
*All v2.7 stability fixes applied and verified*
