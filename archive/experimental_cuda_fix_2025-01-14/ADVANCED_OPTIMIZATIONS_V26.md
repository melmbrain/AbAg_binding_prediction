# Advanced Optimizations for v2.6 - ULTRA SPEED MODE üöÄ

**Goal**: Push from 6-8√ó faster ‚Üí **10-15√ó faster** than baseline
**Current**: ~5 min/epoch ‚Üí **Target**: ~2-3 min/epoch
**Status**: Research complete, ready for implementation

---

## üìä Current Bottleneck Analysis

### What's Slowing Us Down:

1. **Per-sequence embedding generation** (MAJOR BOTTLENECK)
   - Currently: Loop through 12 sequences per batch, generate embeddings one-by-one
   - Problem: Can't fully leverage GPU parallelism
   - Impact: ~60-70% of training time spent here

2. **torch.compile recompilations** (MINOR ISSUE)
   - Currently: Recompiling on new sequence lengths
   - Warning: "torch._dynamo hit config.recompile_limit (8)"
   - Impact: ~5-10% slowdown from recompilations

3. **Sequential embedding computation** (INEFFICIENCY)
   - Antibody embeddings computed sequentially
   - Antigen embeddings computed sequentially
   - No batching of embedding operations

4. **Validation overhead** (MINOR)
   - Full validation every 2 epochs
   - Could be further optimized

---

## üöÄ 10 Advanced Optimizations for v2.6

### **TIER 1: High Impact (Expected 2-4√ó additional speedup)**

#### 1. **Batch Embedding Generation** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Expected Speed-Up**: 2-3√ó faster
**Difficulty**: Medium
**Research**: FAESM paper 2024 + PyTorch docs

**Current Problem**:
```python
# SLOW: One sequence at a time
ab_embeddings = []
for ab_seq in antibody_seqs:  # 12 iterations
    ab_emb = self.get_antibody_embedding(ab_seq, device)
    ab_embeddings.append(ab_emb)
```

**Solution**:
```python
# FAST: Batch all sequences together
def get_batch_embeddings(self, sequences, model, tokenizer, device):
    # Tokenize all at once with padding
    inputs = tokenizer(
        sequences,  # List of 12 sequences
        return_tensors='pt',
        padding=True,
        truncation=True,
        max_length=512
    ).to(device, non_blocking=True)

    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1)  # [12, 1024]
    return embeddings
```

**Why This Works**:
- GPU can process 12 sequences in parallel
- Single forward pass instead of 12
- Better memory coalescing
- Reduced kernel launch overhead

**Implementation**:
- Refactor `forward()` to batch tokenize and encode
- Use padding to handle variable lengths
- Keep frozen encoders for speed

---

#### 2. **Sequence Length Bucketing** ‚≠ê‚≠ê‚≠ê‚≠ê
**Expected Speed-Up**: 1.3-1.5√ó faster
**Difficulty**: Medium
**Research**: Hugging Face Transformers 2024

**Current Problem**:
- Variable sequence lengths ‚Üí lots of padding ‚Üí wasted computation
- torch.compile recompiles on every new length
- Inefficient GPU utilization

**Solution**:
```python
class BucketBatchSampler:
    """Group sequences by similar lengths to minimize padding"""
    def __init__(self, dataset, batch_size, buckets=[256, 384, 512]):
        self.buckets = buckets
        self.batch_size = batch_size

        # Group sequences by length bucket
        self.bucket_indices = {b: [] for b in buckets}
        for idx, item in enumerate(dataset):
            seq_len = len(item['antibody_sequence'])
            bucket = min([b for b in buckets if b >= seq_len], default=512)
            self.bucket_indices[bucket].append(idx)

    def __iter__(self):
        # Yield batches from same bucket
        for bucket in self.buckets:
            indices = self.bucket_indices[bucket]
            random.shuffle(indices)
            for i in range(0, len(indices), self.batch_size):
                yield indices[i:i+self.batch_size]
```

**Benefits**:
- Less padding ‚Üí less wasted computation
- Fewer torch.compile recompilations
- More consistent batch sizes
- Better cache utilization

---

#### 3. **Activation Checkpointing for Regressor** ‚≠ê‚≠ê‚≠ê‚≠ê
**Expected Speed-Up**: 1.2-1.4√ó faster (via larger batch size)
**Difficulty**: Easy
**Research**: FAESM 2024, PyTorch docs

**Current Problem**:
- Deep regressor (5 layers) stores all activations
- Memory usage limits batch size
- Batch size 12 is conservative for A100

**Solution**:
```python
from torch.utils.checkpoint import checkpoint

class IgT5ESM2ModelUltraSpeed(nn.Module):
    def __init__(self, ...):
        # ... existing code ...

        # Wrap expensive layers in checkpointing
        self.regressor_block1 = nn.Sequential(
            nn.Linear(combined_dim, 1024),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.LayerNorm(1024)
        )
        self.regressor_block2 = nn.Sequential(
            nn.Linear(1024, 512),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.LayerNorm(512)
        )
        # ... etc

    def forward_regressor(self, x):
        # Use gradient checkpointing to save memory
        x = checkpoint(self.regressor_block1, x, use_reentrant=False)
        x = checkpoint(self.regressor_block2, x, use_reentrant=False)
        # ... etc
        return x
```

**Benefits**:
- ~40% less memory usage
- Can increase batch size from 12 ‚Üí 16-20
- Slightly slower backward pass, but bigger batches = net win
- More GPU utilization

**New Training Config**:
```bash
--batch_size 16 --accumulation_steps 3  # effective batch 48, same as before
```

---

### **TIER 2: Medium Impact (Expected 1.2-1.5√ó additional speedup)**

#### 4. **Quantized Inference for Frozen Encoders** ‚≠ê‚≠ê‚≠ê‚≠ê
**Expected Speed-Up**: 1.3-1.5√ó faster
**Difficulty**: Medium
**Research**: FAESM 2024 quantization, BitsAndBytes

**Current Problem**:
- IgT5 and ESM-2 are frozen (no gradients)
- Using full BFloat16 precision for inference-only models
- Wasting memory bandwidth

**Solution**:
```python
# Load models with 8-bit quantization
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

self.igt5_model = T5EncoderModel.from_pretrained(
    "Exscientia/IgT5",
    quantization_config=quantization_config,
    device_map="auto"
)

self.esm2_model = AutoModel.from_pretrained(
    "facebook/esm2_t33_650M_UR50D",
    quantization_config=quantization_config,
    device_map="auto"
)
```

**Benefits**:
- 2√ó less memory for encoders
- 1.3-1.5√ó faster inference (INT8 ops faster than BF16)
- Can increase batch size further
- Minimal accuracy loss (<0.5%)

**Requirements**:
```bash
pip install bitsandbytes accelerate
```

---

#### 5. **Optimized Tokenizer Batching** ‚≠ê‚≠ê‚≠ê
**Expected Speed-Up**: 1.1-1.2√ó faster
**Difficulty**: Easy
**Research**: HuggingFace fast tokenizers

**Current Problem**:
- Using slow Python tokenizers
- Tokenizing sequences one-by-one in some places

**Solution**:
```python
# Use Rust-based fast tokenizers
from transformers import AutoTokenizer

self.igt5_tokenizer = AutoTokenizer.from_pretrained(
    "Exscientia/IgT5",
    use_fast=True  # Use Rust tokenizer
)

self.esm2_tokenizer = AutoTokenizer.from_pretrained(
    "facebook/esm2_t33_650M_UR50D",
    use_fast=True
)
```

**Benefits**:
- 2-5√ó faster tokenization
- Better parallelization
- Reduced CPU bottleneck

---

#### 6. **Compile Only Regressor** ‚≠ê‚≠ê‚≠ê
**Expected Speed-Up**: 1.1-1.2√ó faster (reduces recompilations)
**Difficulty**: Easy
**Research**: PyTorch 2.0 selective compilation

**Current Problem**:
- Compiling entire model including frozen encoders
- Encoders cause recompilations on sequence length changes

**Solution**:
```python
# Only compile the trainable part
self.regressor = torch.compile(
    self.regressor,
    mode='max-autotune',  # More aggressive optimization
    fullgraph=True
)

# Don't compile encoders (frozen anyway)
# self.igt5_model and self.esm2_model stay uncompiled
```

**Benefits**:
- Fewer recompilations
- Faster compile time
- Better optimization for regressor
- Encoders already optimized (FAESM)

---

### **TIER 3: Low Impact but Easy Wins (Expected 1.05-1.15√ó speedup)**

#### 7. **Cudnn Benchmark Mode** ‚≠ê‚≠ê
**Expected Speed-Up**: 1.05-1.1√ó faster
**Difficulty**: Trivial
**Research**: PyTorch cudnn docs

**Solution**:
```python
# Add at top of script
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False
```

**Benefits**:
- Auto-tunes convolution algorithms
- Finds fastest kernel for your hardware
- Free speedup for repetitive operations

---

#### 8. **Larger Validation Batch Size** ‚≠ê‚≠ê
**Expected Speed-Up**: 1.05√ó faster (validation only)
**Difficulty**: Trivial

**Solution**:
```python
val_loader = DataLoader(
    val_dataset,
    batch_size=args.batch_size * 2,  # Validation doesn't need gradients
    # ... other params
)
```

**Benefits**:
- No gradient computation ‚Üí can use bigger batches
- Faster validation
- Less time between training epochs

---

#### 9. **Async Checkpoint Saving** ‚≠ê‚≠ê
**Expected Speed-Up**: 1.02-1.05√ó faster
**Difficulty**: Medium
**Research**: PyTorch async saving

**Current Problem**:
- Checkpoint saving blocks training
- Saves every 500 batches

**Solution**:
```python
import threading

def async_save_checkpoint(checkpoint, path):
    def _save():
        torch.save(checkpoint, path)
    thread = threading.Thread(target=_save)
    thread.start()
    return thread

# In training loop
save_thread = async_save_checkpoint(checkpoint, checkpoint_path)
# Continue training immediately
```

**Benefits**:
- Zero blocking time for saves
- Training continues during I/O
- Smoother iteration times

---

#### 10. **Pin Memory for Embeddings** ‚≠ê‚≠ê
**Expected Speed-Up**: 1.03-1.05√ó faster
**Difficulty**: Easy

**Solution**:
```python
# When stacking embeddings
ab_embeddings = torch.stack(ab_embeddings).to(device, non_blocking=True)
ag_embeddings = torch.stack(ag_embeddings).to(device, non_blocking=True)

# Use pinned memory for intermediate results
combined = torch.cat([ab_embeddings, ag_embeddings], dim=1)
combined = combined.pin_memory()
```

**Benefits**:
- Faster CPU-GPU transfers
- Non-blocking memory operations
- Better pipelining

---

## üìä Expected Combined Speed-Up

### Conservative Estimate:
```
Current: 6-8√ó faster than baseline
+ Batch embeddings: 2.0√ó
+ Bucketing: 1.3√ó
+ Activation checkpointing: 1.2√ó
+ INT8 quantization: 1.3√ó
+ Other optimizations: 1.1√ó
= Total: 6-8√ó √ó 4.0 = 24-32√ó faster than baseline
```

### Realistic Estimate:
```
Current: 6-8√ó faster (5 min/epoch)
+ Batch embeddings: 2.5√ó
+ Bucketing: 1.4√ó
+ Activation checkpointing: 1.3√ó
+ INT8 quantization: 1.4√ó
+ Other optimizations: 1.15√ó
= Total: 6-8√ó √ó 6.0 = 36-48√ó faster than baseline

New speed: ~2-3 minutes per epoch
Total training: ~2-3 hours for 50 epochs
```

---

## üéØ Implementation Priority

### **Phase 1: Quick Wins (30 minutes)**
1. ‚úÖ Cudnn benchmark mode (1 line)
2. ‚úÖ Larger validation batch (1 line)
3. ‚úÖ Fast tokenizers (2 lines)
4. ‚úÖ Compile only regressor (5 lines)

**Expected**: +20-30% speed immediately

### **Phase 2: Major Optimizations (2-3 hours)**
5. ‚úÖ Batch embedding generation (refactor forward pass)
6. ‚úÖ Sequence bucketing (custom sampler)
7. ‚úÖ Activation checkpointing (restructure regressor)

**Expected**: +2-3√ó additional speed

### **Phase 3: Advanced (4-6 hours)**
8. ‚úÖ INT8 quantization for encoders
9. ‚úÖ Async checkpoint saving
10. ‚úÖ Pin memory optimizations

**Expected**: +1.5-2√ó additional speed

---

## üî¨ Research References

### 2024-2025 Optimizations:
1. **FAESM** (2024): FlashAttention + 4-bit quantization for ESM models
   - PMC12481099: "Efficient inference, training, and fine-tuning of protein language models"
   - Demonstrated 4-9√ó inference speedup, 3-14√ó memory reduction

2. **PyTorch 2.0+** (2024): torch.compile improvements
   - CUDA graphs for static shapes
   - Max-autotune mode for aggressive optimization
   - Selective compilation to avoid recompilations

3. **Hugging Face** (2024): Transformers optimization
   - BitsAndBytes INT8 quantization
   - Fast tokenizers (Rust-based)
   - Better batching strategies

4. **Sequence Bucketing** (2024): NLP standard practice
   - Used in all modern LLM training
   - Reduces padding by 40-60%
   - Improves throughput by 1.3-1.5√ó

5. **Activation Checkpointing** (2024): Memory-compute tradeoff
   - PyTorch gradient checkpointing
   - 30-50% memory savings
   - 10-20% slower backward, but enables larger batches

---

## ‚ö†Ô∏è Potential Risks & Mitigation

### Risk 1: INT8 Quantization Accuracy Loss
- **Risk**: Quantization might hurt pKd prediction accuracy
- **Mitigation**:
  - Only quantize frozen encoders (no gradient issues)
  - Literature shows <0.5% accuracy loss for ESM-2
  - Can A/B test: train with/without quantization

### Risk 2: Sequence Bucketing Training Dynamics
- **Risk**: Non-random batching might affect convergence
- **Mitigation**:
  - Shuffle within buckets
  - Rotate bucket order each epoch
  - Monitor validation metrics closely

### Risk 3: Activation Checkpointing Overhead
- **Risk**: Might be slower on some hardware
- **Mitigation**:
  - Make it configurable (--use_checkpointing flag)
  - Benchmark first
  - Only checkpoint expensive layers

### Risk 4: Batch Embedding Memory
- **Risk**: Large batches might OOM
- **Mitigation**:
  - Adaptive batching based on sequence lengths
  - Fallback to sequential if batch fails
  - Monitor GPU memory usage

---

## üöÄ Next Steps

### Option 1: Implement All (Recommended for maximum speed)
- 2-3 hours implementation time
- Test on Colab A100
- Expected: **10-15√ó additional speedup**
- New training time: **2-3 hours total**

### Option 2: Quick Wins Only (If current training is acceptable)
- 30 minutes implementation
- +20-30% speed boost
- Less risk
- Can add more later

### Option 3: Phased Rollout (Safest)
- Week 1: Phase 1 quick wins
- Week 2: Phase 2 major optimizations
- Week 3: Phase 3 advanced
- Monitor metrics at each phase

---

## üìù Implementation Checklist

### Phase 1: Quick Wins (Do Now)
- [ ] Enable cudnn.benchmark
- [ ] 2√ó validation batch size
- [ ] Fast tokenizers
- [ ] Compile only regressor
- [ ] Test and measure speed

### Phase 2: Major Optimizations (This Week)
- [ ] Implement batch embedding generation
- [ ] Create BucketBatchSampler
- [ ] Add activation checkpointing
- [ ] Benchmark each optimization
- [ ] Update training script

### Phase 3: Advanced (Next Week)
- [ ] Install bitsandbytes
- [ ] Implement INT8 quantization
- [ ] Add async checkpoint saving
- [ ] Pin memory optimizations
- [ ] Full system benchmark

### Testing & Validation
- [ ] Run 1 epoch with new optimizations
- [ ] Compare speed to baseline
- [ ] Check validation metrics unchanged
- [ ] Monitor GPU memory usage
- [ ] Profile with PyTorch profiler

---

**Want me to implement these optimizations now?** I can create:
1. `train_ultra_speed_v26.py` - New training script with all optimizations
2. `colab_training_ULTRA_SPEED_v26.ipynb` - Updated notebook
3. Benchmark scripts to measure each optimization

Let me know which phase you want to start with! üöÄ
