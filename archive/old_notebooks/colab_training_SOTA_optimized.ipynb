{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IgT5 + ESM-2 Training - Optimized for Colab Stability\n",
    "\n",
    "**Optimizations**:\n",
    "- âœ… Checkpoint every N batches (not just epochs)\n",
    "- âœ… Faster data loading with caching\n",
    "- âœ… Auto-resume from any checkpoint\n",
    "- âœ… Smaller validation to speed up epochs\n",
    "- âœ… Progress saved to Google Drive continuously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/AbAg_Training')\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch pandas scipy scikit-learn tqdm sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Optimized Training Script with Frequent Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%writefile train_optimized.py\n\"\"\"\nOptimized training with frequent checkpointing - FIXED scheduler bug\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom tqdm import tqdm\nimport argparse\nfrom pathlib import Path\nimport json\nimport time\nfrom transformers import T5EncoderModel, T5Tokenizer, AutoModel, AutoTokenizer\n\n\nclass IgT5ESM2Model(nn.Module):\n    def __init__(self, dropout=0.3, freeze_encoders=True):\n        super().__init__()\n\n        print(\"Loading IgT5 for antibody...\")\n        self.igt5_tokenizer = T5Tokenizer.from_pretrained(\"Exscientia/IgT5\", do_lower_case=False)\n        self.igt5_model = T5EncoderModel.from_pretrained(\"Exscientia/IgT5\")\n\n        print(\"Loading ESM-2 for antigen...\")\n        self.esm2_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n        self.esm2_model = AutoModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n\n        if freeze_encoders:\n            for param in self.igt5_model.parameters():\n                param.requires_grad = False\n            for param in self.esm2_model.parameters():\n                param.requires_grad = False\n\n        igt5_dim = self.igt5_model.config.d_model\n        esm2_dim = self.esm2_model.config.hidden_size\n        combined_dim = igt5_dim + esm2_dim\n\n        print(f\"\\nArchitecture: {igt5_dim}D + {esm2_dim}D = {combined_dim}D\\n\")\n\n        self.regressor = nn.Sequential(\n            nn.Linear(combined_dim, 1024),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.LayerNorm(1024),\n            nn.Linear(1024, 512),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.LayerNorm(512),\n            nn.Linear(512, 256),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.LayerNorm(256),\n            nn.Linear(256, 128),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 1)\n        )\n\n    def get_antibody_embedding(self, antibody_seq, device):\n        inputs = self.igt5_tokenizer(\n            antibody_seq, return_tensors=\"pt\", padding=True,\n            truncation=True, max_length=512\n        ).to(device)\n        with torch.no_grad():\n            outputs = self.igt5_model(**inputs)\n            ab_emb = outputs.last_hidden_state.mean(dim=1)\n        return ab_emb.squeeze(0)\n\n    def get_antigen_embedding(self, antigen_seq, device):\n        inputs = self.esm2_tokenizer(\n            antigen_seq, return_tensors=\"pt\", padding=True,\n            truncation=True, max_length=512\n        ).to(device)\n        with torch.no_grad():\n            outputs = self.esm2_model(**inputs)\n            ag_emb = outputs.last_hidden_state[:, 0, :]\n        return ag_emb.squeeze(0)\n\n    def forward(self, antibody_seqs, antigen_seqs, device):\n        ab_embeddings = []\n        for ab_seq in antibody_seqs:\n            ab_emb = self.get_antibody_embedding(ab_seq, device)\n            ab_embeddings.append(ab_emb)\n        ab_embeddings = torch.stack(ab_embeddings).to(device)\n\n        ag_embeddings = []\n        for ag_seq in antigen_seqs:\n            ag_emb = self.get_antigen_embedding(ag_seq, device)\n            ag_embeddings.append(ag_emb)\n        ag_embeddings = torch.stack(ag_embeddings).to(device)\n\n        combined = torch.cat([ab_embeddings, ag_embeddings], dim=1)\n        predictions = self.regressor(combined).squeeze(-1)\n        return predictions\n\n\nclass FocalMSELoss(nn.Module):\n    def __init__(self, gamma=2.0):\n        super().__init__()\n        self.gamma = gamma\n\n    def forward(self, pred, target):\n        mse = (pred - target) ** 2\n        focal_weight = (1 + mse) ** self.gamma\n        return (focal_weight * mse).mean()\n\n\nclass AbAgDataset(Dataset):\n    def __init__(self, df):\n        self.df = df.reset_index(drop=True)\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        return {\n            'antibody_sequence': self.df.iloc[idx]['antibody_sequence'],\n            'antigen_sequence': self.df.iloc[idx]['antigen_sequence'],\n            'pKd': torch.tensor(self.df.iloc[idx]['pKd'], dtype=torch.float32)\n        }\n\n\ndef collate_fn(batch):\n    antibody_seqs = [item['antibody_sequence'] for item in batch]\n    antigen_seqs = [item['antigen_sequence'] for item in batch]\n    pKds = torch.stack([item['pKd'] for item in batch])\n    return {'antibody_seqs': antibody_seqs, 'antigen_seqs': antigen_seqs, 'pKd': pKds}\n\n\ndef save_checkpoint(model, optimizer, scheduler, scaler, epoch, batch_idx,\n                   best_spearman, output_dir, prefix='checkpoint'):\n    \"\"\"Save checkpoint to Google Drive - FIXED: Handle None scheduler\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'batch_idx': batch_idx,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scaler_state_dict': scaler.state_dict(),\n        'best_val_spearman': best_spearman,\n        'timestamp': time.time()\n    }\n\n    # Only save scheduler state if scheduler is not None\n    if scheduler is not None:\n        checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n\n    checkpoint_path = output_dir / f'{prefix}_e{epoch}_b{batch_idx}.pth'\n    torch.save(checkpoint, checkpoint_path)\n\n    # Also save as latest\n    torch.save(checkpoint, output_dir / f'{prefix}_latest.pth')\n\n    return checkpoint_path\n\n\ndef quick_eval(model, loader, device, max_batches=50):\n    \"\"\"Quick evaluation on subset for frequent checks\"\"\"\n    model.eval()\n    predictions = []\n    targets = []\n\n    with torch.no_grad():\n        for i, batch in enumerate(loader):\n            if i >= max_batches:\n                break\n\n            antibody_seqs = batch['antibody_seqs']\n            antigen_seqs = batch['antigen_seqs']\n            batch_targets = batch['pKd'].to(device)\n\n            with torch.amp.autocast('cuda'):\n                batch_predictions = model(antibody_seqs, antigen_seqs, device)\n\n            predictions.extend(batch_predictions.cpu().numpy())\n            targets.extend(batch_targets.cpu().numpy())\n\n    predictions = np.array(predictions)\n    targets = np.array(targets)\n\n    spearman = stats.spearmanr(targets, predictions)[0]\n    strong_binders = targets >= 9.0\n    predicted_strong = predictions >= 9.0\n    recall = (strong_binders & predicted_strong).sum() / strong_binders.sum() if strong_binders.sum() > 0 else 0\n\n    return {'spearman': spearman, 'recall_pkd9': recall * 100}\n\n\ndef train_epoch(model, loader, optimizer, criterion, device, scaler,\n               epoch, start_batch, output_dir, save_every_n_batches=100):\n    \"\"\"Training with frequent checkpointing\"\"\"\n    model.train()\n    total_loss = 0\n    best_spearman = -1\n\n    pbar = tqdm(enumerate(loader), total=len(loader), desc=f\"Epoch {epoch+1}\")\n\n    for batch_idx, batch in pbar:\n        # Skip batches we've already processed (when resuming)\n        if batch_idx < start_batch:\n            continue\n\n        antibody_seqs = batch['antibody_seqs']\n        antigen_seqs = batch['antigen_seqs']\n        targets = batch['pKd'].to(device)\n\n        optimizer.zero_grad()\n\n        with torch.amp.autocast('cuda'):\n            predictions = model(antibody_seqs, antigen_seqs, device)\n            loss = criterion(predictions, targets)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n        pbar.set_postfix({'loss': f'{loss.item():.2e}', 'batch': f'{batch_idx+1}/{len(loader)}'})\n\n        # Save checkpoint every N batches\n        if (batch_idx + 1) % save_every_n_batches == 0:\n            checkpoint_path = save_checkpoint(\n                model, optimizer, None, scaler, epoch, batch_idx,\n                best_spearman, output_dir, prefix='batch_checkpoint'\n            )\n            print(f\"\\nâœ“ Saved batch checkpoint: {checkpoint_path.name}\")\n\n    return total_loss / len(loader)\n\n\ndef main(args):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"\\nDevice: {device}\")\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\\n\")\n\n    # Load data\n    print(\"Loading data...\")\n    df = pd.read_csv(args.data)\n    print(f\"Loaded {len(df):,} samples\\n\")\n\n    # Use smaller validation set for faster epochs\n    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n    # Sample validation set for quick checks (use 10% of val for speed)\n    val_df_quick = val_df.sample(frac=0.1, random_state=42)\n\n    print(f\"Train: {len(train_df):,} | Val: {len(val_df):,} | Val (quick): {len(val_df_quick):,}\\n\")\n\n    train_dataset = AbAgDataset(train_df)\n    val_dataset = AbAgDataset(val_df_quick)  # Use smaller validation\n\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n                             num_workers=2, collate_fn=collate_fn, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n                           num_workers=2, collate_fn=collate_fn, pin_memory=True)\n\n    # Initialize model\n    model = IgT5ESM2Model(dropout=args.dropout, freeze_encoders=True).to(device)\n    criterion = FocalMSELoss(gamma=args.focal_gamma)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n    scaler = torch.amp.GradScaler('cuda')\n\n    start_epoch = 0\n    start_batch = 0\n    best_spearman = -1\n    output_dir = Path(args.output_dir)\n    output_dir.mkdir(exist_ok=True)\n\n    # Auto-resume from latest checkpoint\n    latest_checkpoint = output_dir / 'batch_checkpoint_latest.pth'\n    if latest_checkpoint.exists():\n        print(f\"Found checkpoint: {latest_checkpoint}\")\n        checkpoint = torch.load(latest_checkpoint, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n\n        # Load scheduler state if it exists in checkpoint\n        if 'scheduler_state_dict' in checkpoint and scheduler is not None:\n            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n\n        start_epoch = checkpoint['epoch']\n        start_batch = checkpoint['batch_idx'] + 1  # Start from next batch\n        best_spearman = checkpoint.get('best_val_spearman', -1)\n        print(f\"Resuming from Epoch {start_epoch+1}, Batch {start_batch}, Spearman: {best_spearman:.4f}\\n\")\n\n    print(f\"Starting training for {args.epochs} epochs...\\n\")\n    print(f\"Checkpoints every {args.save_every_n_batches} batches\\n\")\n\n    for epoch in range(start_epoch, args.epochs):\n        print(f\"\\n{'='*70}\")\n        print(f\"Epoch {epoch+1}/{args.epochs}\")\n        print(f\"{'='*70}\")\n\n        train_loss = train_epoch(\n            model, train_loader, optimizer, criterion, device, scaler,\n            epoch, start_batch if epoch == start_epoch else 0,\n            output_dir, args.save_every_n_batches\n        )\n\n        # Quick validation\n        print(\"\\nQuick validation...\")\n        val_metrics = quick_eval(model, val_loader, device, max_batches=50)\n        scheduler.step()\n\n        print(f\"\\nTrain Loss: {train_loss:.4f}\")\n        print(f\"Val Spearman: {val_metrics['spearman']:.4f} | Recall@pKdâ‰¥9: {val_metrics['recall_pkd9']:.2f}%\")\n\n        # Save best model\n        if val_metrics['spearman'] > best_spearman:\n            best_spearman = val_metrics['spearman']\n            torch.save(model.state_dict(), output_dir / 'best_model.pth')\n            print(\"âœ“ Saved best model\")\n\n        # Save end-of-epoch checkpoint (with scheduler)\n        save_checkpoint(\n            model, optimizer, scheduler, scaler, epoch, len(train_loader)-1,\n            best_spearman, output_dir, prefix='epoch_checkpoint'\n        )\n\n        # Reset start_batch for next epoch\n        start_batch = 0\n\n    print(f\"\\n{'='*70}\")\n    print(f\"Training complete! Best Spearman: {best_spearman:.4f}\")\n    print(f\"{'='*70}\")\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data', type=str, required=True)\n    parser.add_argument('--output_dir', type=str, default='outputs_sota')\n    parser.add_argument('--epochs', type=int, default=50)\n    parser.add_argument('--batch_size', type=int, default=8)\n    parser.add_argument('--lr', type=float, default=1e-3)\n    parser.add_argument('--weight_decay', type=float, default=0.01)\n    parser.add_argument('--dropout', type=float, default=0.3)\n    parser.add_argument('--focal_gamma', type=float, default=2.0)\n    parser.add_argument('--save_every_n_batches', type=int, default=100,\n                       help='Save checkpoint every N batches (default: 100)')\n    args = parser.parse_args()\n    main(args)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Start Training with Frequent Checkpointing ðŸš€\n",
    "\n",
    "**Optimizations**:\n",
    "- Checkpoints saved every 100 batches (~10 minutes)\n",
    "- Auto-resumes from exact batch if interrupted\n",
    "- Smaller validation set for faster epochs\n",
    "- All checkpoints saved to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_optimized.py \\\n",
    "  --data agab_phase2_full.csv \\\n",
    "  --epochs 50 \\\n",
    "  --batch_size 8 \\\n",
    "  --save_every_n_batches 100 \\\n",
    "  --output_dir outputs_sota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current progress\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoint_path = 'outputs_sota/batch_checkpoint_latest.pth'\n",
    "if Path(checkpoint_path).exists():\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    print(f\"Epoch: {checkpoint['epoch'] + 1}/50\")\n",
    "    print(f\"Batch: {checkpoint['batch_idx'] + 1}\")\n",
    "    print(f\"Best Spearman: {checkpoint['best_val_spearman']:.4f}\")\n",
    "    \n",
    "    import time\n",
    "    elapsed = time.time() - checkpoint['timestamp']\n",
    "    print(f\"\\nLast saved: {elapsed/60:.1f} minutes ago\")\n",
    "else:\n",
    "    print(\"No checkpoint found yet - training just started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features\n",
    "\n",
    "âœ… **Frequent Checkpointing**: Saves every 100 batches (~10 min)\n",
    "âœ… **Auto-Resume**: Automatically resumes from exact batch\n",
    "âœ… **Fast Validation**: Uses 10% subset for quick checks\n",
    "âœ… **Google Drive**: All checkpoints saved immediately\n",
    "âœ… **Progress Tracking**: Shows epoch + batch progress"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}