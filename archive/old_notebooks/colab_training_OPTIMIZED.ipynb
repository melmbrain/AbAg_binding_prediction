{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß¨ Antibody-Antigen Binding Prediction - OPTIMIZED Training\n",
        "\n",
        "## A100 + Google Drive + ESM-2 3B + Optuna Optimization\n",
        "\n",
        "**This notebook includes all optimizations:**\n",
        "- ‚úÖ **Google Drive integration** - Auto-loads data, auto-saves results\n",
        "- ‚úÖ **A100-80GB optimized** - TF32, large batches, optimized memory\n",
        "- ‚úÖ **ESM-2 3B model** - State-of-the-art protein encoder\n",
        "- ‚úÖ **Optuna hyperparameter optimization** - Find optimal training settings\n",
        "- ‚úÖ **Fixed training issues** - Proper warmup, patience, validation\n",
        "\n",
        "**Key improvements over COMPLETE version:**\n",
        "- üîß **Full validation** for early stopping (not 5% sample)\n",
        "- üîß **Reduced warmup** to prevent early stopping during warmup\n",
        "- üîß **ReduceLROnPlateau** for dynamic LR adjustment\n",
        "- üîß **Optuna search** for LR, dropout, batch size\n",
        "- üîß **Better patience** settings\n",
        "\n",
        "**Expected performance:**\n",
        "- Test Spearman: **0.42-0.50**\n",
        "- Training time: ~1-2 hours (with optimization)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "cell-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Environment Setup"
      ],
      "metadata": {
        "id": "cell-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU - should be A100\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"GPU Memory: {gpu_memory:.2f} GB\")\n",
        "    device = torch.device('cuda')\n",
        "    \n",
        "    if 'A100' in gpu_name:\n",
        "        print(\"\\n‚úÖ A100 GPU detected! Optimizations will be enabled.\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è WARNING: Expected A100 but got {gpu_name}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: GPU not available!\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f\"\\nUsing device: {device}\")"
      ],
      "metadata": {
        "id": "cell-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "print(\"Installing required packages...\\n\")\n",
        "\n",
        "!pip install -q transformers>=4.41.0\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q optuna\n",
        "\n",
        "print(\"\\n‚úÖ All packages installed successfully!\")\n",
        "print(\"‚úÖ Using Colab's pre-installed numpy, pandas, scikit-learn, scipy\")"
      ],
      "metadata": {
        "id": "cell-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable A100-specific optimizations\n",
        "import torch\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "print(\"‚úÖ A100 optimizations enabled\")"
      ],
      "metadata": {
        "id": "cell-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Import Libraries & Utilities"
      ],
      "metadata": {
        "id": "cell-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import json\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Transformers\n",
        "from transformers import (\n",
        "    T5Tokenizer, T5EncoderModel,\n",
        "    AutoTokenizer, AutoModel\n",
        ")\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from scipy import stats\n",
        "\n",
        "# Optuna\n",
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"Optuna version: {optuna.__version__}\")"
      ],
      "metadata": {
        "id": "cell-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprehensive metrics function\n",
        "def compute_comprehensive_metrics(targets, predictions):\n",
        "    \"\"\"Compute all standard metrics\"\"\"\n",
        "    mse = mean_squared_error(targets, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(targets, predictions)\n",
        "    r2 = r2_score(targets, predictions)\n",
        "    \n",
        "    spearman, spearman_p = stats.spearmanr(targets, predictions)\n",
        "    pearson, pearson_p = stats.pearsonr(targets, predictions)\n",
        "    \n",
        "    strong_binders = targets >= 9.0\n",
        "    predicted_strong = predictions >= 9.0\n",
        "    \n",
        "    tp = np.sum(strong_binders & predicted_strong)\n",
        "    fp = np.sum(~strong_binders & predicted_strong)\n",
        "    tn = np.sum(~strong_binders & ~predicted_strong)\n",
        "    fn = np.sum(strong_binders & ~predicted_strong)\n",
        "    \n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "    \n",
        "    return {\n",
        "        'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2,\n",
        "        'spearman': spearman, 'spearman_p': spearman_p,\n",
        "        'pearson': pearson, 'pearson_p': pearson_p,\n",
        "        'recall_pkd9': recall * 100, 'precision_pkd9': precision * 100,\n",
        "        'f1_pkd9': f1 * 100, 'specificity_pkd9': specificity * 100,\n",
        "        'n_samples': len(targets), 'n_strong_binders': int(strong_binders.sum())\n",
        "    }\n",
        "\n",
        "# Early Stopping with better tracking\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=15, min_delta=0.001, mode='max', verbose=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.best_epoch = 0\n",
        "    \n",
        "    def __call__(self, score, epoch):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.best_epoch = epoch\n",
        "            return False\n",
        "        \n",
        "        if self.mode == 'max':\n",
        "            improved = score > (self.best_score + self.min_delta)\n",
        "        else:\n",
        "            improved = score < (self.best_score - self.min_delta)\n",
        "        \n",
        "        if improved:\n",
        "            self.best_score = score\n",
        "            self.best_epoch = epoch\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"   No improvement for {self.counter}/{self.patience} epochs\")\n",
        "            \n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                if self.verbose:\n",
        "                    print(f\"\\n‚ö†Ô∏è Early stopping triggered!\")\n",
        "                    print(f\"   Best score: {self.best_score:.4f} at epoch {self.best_epoch+1}\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "# LR Scheduler with shorter warmup\n",
        "def get_warmup_cosine_scheduler(optimizer, warmup_epochs, total_epochs):\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < warmup_epochs:\n",
        "            return float(epoch + 1) / float(max(1, warmup_epochs))  # Start from 1/warmup, not 0\n",
        "        progress = float(epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
        "        return max(0.1, 0.5 * (1.0 + np.cos(np.pi * progress)))  # Min 10% of LR\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# ReduceLROnPlateau wrapper\n",
        "class ReduceLROnPlateauWrapper:\n",
        "    def __init__(self, optimizer, factor=0.5, patience=5, min_lr=1e-6):\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='max', factor=factor, patience=patience, \n",
        "            min_lr=min_lr, verbose=True\n",
        "        )\n",
        "    \n",
        "    def step(self, val_score):\n",
        "        self.scheduler.step(val_score)\n",
        "\n",
        "# Focal MSE Loss\n",
        "class FocalMSELoss(nn.Module):\n",
        "    def __init__(self, gamma=2.0, label_smoothing=0.0):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.label_smoothing = label_smoothing\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        if self.label_smoothing > 0:\n",
        "            target_mean = target.mean()\n",
        "            target = (1 - self.label_smoothing) * target + self.label_smoothing * target_mean\n",
        "        mse = (pred - target) ** 2\n",
        "        focal_weight = (1 + mse) ** self.gamma\n",
        "        return (focal_weight * mse).mean()\n",
        "\n",
        "print(\"‚úÖ Utility functions defined\")"
      ],
      "metadata": {
        "id": "cell-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Mount Google Drive & Load Data"
      ],
      "metadata": {
        "id": "cell-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted!\")\n",
        "\n",
        "# Set up paths\n",
        "DRIVE_DIR = '/content/drive/MyDrive/AbAg_Training_02'\n",
        "OUTPUT_DIR = f'{DRIVE_DIR}/training_output_OPTIMIZED'\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\nüìÇ Working directories:\")\n",
        "print(f\"   Data directory: {DRIVE_DIR}\")\n",
        "print(f\"   Output directory: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "cell-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "CSV_FILENAME = 'agab_phase2_full.csv'  # ‚Üê CHANGE THIS to your filename\n",
        "\n",
        "csv_path = os.path.join(DRIVE_DIR, CSV_FILENAME)\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(f\"\\nüìä Dataset: {len(df):,} samples\")\n",
        "print(f\"   pKd range: {df['pKd'].min():.2f} - {df['pKd'].max():.2f}\")\n",
        "print(f\"   Strong binders (‚â•9): {(df['pKd']>=9).sum():,} ({100*(df['pKd']>=9).sum()/len(df):.1f}%)\")"
      ],
      "metadata": {
        "id": "cell-10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"\\nüìä Dataset splits:\")\n",
        "print(f\"   Train:  {len(train_df):,}\")\n",
        "print(f\"   Val:    {len(val_df):,}\")\n",
        "print(f\"   Test:   {len(test_df):,}\")"
      ],
      "metadata": {
        "id": "cell-11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset and DataLoader\n",
        "class AbAgDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe.reset_index(drop=True)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        return {\n",
        "            'antibody_seqs': row['antibody_sequence'],\n",
        "            'antigen_seqs': row['antigen_sequence'],\n",
        "            'pKd': torch.tensor(row['pKd'], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'antibody_seqs': [item['antibody_seqs'] for item in batch],\n",
        "        'antigen_seqs': [item['antigen_seqs'] for item in batch],\n",
        "        'pKd': torch.stack([item['pKd'] for item in batch])\n",
        "    }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = AbAgDataset(train_df)\n",
        "val_dataset = AbAgDataset(val_df)  # FULL validation set\n",
        "test_dataset = AbAgDataset(test_df)\n",
        "\n",
        "print(\"‚úÖ Datasets created\")"
      ],
      "metadata": {
        "id": "cell-12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Model Architecture (ESM-2 3B)"
      ],
      "metadata": {
        "id": "cell-13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with ESM-2 3B\n",
        "class IgT5ESM2_3B_Model(nn.Module):\n",
        "    def __init__(self, dropout=0.3, freeze_encoders=True, use_checkpointing=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        print(\"üî® Building model with ESM-2 3B...\")\n",
        "        \n",
        "        # IgT5 for antibodies\n",
        "        print(\"  üì• Loading IgT5...\")\n",
        "        self.igt5_tokenizer = T5Tokenizer.from_pretrained(\"Exscientia/IgT5\")\n",
        "        self.igt5_model = T5EncoderModel.from_pretrained(\"Exscientia/IgT5\")\n",
        "        \n",
        "        # ESM-2 3B for antigens\n",
        "        print(\"  üì• Loading ESM-2 3B (this will take a moment)...\")\n",
        "        self.esm2_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t36_3B_UR50D\")\n",
        "        self.esm2_model = AutoModel.from_pretrained(\"facebook/esm2_t36_3B_UR50D\")\n",
        "        print(\"  ‚úÖ ESM-2 3B loaded!\")\n",
        "        \n",
        "        # Freeze encoders\n",
        "        if freeze_encoders:\n",
        "            for param in self.igt5_model.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.esm2_model.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "        # Gradient checkpointing\n",
        "        if use_checkpointing:\n",
        "            self.igt5_model.gradient_checkpointing_enable()\n",
        "            self.esm2_model.gradient_checkpointing_enable()\n",
        "        \n",
        "        # Dimensions\n",
        "        self.igt5_dim = self.igt5_model.config.d_model  # 512\n",
        "        self.esm2_dim = self.esm2_model.config.hidden_size  # 2560\n",
        "        self.combined_dim = self.igt5_dim + self.esm2_dim  # 3072\n",
        "        \n",
        "        # Regression head\n",
        "        self.regression_head = nn.Sequential(\n",
        "            nn.Linear(self.combined_dim, 1536),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.LayerNorm(1536),\n",
        "            \n",
        "            nn.Linear(1536, 768),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.LayerNorm(768),\n",
        "            \n",
        "            nn.Linear(768, 384),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.LayerNorm(384),\n",
        "            \n",
        "            nn.Linear(384, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.LayerNorm(128),\n",
        "            \n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        \n",
        "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        print(f\"  üìä Trainable parameters: {trainable_params/1e6:.1f}M\")\n",
        "    \n",
        "    def forward(self, antibody_seqs, antigen_seqs, device):\n",
        "        antibody_tokens = self.igt5_tokenizer(\n",
        "            antibody_seqs, return_tensors='pt', padding=True,\n",
        "            truncation=True, max_length=512\n",
        "        ).to(device)\n",
        "        \n",
        "        antigen_tokens = self.esm2_tokenizer(\n",
        "            antigen_seqs, return_tensors='pt', padding=True,\n",
        "            truncation=True, max_length=2048\n",
        "        ).to(device)\n",
        "        \n",
        "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "            antibody_outputs = self.igt5_model(**antibody_tokens)\n",
        "            antibody_embedding = antibody_outputs.last_hidden_state.mean(dim=1)\n",
        "            \n",
        "            antigen_outputs = self.esm2_model(**antigen_tokens)\n",
        "            antigen_embedding = antigen_outputs.last_hidden_state.mean(dim=1)\n",
        "            \n",
        "            combined = torch.cat([antibody_embedding, antigen_embedding], dim=1)\n",
        "            pKd_pred = self.regression_head(combined).squeeze(-1)\n",
        "        \n",
        "        return pKd_pred\n",
        "\n",
        "print(\"‚úÖ Model class defined\")"
      ],
      "metadata": {
        "id": "cell-14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Optuna Hyperparameter Optimization\n",
        "\n",
        "**Search space:**\n",
        "- Learning rate: 1e-4 to 5e-3\n",
        "- Dropout: 0.1 to 0.5\n",
        "- Batch size: 32, 48, 64\n",
        "- Warmup epochs: 1-3\n",
        "- Label smoothing: 0-0.1\n",
        "\n",
        "**Quick optimization:** 10 trials (adjust for more thorough search)"
      ],
      "metadata": {
        "id": "cell-15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training functions\n",
        "def train_epoch(model, loader, optimizer, criterion, device, max_grad_norm):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in loader:\n",
        "        antibody_seqs = batch['antibody_seqs']\n",
        "        antigen_seqs = batch['antigen_seqs']\n",
        "        targets = batch['pKd'].to(device)\n",
        "        \n",
        "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "            predictions = model(antibody_seqs, antigen_seqs, device)\n",
        "            loss = criterion(predictions, targets)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def eval_model(model, loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            antibody_seqs = batch['antibody_seqs']\n",
        "            antigen_seqs = batch['antigen_seqs']\n",
        "            batch_targets = batch['pKd'].to(device)\n",
        "            \n",
        "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "                batch_predictions = model(antibody_seqs, antigen_seqs, device)\n",
        "            \n",
        "            predictions.extend(batch_predictions.float().cpu().numpy())\n",
        "            targets.extend(batch_targets.float().cpu().numpy())\n",
        "    \n",
        "    predictions = np.array(predictions)\n",
        "    targets = np.array(targets)\n",
        "    metrics = compute_comprehensive_metrics(targets, predictions)\n",
        "    return metrics, predictions, targets\n",
        "\n",
        "print(\"‚úÖ Training functions defined\")"
      ],
      "metadata": {
        "id": "cell-16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna objective function\n",
        "def objective(trial):\n",
        "    \"\"\"Optuna objective - maximize Spearman correlation\"\"\"\n",
        "    \n",
        "    # Hyperparameter search space\n",
        "    lr = trial.suggest_float('learning_rate', 5e-4, 3e-3, log=True)\n",
        "    dropout = trial.suggest_float('dropout', 0.2, 0.4)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [32, 48, 64])\n",
        "    warmup_epochs = trial.suggest_int('warmup_epochs', 1, 3)\n",
        "    label_smoothing = trial.suggest_float('label_smoothing', 0.0, 0.1)\n",
        "    weight_decay = trial.suggest_float('weight_decay', 0.005, 0.02, log=True)\n",
        "    \n",
        "    # Create model\n",
        "    model = IgT5ESM2_3B_Model(dropout=dropout, freeze_encoders=True)\n",
        "    model = model.to(device)\n",
        "    \n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                              num_workers=4, collate_fn=collate_fn, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                            num_workers=4, collate_fn=collate_fn, pin_memory=True)\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay,\n",
        "        fused=True\n",
        "    )\n",
        "    \n",
        "    # Scheduler\n",
        "    scheduler = get_warmup_cosine_scheduler(optimizer, warmup_epochs, 20)\n",
        "    \n",
        "    # Loss\n",
        "    criterion = FocalMSELoss(gamma=2.0, label_smoothing=label_smoothing)\n",
        "    \n",
        "    # Training (shorter for optimization)\n",
        "    best_spearman = -1\n",
        "    patience_counter = 0\n",
        "    \n",
        "    for epoch in range(20):  # Max 20 epochs for optimization\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, 1.0)\n",
        "        \n",
        "        # Validate\n",
        "        val_metrics, _, _ = eval_model(model, val_loader, device)\n",
        "        val_spearman = val_metrics['spearman']\n",
        "        \n",
        "        if val_spearman > best_spearman:\n",
        "            best_spearman = val_spearman\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "        \n",
        "        # Early stopping for optimization\n",
        "        if patience_counter >= 5:\n",
        "            break\n",
        "        \n",
        "        # Pruning\n",
        "        trial.report(val_spearman, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "        \n",
        "        scheduler.step()\n",
        "    \n",
        "    # Cleanup\n",
        "    del model, optimizer, train_loader, val_loader\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return best_spearman\n",
        "\n",
        "print(\"‚úÖ Optuna objective function defined\")"
      ],
      "metadata": {
        "id": "cell-17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run hyperparameter optimization\n",
        "print(\"=\"*70)\n",
        "print(\"üîç OPTUNA HYPERPARAMETER OPTIMIZATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create study\n",
        "study = optuna.create_study(\n",
        "    direction='maximize',\n",
        "    sampler=TPESampler(seed=42),\n",
        "    pruner=MedianPruner(n_startup_trials=3, n_warmup_steps=5)\n",
        ")\n",
        "\n",
        "# Number of trials (increase for better optimization)\n",
        "N_TRIALS = 10  # Adjust: 10 for quick, 20-30 for thorough\n",
        "\n",
        "print(f\"\\nRunning {N_TRIALS} optimization trials...\")\n",
        "print(\"This will take ~30-60 minutes\\n\")\n",
        "\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ OPTIMIZATION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüèÜ Best Spearman: {study.best_trial.value:.4f}\")\n",
        "print(f\"\\nBest hyperparameters:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"   {key}: {value}\")"
      ],
      "metadata": {
        "id": "cell-18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Train Final Model with Best Hyperparameters"
      ],
      "metadata": {
        "id": "cell-19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get best hyperparameters\n",
        "best_params = study.best_params\n",
        "\n",
        "# Final configuration\n",
        "config = {\n",
        "    'epochs': 50,\n",
        "    'batch_size': best_params['batch_size'],\n",
        "    'lr': best_params['learning_rate'],\n",
        "    'weight_decay': best_params['weight_decay'],\n",
        "    'dropout': best_params['dropout'],\n",
        "    'warmup_epochs': best_params['warmup_epochs'],\n",
        "    'label_smoothing': best_params['label_smoothing'],\n",
        "    'early_stopping_patience': 15,\n",
        "    'reduce_lr_patience': 5,\n",
        "    'max_grad_norm': 1.0\n",
        "}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"üìã FINAL TRAINING CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "for key, value in config.items():\n",
        "    print(f\"   {key}: {value}\")"
      ],
      "metadata": {
        "id": "cell-20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create final model\n",
        "print(\"\\nBuilding final model with optimized hyperparameters...\\n\")\n",
        "\n",
        "model = IgT5ESM2_3B_Model(\n",
        "    dropout=config['dropout'],\n",
        "    freeze_encoders=True,\n",
        "    use_checkpointing=True\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True,\n",
        "                          num_workers=4, collate_fn=collate_fn, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False,\n",
        "                        num_workers=4, collate_fn=collate_fn, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False,\n",
        "                         num_workers=4, collate_fn=collate_fn, pin_memory=True)\n",
        "\n",
        "print(f\"\\n‚úÖ DataLoaders created:\")\n",
        "print(f\"   ‚Ä¢ train: {len(train_loader)} batches\")\n",
        "print(f\"   ‚Ä¢ val: {len(val_loader)} batches\")\n",
        "print(f\"   ‚Ä¢ test: {len(test_loader)} batches\")"
      ],
      "metadata": {
        "id": "cell-21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer and schedulers\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    weight_decay=config['weight_decay'],\n",
        "    fused=True\n",
        ")\n",
        "\n",
        "# Warmup + Cosine scheduler\n",
        "scheduler = get_warmup_cosine_scheduler(\n",
        "    optimizer,\n",
        "    warmup_epochs=config['warmup_epochs'],\n",
        "    total_epochs=config['epochs']\n",
        ")\n",
        "\n",
        "# ReduceLROnPlateau (additional)\n",
        "reduce_lr = ReduceLROnPlateauWrapper(\n",
        "    optimizer,\n",
        "    factor=0.5,\n",
        "    patience=config['reduce_lr_patience'],\n",
        "    min_lr=1e-6\n",
        ")\n",
        "\n",
        "# Loss\n",
        "criterion = FocalMSELoss(gamma=2.0, label_smoothing=config['label_smoothing'])\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    patience=config['early_stopping_patience'],\n",
        "    min_delta=0.001,\n",
        "    mode='max'\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training components configured\")"
      ],
      "metadata": {
        "id": "cell-22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training loop\n",
        "print(\"=\"*70)\n",
        "print(\"üöÄ STARTING FINAL TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "model_save_path = os.path.join(OUTPUT_DIR, 'best_model.pth')\n",
        "best_spearman = -1\n",
        "training_history = {'train_loss': [], 'val_spearman': [], 'val_recall': [], 'epoch': [], 'time_per_epoch': []}\n",
        "\n",
        "print(f\"\\nTarget: Test Spearman > 0.42\\n\")\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    start_time = time.time()\n",
        "    print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    # Train\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    pbar = tqdm(train_loader, desc=f\"Training\")\n",
        "    \n",
        "    for batch in pbar:\n",
        "        antibody_seqs = batch['antibody_seqs']\n",
        "        antigen_seqs = batch['antigen_seqs']\n",
        "        targets = batch['pKd'].to(device)\n",
        "        \n",
        "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "            predictions = model(antibody_seqs, antigen_seqs, device)\n",
        "            loss = criterion(predictions, targets)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['max_grad_norm'])\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "    \n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    epoch_time = time.time() - start_time\n",
        "    \n",
        "    # Validate on FULL validation set\n",
        "    val_metrics, _, _ = eval_model(model, val_loader, device)\n",
        "    val_spearman = val_metrics['spearman']\n",
        "    val_recall = val_metrics['recall_pkd9']\n",
        "    \n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Spearman: {val_spearman:.4f} | Recall: {val_recall:.1f}% | Time: {epoch_time:.1f}s\")\n",
        "    \n",
        "    # Save best\n",
        "    if val_spearman > best_spearman:\n",
        "        best_spearman = val_spearman\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_spearman': val_spearman,\n",
        "            'config': config,\n",
        "            'best_params': best_params\n",
        "        }, model_save_path)\n",
        "        print(f\"‚úÖ Saved best model (Spearman: {val_spearman:.4f})\")\n",
        "    \n",
        "    # Early stopping\n",
        "    if early_stopping(val_spearman, epoch):\n",
        "        print(f\"\\n‚õî Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "    \n",
        "    # Update schedulers\n",
        "    scheduler.step()\n",
        "    reduce_lr.step(val_spearman)\n",
        "    \n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
        "    \n",
        "    # Record history\n",
        "    training_history['train_loss'].append(train_loss)\n",
        "    training_history['val_spearman'].append(val_spearman)\n",
        "    training_history['val_recall'].append(val_recall)\n",
        "    training_history['epoch'].append(epoch + 1)\n",
        "    training_history['time_per_epoch'].append(epoch_time)\n",
        "\n",
        "total_time = sum(training_history['time_per_epoch'])\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"TRAINING COMPLETE!\")\n",
        "print(f\"Best Validation Spearman: {best_spearman:.4f}\")\n",
        "print(f\"Total training time: {total_time/60:.1f} minutes\")\n",
        "print(f\"{'='*70}\")"
      ],
      "metadata": {
        "id": "cell-23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Final Evaluation"
      ],
      "metadata": {
        "id": "cell-24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model and evaluate\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "checkpoint = torch.load(model_save_path, weights_only=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"\\n‚úÖ Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
        "\n",
        "# Validation\n",
        "print(\"\\nEvaluating on validation set...\")\n",
        "val_metrics, val_preds, val_targets = eval_model(model, val_loader, device)\n",
        "\n",
        "print(f\"\\nüìä VALIDATION METRICS:\")\n",
        "print(f\"   Spearman: {val_metrics['spearman']:.4f}\")\n",
        "print(f\"   RMSE:     {val_metrics['rmse']:.4f}\")\n",
        "print(f\"   MAE:      {val_metrics['mae']:.4f}\")\n",
        "print(f\"   R¬≤:       {val_metrics['r2']:.4f}\")\n",
        "print(f\"   Recall:   {val_metrics['recall_pkd9']:.2f}%\")\n",
        "\n",
        "# Test\n",
        "print(\"\\nEvaluating on TEST set...\")\n",
        "test_metrics, test_preds, test_targets = eval_model(model, test_loader, device)\n",
        "\n",
        "print(f\"\\nüìä TEST METRICS (TRUE PERFORMANCE):\")\n",
        "print(f\"   Spearman: {test_metrics['spearman']:.4f} ‚Üê FINAL RESULT\")\n",
        "print(f\"   RMSE:     {test_metrics['rmse']:.4f}\")\n",
        "print(f\"   MAE:      {test_metrics['mae']:.4f}\")\n",
        "print(f\"   R¬≤:       {test_metrics['r2']:.4f}\")\n",
        "print(f\"   Recall:   {test_metrics['recall_pkd9']:.2f}%\")\n",
        "\n",
        "# Save results\n",
        "val_results = pd.DataFrame({\n",
        "    'true_pKd': val_targets, 'pred_pKd': val_preds,\n",
        "    'error': val_preds - val_targets, 'abs_error': np.abs(val_preds - val_targets)\n",
        "})\n",
        "val_results.to_csv(os.path.join(OUTPUT_DIR, 'val_predictions.csv'), index=False)\n",
        "\n",
        "test_results = pd.DataFrame({\n",
        "    'true_pKd': test_targets, 'pred_pKd': test_preds,\n",
        "    'error': test_preds - test_targets, 'abs_error': np.abs(test_preds - test_targets)\n",
        "})\n",
        "test_results.to_csv(os.path.join(OUTPUT_DIR, 'test_predictions.csv'), index=False)\n",
        "\n",
        "# Save all metrics\n",
        "try:\n",
        "    training_time = total_time / 60\n",
        "except NameError:\n",
        "    training_time = None\n",
        "\n",
        "all_metrics = {\n",
        "    'model': 'IgT5 + ESM-2 3B (Optuna Optimized)',\n",
        "    'gpu': 'A100-80GB',\n",
        "    'best_params': best_params,\n",
        "    'validation': {k: float(v) if isinstance(v, (np.floating, np.integer)) else v\n",
        "                   for k, v in val_metrics.items()},\n",
        "    'test': {k: float(v) if isinstance(v, (np.floating, np.integer)) else v\n",
        "            for k, v in test_metrics.items()},\n",
        "    'config': config,\n",
        "    'training_time_minutes': training_time,\n",
        "    'optuna_trials': len(study.trials)\n",
        "}\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'final_metrics.json'), 'w') as f:\n",
        "    json.dump(all_metrics, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úÖ All results saved to: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "cell-25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Training curves\n",
        "ax1 = axes[0]\n",
        "ax1.plot(training_history['epoch'], training_history['val_spearman'], 'g-o', linewidth=2)\n",
        "ax1.axhline(y=best_spearman, color='r', linestyle='--', label=f'Best: {best_spearman:.4f}')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Validation Spearman')\n",
        "ax1.set_title('Training Progress (Optuna Optimized)', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Test predictions\n",
        "ax2 = axes[1]\n",
        "ax2.scatter(test_targets, test_preds, alpha=0.3, s=10, color='orange')\n",
        "ax2.plot([4, 14], [4, 14], 'r--', linewidth=2, label='Perfect')\n",
        "ax2.set_xlabel('True pKd')\n",
        "ax2.set_ylabel('Predicted pKd')\n",
        "ax2.set_title(f'Test Set\\nSpearman: {test_metrics[\"spearman\"]:.4f}', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.set_xlim(4, 14)\n",
        "ax2.set_ylim(4, 14)\n",
        "\n",
        "# Optuna optimization history\n",
        "ax3 = axes[2]\n",
        "trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
        "trial_values = [t.value for t in trials]\n",
        "ax3.plot(range(1, len(trial_values)+1), trial_values, 'b-o', linewidth=2)\n",
        "ax3.axhline(y=study.best_trial.value, color='r', linestyle='--', \n",
        "            label=f'Best: {study.best_trial.value:.4f}')\n",
        "ax3.set_xlabel('Trial')\n",
        "ax3.set_ylabel('Validation Spearman')\n",
        "ax3.set_title('Optuna Optimization History', fontweight='bold')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'results_summary.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualization saved\")"
      ],
      "metadata": {
        "id": "cell-26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üéâ Training Complete!\n",
        "\n",
        "## Summary\n",
        "\n",
        "**Optimizations applied:**\n",
        "- ‚úÖ Optuna hyperparameter search\n",
        "- ‚úÖ Full validation set (not 5% sample)\n",
        "- ‚úÖ Reduced warmup epochs\n",
        "- ‚úÖ ReduceLROnPlateau + Cosine annealing\n",
        "- ‚úÖ Better early stopping patience\n",
        "\n",
        "**Results saved to:** `Google Drive/AbAg_Training_02/training_output_OPTIMIZED/`\n",
        "\n",
        "---\n",
        "\n",
        "**To improve further:**\n",
        "- Increase `N_TRIALS` to 20-30 for more thorough optimization\n",
        "- Try different loss functions (Huber, custom weighted loss)\n",
        "- Ensemble multiple models\n",
        "\n",
        "---\n",
        "\n",
        "**Happy modeling! üß¨üöÄ**"
      ],
      "metadata": {
        "id": "cell-27"
      }
    }
  ]
}
