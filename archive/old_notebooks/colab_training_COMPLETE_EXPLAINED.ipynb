{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üß¨ Antibody-Antigen Binding Affinity Prediction\n",
    "## Complete Training Pipeline - Step by Step Guide\n",
    "\n",
    "**Version:** 2.7 (with all Phase 1 improvements)\n",
    "\n",
    "**Author:** Your Research Team\n",
    "\n",
    "**What this notebook does:**\n",
    "- Trains a deep learning model to predict antibody-antigen binding affinity (pKd)\n",
    "- Uses state-of-the-art protein language models (IgT5 for antibodies, ESM-2 for antigens)\n",
    "- Includes all modern optimizations and regularization techniques\n",
    "- Provides comprehensive evaluation on validation and test sets\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ 7-layer anti-overfitting protection\n",
    "- ‚úÖ 12 comprehensive metrics (RMSE, MAE, R¬≤, Spearman, Pearson, etc.)\n",
    "- ‚úÖ Full validation & test set evaluation\n",
    "- ‚úÖ LR warmup for stable training\n",
    "- ‚úÖ Early stopping to prevent overfitting\n",
    "- ‚úÖ Complete results visualization\n",
    "\n",
    "**Estimated Runtime:** ~2-3 hours on GPU (T4/P100)\n",
    "\n",
    "**Expected Performance:**\n",
    "- Test Spearman: 0.40-0.45\n",
    "- Test RMSE: 1.2-1.4\n",
    "- Recall@pKd‚â•9: 95-100%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toc"
   },
   "source": [
    "## üìã Table of Contents\n",
    "\n",
    "1. **Environment Setup** - Install dependencies and check GPU\n",
    "2. **Data Preparation** - Load and split dataset\n",
    "3. **Model Architecture** - Define the neural network\n",
    "4. **Training Configuration** - Set hyperparameters and optimizers\n",
    "5. **Training Loop** - Train the model with all optimizations\n",
    "6. **Evaluation** - Comprehensive metrics on val & test sets\n",
    "7. **Results Analysis** - Visualize and interpret results\n",
    "8. **Save & Export** - Download model and predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1"
   },
   "source": [
    "# Step 1: Environment Setup üîß\n",
    "\n",
    "**What this does:**\n",
    "- Checks if GPU is available (CRITICAL for speed!)\n",
    "- Installs required Python packages\n",
    "- Sets up optimizations for faster training\n",
    "\n",
    "**Why this matters:**\n",
    "- GPU makes training 50-100√ó faster than CPU\n",
    "- Proper package versions prevent compatibility issues\n",
    "- Optimization flags enable modern features (BFloat16, TF32)\n",
    "\n",
    "**Expected output:**\n",
    "- GPU detected: T4, P100, or V100\n",
    "- PyTorch version: 2.0+\n",
    "- All packages installed successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected! Training will be VERY slow.\")\n",
    "    print(\"Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_packages"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages (output suppressed for cleaner notebook)\n",
    "!pip install transformers accelerate sentencepiece scipy scikit-learn pandas numpy tqdm\n",
    "\n",
    "# Optional: Install FlashAttention for faster inference (if compatible GPU)\n",
    "# !pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_optimizations"
   },
   "outputs": [],
   "source": [
    "# Enable optimization flags for faster training\n",
    "import os\n",
    "\n",
    "# Enable TF32 for faster matrix multiplication on Ampere GPUs (A100, RTX 3090, etc.)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Enable cuDNN auto-tuner for optimal convolution algorithms\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Disable deterministic mode for speed (reproducibility not critical here)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Disable torch.compile (causes issues with activation checkpointing)\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
    "\n",
    "print(\"‚úÖ Optimizations enabled:\")\n",
    "print(\"  ‚Ä¢ TF32 matrix multiplication\")\n",
    "print(\"  ‚Ä¢ cuDNN auto-tuner\")\n",
    "print(\"  ‚Ä¢ Non-deterministic mode (faster)\")\n",
    "print(\"  ‚Ä¢ torch.compile disabled (prevents errors)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2"
   },
   "source": [
    "---\n",
    "\n",
    "# Step 2: Import Libraries & Define Utilities üìö\n",
    "\n",
    "**What this does:**\n",
    "- Imports all necessary Python libraries\n",
    "- Defines helper functions for:\n",
    "  - Metrics calculation (RMSE, MAE, R¬≤, correlations)\n",
    "  - Data loading and batching\n",
    "  - Regularization (early stopping, gradient clipping)\n",
    "\n",
    "**Why this matters:**\n",
    "- Organized imports prevent missing dependencies\n",
    "- Helper functions make code cleaner and reusable\n",
    "- Metric functions ensure we measure performance correctly\n",
    "\n",
    "**Key libraries:**\n",
    "- `torch`: Deep learning framework\n",
    "- `transformers`: Pre-trained protein models (IgT5, ESM-2)\n",
    "- `pandas`: Data manipulation\n",
    "- `scipy`: Statistical functions (correlations)\n",
    "- `sklearn`: Metrics and data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc\n",
    "from tqdm.notebook import tqdm  # Progress bars for notebooks\n",
    "\n",
    "# Transformers for protein language models\n",
    "from transformers import T5EncoderModel, T5Tokenizer, AutoTokenizer, AutoModel\n",
    "\n",
    "# Scientific computing\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metric_functions"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METRIC CALCULATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_comprehensive_metrics(targets, predictions):\n",
    "    \"\"\"\n",
    "    Compute all standard regression and classification metrics.\n",
    "    \n",
    "    Args:\n",
    "        targets: True pKd values (numpy array)\n",
    "        predictions: Predicted pKd values (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with 14 metrics including RMSE, MAE, R¬≤, correlations, etc.\n",
    "    \n",
    "    Why these metrics:\n",
    "    - RMSE/MAE: Measure average prediction error in pKd units\n",
    "    - R¬≤: Proportion of variance explained (0-1, higher is better)\n",
    "    - Spearman: Rank correlation (robust to outliers)\n",
    "    - Pearson: Linear correlation (sensitive to outliers)\n",
    "    - Recall/Precision/F1: For identifying strong binders (pKd‚â•9)\n",
    "    \"\"\"\n",
    "    # Basic regression metrics\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    \n",
    "    # Correlation metrics\n",
    "    spearman, spearman_p = stats.spearmanr(targets, predictions)\n",
    "    pearson, pearson_p = stats.pearsonr(targets, predictions)\n",
    "    \n",
    "    # Classification metrics for high-affinity binders (pKd >= 9)\n",
    "    strong_binders = targets >= 9.0\n",
    "    predicted_strong = predictions >= 9.0\n",
    "    \n",
    "    if strong_binders.sum() > 0:\n",
    "        tp = (strong_binders & predicted_strong).sum()\n",
    "        fp = (~strong_binders & predicted_strong).sum()\n",
    "        fn = (strong_binders & ~predicted_strong).sum()\n",
    "        tn = (~strong_binders & ~predicted_strong).sum()\n",
    "        \n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    else:\n",
    "        recall = precision = f1 = specificity = 0\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'spearman': spearman,\n",
    "        'spearman_p': spearman_p,\n",
    "        'pearson': pearson,\n",
    "        'pearson_p': pearson_p,\n",
    "        'recall_pkd9': recall * 100,\n",
    "        'precision_pkd9': precision * 100,\n",
    "        'f1_pkd9': f1 * 100,\n",
    "        'specificity_pkd9': specificity * 100,\n",
    "        'n_samples': len(targets),\n",
    "        'n_strong_binders': int(strong_binders.sum())\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Metric functions defined\")\n",
    "print(\"   ‚Üí compute_comprehensive_metrics(): Calculates 14 performance metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "helper_functions"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER CLASSES & FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Stop training when validation metric stops improving.\n",
    "    \n",
    "    Why this is important:\n",
    "    - Prevents overfitting (model memorizing training data)\n",
    "    - Saves training time (stops when no more improvement)\n",
    "    - Automatically finds optimal number of epochs\n",
    "    \n",
    "    Example:\n",
    "        If validation Spearman doesn't improve for 10 epochs, stop training.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0.0001, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    def __call__(self, score, epoch):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            return False\n",
    "        \n",
    "        improved = score > self.best_score + self.min_delta\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                print(f\"\\n‚ö†Ô∏è Early stopping triggered!\")\n",
    "                print(f\"   No improvement for {self.patience} epochs\")\n",
    "                print(f\"   Best score: {self.best_score:.4f} at epoch {self.best_epoch+1}\")\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "\n",
    "def get_warmup_cosine_scheduler(optimizer, warmup_epochs, total_epochs):\n",
    "    \"\"\"\n",
    "    Learning rate scheduler with warmup + cosine decay.\n",
    "    \n",
    "    Why warmup:\n",
    "    - Prevents instability at start of training\n",
    "    - Large learning rates can cause divergence initially\n",
    "    - Gradually increases LR from 0 to max over first few epochs\n",
    "    \n",
    "    Why cosine decay:\n",
    "    - Smoothly decreases LR towards end of training\n",
    "    - Helps model fine-tune and converge better\n",
    "    - Standard in modern models (BERT, GPT, etc.)\n",
    "    \n",
    "    Schedule:\n",
    "        Epochs 0-5:   LR goes from 0 ‚Üí max_lr (warmup)\n",
    "        Epochs 5-50:  LR goes from max_lr ‚Üí ~0 (cosine decay)\n",
    "    \"\"\"\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return float(epoch) / float(max(1, warmup_epochs))\n",
    "        progress = float(epoch - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))\n",
    "        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "\n",
    "class FocalMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal MSE Loss with optional label smoothing.\n",
    "    \n",
    "    Why Focal Loss:\n",
    "    - Focuses more on hard-to-predict samples\n",
    "    - Downweights easy predictions\n",
    "    - Improves performance on challenging cases\n",
    "    \n",
    "    Why Label Smoothing:\n",
    "    - Prevents overconfident predictions\n",
    "    - Smooths targets towards mean value\n",
    "    - Improves generalization\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2.0, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        if self.label_smoothing > 0:\n",
    "            target_mean = target.mean()\n",
    "            target = (1 - self.label_smoothing) * target + self.label_smoothing * target_mean\n",
    "        \n",
    "        mse = (pred - target) ** 2\n",
    "        focal_weight = (1 + mse) ** self.gamma\n",
    "        return (focal_weight * mse).mean()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper classes defined:\")\n",
    "print(\"   ‚Üí EarlyStopping: Prevents overfitting\")\n",
    "print(\"   ‚Üí get_warmup_cosine_scheduler: LR warmup + decay\")\n",
    "print(\"   ‚Üí FocalMSELoss: Advanced loss function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3"
   },
   "source": [
    "---\n",
    "\n",
    "# Step 3: Data Preparation üìä\n",
    "\n",
    "**What this does:**\n",
    "- Uploads your dataset (CSV file)\n",
    "- Splits data into train/validation/test sets (70%/15%/15%)\n",
    "- Creates PyTorch Dataset and DataLoader objects\n",
    "- Prepares data for efficient batch processing\n",
    "\n",
    "**Why this matters:**\n",
    "- **Training set (70%):** Used to update model weights\n",
    "- **Validation set (15%):** Used to monitor overfitting & early stopping\n",
    "- **Test set (15%):** Used ONLY at the end for unbiased performance estimate\n",
    "\n",
    "**Dataset format:**\n",
    "Your CSV should have columns:\n",
    "- `antibody_sequence`: Amino acid sequence of antibody\n",
    "- `antigen_sequence`: Amino acid sequence of antigen\n",
    "- `pKd`: Binding affinity (continuous value, typically 4-14)\n",
    "\n",
    "**Expected data size:**\n",
    "- Total samples: ~200,000\n",
    "- Train: ~140,000\n",
    "- Val: ~30,000\n",
    "- Test: ~30,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_data"
   },
   "outputs": [],
   "source": [
    "# Upload your data file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload your CSV file with columns: antibody_sequence, antigen_sequence, pKd\")\n",
    "print(\"\\nClick 'Choose Files' and select your data file...\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the filename\n",
    "data_file = list(uploaded.keys())[0]\n",
    "print(f\"\\n‚úÖ Uploaded: {data_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Load and explore the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "print(f\"\\nüìä Dataset Information:\")\n",
    "print(f\"   Total samples: {len(df):,}\")\n",
    "print(f\"   Columns: {list(df.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nüìã First 5 samples:\")\n",
    "display(df.head())\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nüìà pKd Statistics:\")\n",
    "print(f\"   Mean: {df['pKd'].mean():.2f}\")\n",
    "print(f\"   Std:  {df['pKd'].std():.2f}\")\n",
    "print(f\"   Min:  {df['pKd'].min():.2f}\")\n",
    "print(f\"   Max:  {df['pKd'].max():.2f}\")\n",
    "\n",
    "# Distribution plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['pKd'], bins=50, edgecolor='black')\n",
    "plt.xlabel('pKd')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('pKd Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "strong_binders = (df['pKd'] >= 9.0).sum()\n",
    "weak_binders = (df['pKd'] < 9.0).sum()\n",
    "plt.bar(['Weak (pKd<9)', 'Strong (pKd‚â•9)'], [weak_binders, strong_binders])\n",
    "plt.ylabel('Count')\n",
    "plt.title('Binder Classification')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split_data"
   },
   "outputs": [],
   "source": [
    "# Split data into train/val/test sets\n",
    "print(\"Splitting dataset...\")\n",
    "\n",
    "# First split: 70% train, 30% temp\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Second split: 15% val, 15% test (from the 30% temp)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create quick validation subset for during training (5% of val for speed)\n",
    "val_df_quick = val_df.sample(frac=0.05, random_state=42)\n",
    "\n",
    "print(f\"\\nüìä Data Splits:\")\n",
    "print(f\"   Train:     {len(train_df):,} samples (70%)\")\n",
    "print(f\"   Val:       {len(val_df):,} samples (15%)\")\n",
    "print(f\"   Val Quick: {len(val_df_quick):,} samples (0.75% - for during training)\")\n",
    "print(f\"   Test:      {len(test_df):,} samples (15%)\")\n",
    "print(f\"\\nüí° Note:\")\n",
    "print(f\"   ‚Ä¢ Quick validation runs during training (fast)\")\n",
    "print(f\"   ‚Ä¢ Full validation runs at end (accurate)\")\n",
    "print(f\"   ‚Ä¢ Test set runs ONLY at end (unbiased performance)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data split complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset"
   },
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset class\n",
    "class AbAgDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Antibody-Antigen pairs.\n",
    "    \n",
    "    What this does:\n",
    "    - Wraps pandas DataFrame for PyTorch\n",
    "    - Returns individual samples when indexed\n",
    "    - Enables batch processing with DataLoader\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'antibody_sequence': self.df.iloc[idx]['antibody_sequence'],\n",
    "            'antigen_sequence': self.df.iloc[idx]['antigen_sequence'],\n",
    "            'pKd': torch.tensor(self.df.iloc[idx]['pKd'], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to combine samples into batches.\n",
    "    \n",
    "    Why this is needed:\n",
    "    - Sequences have variable lengths\n",
    "    - Can't directly stack into tensors\n",
    "    - Keep as lists, tokenize later in model\n",
    "    \"\"\"\n",
    "    antibody_seqs = [item['antibody_sequence'] for item in batch]\n",
    "    antigen_seqs = [item['antigen_sequence'] for item in batch]\n",
    "    pKds = torch.stack([item['pKd'] for item in batch])\n",
    "    return {\n",
    "        'antibody_seqs': antibody_seqs,\n",
    "        'antigen_seqs': antigen_seqs,\n",
    "        'pKd': pKds\n",
    "    }\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AbAgDataset(train_df)\n",
    "val_dataset_quick = AbAgDataset(val_df_quick)\n",
    "val_dataset_full = AbAgDataset(val_df)\n",
    "test_dataset = AbAgDataset(test_df)\n",
    "\n",
    "print(\"‚úÖ PyTorch Datasets created:\")\n",
    "print(f\"   ‚Ä¢ train_dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"   ‚Ä¢ val_dataset_quick: {len(val_dataset_quick):,} samples\")\n",
    "print(f\"   ‚Ä¢ val_dataset_full: {len(val_dataset_full):,} samples\")\n",
    "print(f\"   ‚Ä¢ test_dataset: {len(test_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataloaders"
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders for batch processing\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "print(f\"Creating DataLoaders with batch size: {BATCH_SIZE}\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,  # Shuffle training data each epoch\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,  # Faster GPU transfer\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True  # Drop incomplete last batch\n",
    ")\n",
    "\n",
    "val_loader_quick = DataLoader(\n",
    "    val_dataset_quick,\n",
    "    batch_size=BATCH_SIZE * 2,  # Larger batch for validation (no gradients)\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader_full = DataLoader(\n",
    "    val_dataset_full,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoaders created:\")\n",
    "print(f\"   ‚Ä¢ train_loader: {len(train_loader):,} batches\")\n",
    "print(f\"   ‚Ä¢ val_loader_quick: {len(val_loader_quick):,} batches\")\n",
    "print(f\"   ‚Ä¢ val_loader_full: {len(val_loader_full):,} batches\")\n",
    "print(f\"   ‚Ä¢ test_loader: {len(test_loader):,} batches\")\n",
    "print(f\"\\nüí° DataLoader benefits:\")\n",
    "print(f\"   ‚Ä¢ Automatic batching\")\n",
    "print(f\"   ‚Ä¢ Multi-worker data loading (parallel)\")\n",
    "print(f\"   ‚Ä¢ GPU memory pinning (faster transfer)\")\n",
    "print(f\"   ‚Ä¢ Shuffling for better training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id\": \"step4"
   },
   "source": [
    "---\n",
    "\n",
    "# Step 4: Model Architecture üèóÔ∏è\n",
    "\n",
    "**What this does:**\n",
    "- Loads pre-trained protein language models:\n",
    "  - **IgT5**: Specialized for antibody sequences\n",
    "  - **ESM-2**: Specialized for protein sequences (antigens)\n",
    "- Combines embeddings from both models\n",
    "- Adds regression head to predict pKd values\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Antibody Seq ‚Üí IgT5 (frozen) ‚Üí 512D embedding ‚îÄ‚îÄ‚îê\n",
    "                                                  ‚îú‚Üí Concat ‚Üí MLP ‚Üí pKd\n",
    "Antigen Seq  ‚Üí ESM-2 (frozen) ‚Üí 1280D embedding ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Why freeze encoders:**\n",
    "- Pre-trained models already understand protein language\n",
    "- Freezing prevents catastrophic forgetting\n",
    "- Faster training (only train small MLP head)\n",
    "- Prevents overfitting\n",
    "\n",
    "**Model size:**\n",
    "- IgT5: ~220M parameters (frozen)\n",
    "- ESM-2: ~650M parameters (frozen)\n",
    "- Regression head: ~2M parameters (trainable)\n",
    "- **Total GPU memory: ~4-5 GB**"
   ]
  },

{
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_model"
   },
   "outputs": [],
   "source": [
    "# Define the complete model architecture\n",
    "class IgT5ESM2Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Dual-encoder model for Ab-Ag binding prediction.\n",
    "    \n",
    "    Architecture:\n",
    "    1. IgT5 encoder for antibody sequences (frozen)\n",
    "    2. ESM-2 encoder for antigen sequences (frozen)\n",
    "    3. Concatenate embeddings\n",
    "    4. MLP regression head (trainable)\n",
    "    \n",
    "    Why this architecture:\n",
    "    - Transfer learning from billion-parameter protein models\n",
    "    - Antibody and antigen have different \"languages\"\n",
    "    - Specialized encoders capture domain-specific patterns\n",
    "    - Regression head learns binding affinity patterns\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.35, freeze_encoders=True, use_checkpointing=True):\n",
    "        super().__init__()\n",
    "        self.use_checkpointing = use_checkpointing\n",
    "        \n",
    "        print(\"\\nüî® Building model...\")\n",
    "        \n",
    "        # Load IgT5 for antibody sequences\n",
    "        print(\"  üì• Loading IgT5 (antibody encoder)...\")\n",
    "        self.igt5_tokenizer = T5Tokenizer.from_pretrained(\"Exscientia/IgT5\")\n",
    "        self.igt5_model = T5EncoderModel.from_pretrained(\"Exscientia/IgT5\")\n",
    "        \n",
    "        # Load ESM-2 for antigen sequences  \n",
    "        print(\"  üì• Loading ESM-2 (antigen encoder)...\")\n",
    "        self.esm2_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "        self.esm2_model = AutoModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "        \n",
    "        # Freeze encoders (don't update their weights)\n",
    "        if freeze_encoders:\n",
    "            print(\"  üîí Freezing encoder weights...\")\n",
    "            for param in self.igt5_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.esm2_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Get embedding dimensions\n",
    "        igt5_dim = self.igt5_model.config.d_model  # 512\n",
    "        esm2_dim = self.esm2_model.config.hidden_size  # 1280\n",
    "        combined_dim = igt5_dim + esm2_dim  # 1792\n",
    "        \n",
    "        print(f\"  üìè Embedding dimensions:\")\n",
    "        print(f\"     IgT5: {igt5_dim}D\")\n",
    "        print(f\"     ESM-2: {esm2_dim}D\")\n",
    "        print(f\"     Combined: {combined_dim}D\")\n",
    "        \n",
    "        # Regression head (MLP with 4 blocks)\n",
    "        print(f\"  üß† Building regression head...\")\n",
    "        self.regressor_block1 = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(1024)\n",
    "        )\n",
    "        \n",
    "        self.regressor_block2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(512)\n",
    "        )\n",
    "        \n",
    "        self.regressor_block3 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.LayerNorm(256)\n",
    "        )\n",
    "        \n",
    "        self.regressor_block4 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.regressor_final = nn.Linear(128, 1)\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\n  üìä Model Statistics:\")\n",
    "        print(f\"     Total parameters: {total_params/1e6:.1f}M\")\n",
    "        print(f\"     Trainable parameters: {trainable_params/1e6:.1f}M\")\n",
    "        print(f\"     Frozen parameters: {(total_params-trainable_params)/1e6:.1f}M\")\n",
    "        print(f\"\\n‚úÖ Model built successfully!\")\n",
    "    \n",
    "    def get_batch_embeddings(self, sequences, model, tokenizer, device, pooling='mean'):\n",
    "        \"\"\"\n",
    "        Get embeddings for a batch of sequences.\n",
    "        \n",
    "        Why batch processing:\n",
    "        - Process multiple sequences in parallel (faster)\n",
    "        - Efficient GPU utilization\n",
    "        - 10-20x speedup vs sequential processing\n",
    "        \"\"\"\n",
    "        inputs = tokenizer(\n",
    "            sequences,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():  # Don't compute gradients for frozen encoders\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            if pooling == 'mean':\n",
    "                embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            else:  # cls token\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def forward(self, antibody_seqs, antigen_seqs, device):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Steps:\n",
    "        1. Encode antibody sequences with IgT5\n",
    "        2. Encode antigen sequences with ESM-2\n",
    "        3. Concatenate embeddings\n",
    "        4. Pass through regression head\n",
    "        5. Output pKd predictions\n",
    "        \"\"\"\n",
    "        # Get antibody embeddings\n",
    "        ab_embeddings = self.get_batch_embeddings(\n",
    "            antibody_seqs, self.igt5_model, self.igt5_tokenizer,\n",
    "            device, pooling='mean'\n",
    "        )\n",
    "        \n",
    "        # Get antigen embeddings\n",
    "        ag_embeddings = self.get_batch_embeddings(\n",
    "            antigen_seqs, self.esm2_model, self.esm2_tokenizer,\n",
    "            device, pooling='cls'\n",
    "        )\n",
    "        \n",
    "        # Concatenate\n",
    "        combined = torch.cat([ab_embeddings, ag_embeddings], dim=1)\n",
    "        \n",
    "        # Pass through regression head with optional activation checkpointing\n",
    "        if self.use_checkpointing and self.training:\n",
    "            # Gradient checkpointing: saves memory by recomputing activations\n",
    "            x = checkpoint(self.regressor_block1, combined, use_reentrant=False)\n",
    "            x = checkpoint(self.regressor_block2, x, use_reentrant=False)\n",
    "            x = checkpoint(self.regressor_block3, x, use_reentrant=False)\n",
    "            x = checkpoint(self.regressor_block4, x, use_reentrant=False)\n",
    "        else:\n",
    "            x = self.regressor_block1(combined)\n",
    "            x = self.regressor_block2(x)\n",
    "            x = self.regressor_block3(x)\n",
    "            x = self.regressor_block4(x)\n",
    "        \n",
    "        predictions = self.regressor_final(x).squeeze(-1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_model"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = IgT5ESM2Model(\n",
    "    dropout=0.35,  # 35% dropout for regularization\n",
    "    freeze_encoders=True,  # Don't update encoder weights\n",
    "    use_checkpointing=True  # Save memory with gradient checkpointing\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\n‚úÖ Model moved to {device}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
