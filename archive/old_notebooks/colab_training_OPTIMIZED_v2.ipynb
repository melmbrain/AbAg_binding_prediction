{"cells":[{"cell_type":"markdown","metadata":{"id":"header"},"source":["# Antibody-Antigen Binding Prediction - OPTIMIZED v2\n","\n","## Research-Enhanced Training with Advanced Regularization\n","\n","**Key Improvements (Based on 2024 Research):**\n","- Cross-attention between antibody and antigen embeddings\n","- Cosine annealing with warm restarts (escape local minima)\n","- Combined loss: Huber + Spearman correlation\n","- Stratified batching by pKd range (prevent model collapse)\n","- Mixup augmentation for embeddings\n","- Multi-task learning (regression + classification)\n","- Optuna hyperparameter optimization\n","\n","**Expected Performance:**\n","- Test Spearman: **0.45-0.55**\n","- Recall (pKd>=9): **60-80%**\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"setup-header"},"source":["# Step 1: Environment Setup"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"check-gpu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944777492,"user_tz":-540,"elapsed":4,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"4adb2467-dfce-486c-c34f-89519b894e03"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch: 2.9.0+cu126\n","CUDA: True\n","GPU: NVIDIA A100-SXM4-80GB (85.2GB)\n"]}],"source":["# Check GPU\n","import torch\n","import sys\n","\n","print(f\"PyTorch: {torch.__version__}\")\n","print(f\"CUDA: {torch.cuda.is_available()}\")\n","\n","if torch.cuda.is_available():\n","    gpu_name = torch.cuda.get_device_name(0)\n","    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n","    print(f\"GPU: {gpu_name} ({gpu_memory:.1f}GB)\")\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","    print(\"WARNING: No GPU!\")"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"install","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944782398,"user_tz":-540,"elapsed":4905,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"86ed0bf8-5c55-4745-884f-9b3d7faa2b73"},"outputs":[{"output_type":"stream","name":"stdout","text":["Packages installed!\n"]}],"source":["# Install packages\n","!pip install -q transformers>=4.41.0 sentencepiece optuna\n","print(\"Packages installed!\")"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"a100-opt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944782404,"user_tz":-540,"elapsed":4,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"c54e8517-e88b-4552-82f1-f2ef1d7b99e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["A100 optimizations enabled\n"]}],"source":["# A100 optimizations\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","torch.backends.cudnn.benchmark = True\n","torch.set_float32_matmul_precision('high')\n","print(\"A100 optimizations enabled\")"]},{"cell_type":"markdown","metadata":{"id":"imports-header"},"source":["# Step 2: Imports & Utilities"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"imports","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944782416,"user_tz":-540,"elapsed":11,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"bb3ead3c-6dc6-4913-f920-61e2b89f4f1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Libraries imported!\n","Random seed set to 42\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import json\n","import os\n","import time\n","import math\n","import random\n","from tqdm.auto import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","from torch.utils.tensorboard import SummaryWriter  # TensorBoard for PyTorch\n","\n","from transformers import T5Tokenizer, T5EncoderModel, AutoTokenizer, AutoModel\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from scipy import stats\n","\n","import optuna\n","from optuna.samplers import TPESampler\n","from optuna.pruners import MedianPruner\n","\n","# Set random seeds for reproducibility\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","    torch.cuda.manual_seed_all(SEED)\n","\n","print(\"Libraries imported!\")\n","print(f\"Random seed set to {SEED}\")"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"utilities","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944782420,"user_tz":-540,"elapsed":3,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"bf997642-8960-4dd2-f0f4-1f6312d9d1ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Utilities defined!\n"]}],"source":["# Comprehensive metrics\n","def compute_metrics(targets, predictions):\n","    mse = mean_squared_error(targets, predictions)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(targets, predictions)\n","    r2 = r2_score(targets, predictions)\n","    spearman, _ = stats.spearmanr(targets, predictions)\n","    pearson, _ = stats.pearsonr(targets, predictions)\n","\n","    # Classification metrics at pKd=9\n","    strong = targets >= 9.0\n","    pred_strong = predictions >= 9.0\n","    tp = np.sum(strong & pred_strong)\n","    fn = np.sum(strong & ~pred_strong)\n","    fp = np.sum(~strong & pred_strong)\n","\n","    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","\n","    return {\n","        'rmse': rmse, 'mae': mae, 'r2': r2,\n","        'spearman': spearman, 'pearson': pearson,\n","        'recall': recall * 100, 'precision': precision * 100\n","    }\n","\n","# Early Stopping\n","class EarlyStopping:\n","    def __init__(self, patience=15, min_delta=0.001):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","\n","    def __call__(self, score):\n","        if self.best_score is None or score > self.best_score + self.min_delta:\n","            self.best_score = score\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        return self.early_stop\n","\n","print(\"Utilities defined!\")"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"losses","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944782433,"user_tz":-540,"elapsed":9,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"73bfc7e1-74b6-479e-bb56-9bc1bd3d2761"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loss functions defined (data-informed, therapeutic threshold=9)!\n"]}],"source":["# Advanced Loss Functions (Data-Informed)\n","\n","class HuberLoss(nn.Module):\n","    \"\"\"Huber loss - robust to outliers\"\"\"\n","    def __init__(self, delta=1.0):\n","        super().__init__()\n","        self.delta = delta\n","\n","    def forward(self, pred, target):\n","        diff = torch.abs(pred - target)\n","        loss = torch.where(\n","            diff < self.delta,\n","            0.5 * diff ** 2,\n","            self.delta * (diff - 0.5 * self.delta)\n","        )\n","        return loss.mean()\n","\n","class CombinedLoss(nn.Module):\n","    \"\"\"Combined loss: Huber + Soft Spearman + Classification\n","\n","    Data-informed design:\n","    - Dataset is BIMODAL (peaks at 6-7 and 9-10)\n","    - Soft Spearman forces model to learn both modes\n","    - Classification threshold at 9 (therapeutic standard)\n","\n","    Primary goal: Accurate pKd prediction\n","    Secondary goal: Identify therapeutic candidates (pKd >= 9)\n","    \"\"\"\n","    def __init__(self, huber_weight=0.5, spearman_weight=0.4, class_weight=0.1):\n","        super().__init__()\n","        self.huber = HuberLoss(delta=1.0)\n","        self.huber_weight = huber_weight\n","        self.spearman_weight = spearman_weight\n","        self.class_weight = class_weight\n","        self.bce = nn.BCEWithLogitsLoss()\n","\n","    def soft_spearman_loss(self, pred, target, temperature=1.0):\n","        \"\"\"Differentiable Spearman correlation loss using soft ranking\n","\n","        This is critical for bimodal data - forces model to learn\n","        correct ranking across both low and high affinity peaks.\n","        \"\"\"\n","        # Compute pairwise differences\n","        pred_diff = pred.unsqueeze(1) - pred.unsqueeze(0)  # [B, B]\n","        target_diff = target.unsqueeze(1) - target.unsqueeze(0)  # [B, B]\n","\n","        # Soft ranking using sigmoid\n","        pred_rank = torch.sigmoid(pred_diff / temperature).sum(dim=1)  # [B]\n","        target_rank = torch.sigmoid(target_diff / temperature).sum(dim=1)  # [B]\n","\n","        # Normalize ranks\n","        pred_rank = (pred_rank - pred_rank.mean()) / (pred_rank.std() + 1e-8)\n","        target_rank = (target_rank - target_rank.mean()) / (target_rank.std() + 1e-8)\n","\n","        # Correlation\n","        corr = (pred_rank * target_rank).mean()\n","        return 1 - corr  # Loss = 1 - correlation\n","\n","    def forward(self, pred, target, class_logits=None):\n","        # Primary losses for affinity prediction\n","        huber_loss = self.huber(pred, target)\n","        spearman_loss = self.soft_spearman_loss(pred, target)\n","\n","        loss = self.huber_weight * huber_loss + self.spearman_weight * spearman_loss\n","\n","        # Classification: therapeutic threshold (pKd >= 9)\n","        if class_logits is not None:\n","            class_target = (target >= 9.0).float()  # Therapeutic standard\n","            class_loss = self.bce(class_logits, class_target)\n","            loss += self.class_weight * class_loss\n","\n","        return loss\n","\n","print(\"Loss functions defined (data-informed, therapeutic threshold=9)!\")"]},{"cell_type":"markdown","metadata":{"id":"data-header"},"source":["# Step 3: Load Data"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"mount-drive","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944783705,"user_tz":-540,"elapsed":1271,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"fa81bb2c-be96-41c3-e5f1-5cb8315bf180"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Output: /content/drive/MyDrive/AbAg_Training_02/training_output_OPTIMIZED_v2\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","DRIVE_DIR = '/content/drive/MyDrive/AbAg_Training_02'\n","OUTPUT_DIR = f'{DRIVE_DIR}/training_output_OPTIMIZED_v2'\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","print(f\"Output: {OUTPUT_DIR}\")"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"load-data","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944785180,"user_tz":-540,"elapsed":1475,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"d86a424c-5d68-43b4-963b-dcb1a7613f95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset: 159,735 samples\n","pKd range: -3.0 - 12.4\n","Strong binders: 54,741 (34.3%)\n"]}],"source":["# Load dataset\n","CSV_FILENAME = 'agab_phase2_full.csv'  # <- CHANGE THIS\n","\n","df = pd.read_csv(os.path.join(DRIVE_DIR, CSV_FILENAME))\n","print(f\"Dataset: {len(df):,} samples\")\n","print(f\"pKd range: {df['pKd'].min():.1f} - {df['pKd'].max():.1f}\")\n","print(f\"Strong binders: {(df['pKd']>=9).sum():,} ({100*(df['pKd']>=9).mean():.1f}%)\")"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"split-data","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944785328,"user_tz":-540,"elapsed":143,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"9defe0fb-8878-446a-b272-cca8b03363f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train: 111,814 | Val: 23,960 | Test: 23,961\n"]}],"source":["# Split data with stratification by pKd bins\n","df['pKd_bin'] = pd.cut(df['pKd'], bins=5, labels=False)\n","\n","train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['pKd_bin'])\n","val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['pKd_bin'])\n","\n","print(f\"Train: {len(train_df):,} | Val: {len(val_df):,} | Test: {len(test_df):,}\")"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"dataset","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944786199,"user_tz":-540,"elapsed":865,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"f70ca066-4cae-444c-d48b-18748ab4f3ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Datasets created with stratified sampling weights!\n"]}],"source":["# Dataset with stratified sampling support\n","class AbAgDataset(Dataset):\n","    def __init__(self, dataframe):\n","        self.data = dataframe.reset_index(drop=True)\n","        # Compute sample weights for stratified sampling\n","        pKd_bins = pd.cut(self.data['pKd'], bins=5, labels=False)\n","        bin_counts = pKd_bins.value_counts()\n","        # Use map for proper indexing (handles any bin indices)\n","        self.weights = pKd_bins.map(lambda x: 1.0 / bin_counts[x] if pd.notna(x) else 1.0).values\n","        self.weights = self.weights / self.weights.sum()\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","        return {\n","            'antibody_seqs': row['antibody_sequence'],\n","            'antigen_seqs': row['antigen_sequence'],\n","            'pKd': torch.tensor(row['pKd'], dtype=torch.float32)\n","        }\n","\n","def collate_fn(batch):\n","    return {\n","        'antibody_seqs': [item['antibody_seqs'] for item in batch],\n","        'antigen_seqs': [item['antigen_seqs'] for item in batch],\n","        'pKd': torch.stack([item['pKd'] for item in batch])\n","    }\n","\n","train_dataset = AbAgDataset(train_df)\n","val_dataset = AbAgDataset(val_df)\n","test_dataset = AbAgDataset(test_df)\n","\n","print(\"Datasets created with stratified sampling weights!\")"]},{"cell_type":"markdown","metadata":{"id":"model-header"},"source":["# Step 4: Enhanced Model Architecture\n","\n","**New Features:**\n","- Cross-attention between Ab and Ag embeddings\n","- Multi-task output (regression + classification)\n","- Spectral normalization in regression head"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"cross-attention","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944786211,"user_tz":-540,"elapsed":11,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"b1b062cb-8302-403a-f065-e4291b003c42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cross-attention module defined!\n"]}],"source":["# Cross-Attention Module\n","class CrossAttention(nn.Module):\n","    \"\"\"Cross-attention between antibody and antigen embeddings\"\"\"\n","    def __init__(self, dim, num_heads=8, dropout=0.1):\n","        super().__init__()\n","        self.attention = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.norm2 = nn.LayerNorm(dim)\n","        self.ffn = nn.Sequential(\n","            nn.Linear(dim, dim * 4),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(dim * 4, dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, query, key_value):\n","        # Cross-attention\n","        attn_out, _ = self.attention(query, key_value, key_value)\n","        query = self.norm1(query + attn_out)\n","        # FFN\n","        ffn_out = self.ffn(query)\n","        query = self.norm2(query + ffn_out)\n","        return query\n","\n","print(\"Cross-attention module defined!\")"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"model","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944786218,"user_tz":-540,"elapsed":3,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"4b61630a-b4fb-4879-903f-31c5bb93fe7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Enhanced model class defined!\n"]}],"source":["# Enhanced Model with Cross-Attention\n","class EnhancedAbAgModel(nn.Module):\n","    def __init__(self, dropout=0.3, use_cross_attention=True, use_esm2_3b=True):\n","        super().__init__()\n","\n","        print(\"Building enhanced model...\")\n","\n","        # IgT5 for antibodies\n","        self.igt5_tokenizer = T5Tokenizer.from_pretrained(\"Exscientia/IgT5\")\n","        self.igt5_model = T5EncoderModel.from_pretrained(\"Exscientia/IgT5\")\n","        self.igt5_dim = 1024  # IgT5 outputs 1024-dim embeddings\n","\n","        # ESM-2 for antigens\n","        if use_esm2_3b:\n","            print(\"  Loading ESM-2 3B...\")\n","            self.esm2_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t36_3B_UR50D\")\n","            self.esm2_model = AutoModel.from_pretrained(\"facebook/esm2_t36_3B_UR50D\")\n","            self.esm2_dim = 2560\n","        else:\n","            print(\"  Loading ESM-2 650M...\")\n","            self.esm2_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n","            self.esm2_model = AutoModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n","            self.esm2_dim = 1280\n","\n","        # Freeze encoders\n","        for param in self.igt5_model.parameters():\n","            param.requires_grad = False\n","        for param in self.esm2_model.parameters():\n","            param.requires_grad = False\n","\n","        # Enable gradient checkpointing\n","        self.igt5_model.gradient_checkpointing_enable()\n","        self.esm2_model.gradient_checkpointing_enable()\n","\n","        # Projection layers to common dimension\n","        self.common_dim = 512\n","        self.ab_proj = nn.Linear(self.igt5_dim, self.common_dim)  # 1024 -> 512\n","        self.ag_proj = nn.Linear(self.esm2_dim, self.common_dim)  # 2560 -> 512\n","\n","        # Cross-attention\n","        self.use_cross_attention = use_cross_attention\n","        if use_cross_attention:\n","            self.cross_attn_ab = CrossAttention(self.common_dim, num_heads=8, dropout=dropout)\n","            self.cross_attn_ag = CrossAttention(self.common_dim, num_heads=8, dropout=dropout)\n","\n","        # Regression head with spectral normalization\n","        self.regression_head = nn.Sequential(\n","            nn.utils.spectral_norm(nn.Linear(self.common_dim * 2, 512)),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.LayerNorm(512),\n","\n","            nn.utils.spectral_norm(nn.Linear(512, 256)),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.LayerNorm(256),\n","\n","            nn.utils.spectral_norm(nn.Linear(256, 128)),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.LayerNorm(128),\n","\n","            nn.Linear(128, 1)\n","        )\n","\n","        # Classification head (auxiliary task)\n","        self.classifier = nn.Linear(self.common_dim * 2, 1)\n","\n","        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n","        print(f\"  Trainable parameters: {trainable/1e6:.1f}M\")\n","\n","    def forward(self, antibody_seqs, antigen_seqs, device):\n","        # Tokenize\n","        ab_tokens = self.igt5_tokenizer(\n","            antibody_seqs, return_tensors='pt', padding=True,\n","            truncation=True, max_length=512\n","        ).to(device)\n","\n","        ag_tokens = self.esm2_tokenizer(\n","            antigen_seqs, return_tensors='pt', padding=True,\n","            truncation=True, max_length=2048\n","        ).to(device)\n","\n","        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n","            # Encode\n","            ab_out = self.igt5_model(**ab_tokens).last_hidden_state\n","            ag_out = self.esm2_model(**ag_tokens).last_hidden_state\n","\n","            # Mean pooling\n","            ab_emb = ab_out.mean(dim=1)  # [B, 1024]\n","            ag_emb = ag_out.mean(dim=1)  # [B, 2560]\n","\n","            # Project to common dimension\n","            ab_proj = self.ab_proj(ab_emb)  # [B, 512]\n","            ag_proj = self.ag_proj(ag_emb)  # [B, 512]\n","\n","            # Cross-attention (optional)\n","            if self.use_cross_attention:\n","                # Add sequence dimension for attention\n","                ab_proj = ab_proj.unsqueeze(1)  # [B, 1, 512]\n","                ag_proj = ag_proj.unsqueeze(1)  # [B, 1, 512]\n","\n","                ab_enhanced = self.cross_attn_ab(ab_proj, ag_proj).squeeze(1)\n","                ag_enhanced = self.cross_attn_ag(ag_proj, ab_proj).squeeze(1)\n","\n","                combined = torch.cat([ab_enhanced, ag_enhanced], dim=1)\n","            else:\n","                combined = torch.cat([ab_proj, ag_proj], dim=1)\n","\n","            # Predictions\n","            pKd_pred = self.regression_head(combined).squeeze(-1)\n","            class_logits = self.classifier(combined).squeeze(-1)\n","\n","        return pKd_pred, class_logits\n","\n","print(\"Enhanced model class defined!\")"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"optuna-header","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944786233,"user_tz":-540,"elapsed":14,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"c9ec2e27-3d12-467b-a071-77347418de2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","HYPERPARAMETERS (Improved for stability)\n","============================================================\n","  Batch size: 32\n","  Learning rate: 0.0005\n","  Warmup epochs: 5\n","  Dropout: 0.3\n","  Loss weights: Huber=0.5, Spearman=0.4, Class=0.1\n","  Cross-attention: True\n","  Epochs: 50\n"]}],"source":["# Step 5: Setup Training with Improved Hyperparameters\n","# Focus: Accurate pKd prediction with stable training\n","\n","# Improved hyperparameters\n","BATCH_SIZE = 32\n","LEARNING_RATE = 5e-4  # Reduced from 1e-3 for stability\n","DROPOUT = 0.3\n","HUBER_WEIGHT = 0.5    # Primary: regression accuracy\n","SPEARMAN_WEIGHT = 0.4  # Primary: ranking correlation\n","CLASS_WEIGHT = 0.1     # Secondary: high binder classification\n","USE_CROSS_ATTENTION = True\n","WARMUP_EPOCHS = 5      # NEW: warmup period\n","EPOCHS = 50\n","\n","print(\"=\"*60)\n","print(\"HYPERPARAMETERS (Improved for stability)\")\n","print(\"=\"*60)\n","print(f\"  Batch size: {BATCH_SIZE}\")\n","print(f\"  Learning rate: {LEARNING_RATE}\")\n","print(f\"  Warmup epochs: {WARMUP_EPOCHS}\")\n","print(f\"  Dropout: {DROPOUT}\")\n","print(f\"  Loss weights: Huber={HUBER_WEIGHT}, Spearman={SPEARMAN_WEIGHT}, Class={CLASS_WEIGHT}\")\n","print(f\"  Cross-attention: {USE_CROSS_ATTENTION}\")\n","print(f\"  Epochs: {EPOCHS}\")"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"train-funcs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763944786268,"user_tz":-540,"elapsed":34,"user":{"displayName":"フや","userId":"13551638859201348604"}},"outputId":"67ba0e0d-3ea8-40ca-c23a-0eb8803d04d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training functions defined!\n"]}],"source":["# Training functions\n","def train_epoch(model, loader, optimizer, criterion, device, max_grad_norm=1.0):\n","    model.train()\n","    total_loss = 0\n","\n","    for batch in loader:\n","        ab_seqs = batch['antibody_seqs']\n","        ag_seqs = batch['antigen_seqs']\n","        targets = batch['pKd'].to(device)\n","\n","        pKd_pred, class_logits = model(ab_seqs, ag_seqs, device)\n","        loss = criterion(pKd_pred, targets, class_logits)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(loader)\n","\n","def evaluate(model, loader, device):\n","    model.eval()\n","    preds, targets = [], []\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            ab_seqs = batch['antibody_seqs']\n","            ag_seqs = batch['antigen_seqs']\n","            batch_targets = batch['pKd'].to(device)\n","\n","            pKd_pred, _ = model(ab_seqs, ag_seqs, device)\n","\n","            preds.extend(pKd_pred.float().cpu().numpy())\n","            targets.extend(batch_targets.float().cpu().numpy())\n","\n","    return compute_metrics(np.array(targets), np.array(preds)), np.array(preds), np.array(targets)\n","\n","print(\"Training functions defined!\")"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":230,"referenced_widgets":["81d82d558106444bbf89d600157440ea","4aa01ad8fc3b46f8b6851b1a33fbecde","6e7ca2930a0c4ef6b8f9730cbf5cdb33","b0c29770e3c5418f954a34f748ee7dda","374b4793a4064cc4b3a857ec0e95627e","014352c59c5d483083d90905f4e68612","3e247ac618af4510a6f76979a89a6f29","119ea7c6285f46df879496059b07f957","6065315783fa410a94ec99fff7685a46","2ac4953b5d8845869c90ec513695ebbc","10f4996946424c82b105df5836a33bd7","39bba7a758974f858df365bf1ebf8707","8282774e438c481187b4585a93b70754","559ecdf1292f49808a0937b473ff0fac","86b483893c2f47bcbce9070aa8641f86","c1e60544232548e695476cfa5a8964bd","c85ee11f36ad499eb5a96bc7ff8e4915","0811d809ddbb4af5b293a4c03ef21134","fd836a78aeb548a7b4af871d5827dc70","5c9585eee19f4a299627c0e6a61644dd","845aeb0baa97405390a51f3ea30cdcc8","964b015d6d154995ac17770288b45aad"]},"executionInfo":{"elapsed":7766,"status":"ok","timestamp":1763944794042,"user":{"displayName":"フや","userId":"13551638859201348604"},"user_tz":-540},"id":"optuna-objective","outputId":"fa27acc2-034d-422e-e314-f99c1155c91f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Building model...\n","Building enhanced model...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81d82d558106444bbf89d600157440ea"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Loading ESM-2 3B...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39bba7a758974f858df365bf1ebf8707"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t36_3B_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["  Trainable parameters: 8.8M\n","Model ready!\n"]}],"source":["# Build model\n","print(\"Building model...\")\n","\n","model = EnhancedAbAgModel(\n","    dropout=DROPOUT,\n","    use_cross_attention=USE_CROSS_ATTENTION,\n","    use_esm2_3b=True\n",").to(device)\n","\n","print(\"Model ready!\")"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1763944794077,"user":{"displayName":"フや","userId":"13551638859201348604"},"user_tz":-540},"id":"run-optuna","outputId":"284f2c01-5149-412d-e541-c92dc8fe61df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting fresh training...\n","TensorBoard logs: /content/drive/MyDrive/AbAg_Training_02/training_output_OPTIMIZED_v2/runs/20251124-003952\n","Train batches: 3495\n","Val batches: 749\n","Warmup steps: 17475\n","Total steps: 174750\n","Fused optimizer: True\n","Schedulers: Warmup + Cosine (per-batch) + ReduceLROnPlateau (per-epoch)\n","Ready for training!\n"]}],"source":["# Setup DataLoaders, optimizer, schedulers with warmup + plateau\n","from torch.utils.data import WeightedRandomSampler\n","from torch.optim.lr_scheduler import LambdaLR, ReduceLROnPlateau\n","import datetime\n","import glob\n","import shutil\n","\n","# DataLoaders with stratified sampling\n","# Use generator for reproducible sampling\n","g = torch.Generator()\n","g.manual_seed(SEED)\n","\n","sampler = WeightedRandomSampler(train_dataset.weights, len(train_dataset), replacement=True, generator=g)\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler,\n","                            num_workers=2, collate_fn=collate_fn, pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n","                        num_workers=2, collate_fn=collate_fn, pin_memory=True)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n","                        num_workers=2, collate_fn=collate_fn, pin_memory=True)\n","\n","# Optimizer with fused AdamW (only if CUDA available)\n","use_fused = torch.cuda.is_available()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01, fused=use_fused)\n","\n","# Linear warmup then cosine decay (per-batch)\n","num_training_steps = len(train_loader) * EPOCHS\n","num_warmup_steps = len(train_loader) * WARMUP_EPOCHS\n","\n","def lr_lambda(current_step):\n","    if current_step < num_warmup_steps:\n","        # Linear warmup\n","        return float(current_step) / float(max(1, num_warmup_steps))\n","    # Cosine decay\n","    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n","    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n","\n","scheduler = LambdaLR(optimizer, lr_lambda)\n","\n","# ReduceLROnPlateau as backup (per-epoch)\n","plateau_scheduler = ReduceLROnPlateau(\n","    optimizer,\n","    mode='max',\n","    factor=0.5,\n","    patience=3,\n","    min_lr=1e-6\n",")\n","\n","# Combined loss\n","criterion = CombinedLoss(HUBER_WEIGHT, SPEARMAN_WEIGHT, CLASS_WEIGHT)\n","\n","# Early stopping\n","early_stopping = EarlyStopping(patience=10, min_delta=0.001)\n","\n","# Checkpoint paths\n","model_path = os.path.join(OUTPUT_DIR, 'best_model.pth')\n","checkpoint_path = os.path.join(OUTPUT_DIR, 'checkpoint_latest.pth')\n","best_spearman = -1\n","start_epoch = 0\n","start_batch = 0  # For mid-epoch resume\n","history = {'loss': [], 'spearman': [], 'recall': [], 'lr': []}\n","log_dir = None  # Will be set below\n","\n","# Store hyperparameters for checkpoint\n","hyperparams = {\n","    'batch_size': BATCH_SIZE,\n","    'learning_rate': LEARNING_RATE,\n","    'dropout': DROPOUT,\n","    'huber_weight': HUBER_WEIGHT,\n","    'spearman_weight': SPEARMAN_WEIGHT,\n","    'class_weight': CLASS_WEIGHT,\n","    'use_cross_attention': USE_CROSS_ATTENTION,\n","    'warmup_epochs': WARMUP_EPOCHS,\n","    'epochs': EPOCHS,\n","    'seed': SEED,\n","}\n","\n","# Function to find and restore from best available checkpoint\n","def find_best_checkpoint():\n","    \"\"\"Find the most recent valid checkpoint if latest is corrupted\"\"\"\n","    checkpoint_files = []\n","    expected_size_mb = 15000  # Checkpoints should be ~15GB\n","\n","    for f in os.listdir(OUTPUT_DIR):\n","        if f.endswith('.pth') and f != 'best_model.pth':\n","            filepath = os.path.join(OUTPUT_DIR, f)\n","            size_mb = os.path.getsize(filepath) / (1024 * 1024)\n","\n","            # Extract step/epoch number for sorting\n","            if 'step_' in f:\n","                num = int(f.split('step_')[1].split('.')[0])\n","                checkpoint_files.append((filepath, f, size_mb, num, 'step'))\n","            elif 'epoch_' in f:\n","                num = int(f.split('epoch_')[1].split('.')[0])\n","                # Convert epoch to approximate step for comparison\n","                checkpoint_files.append((filepath, f, size_mb, num * len(train_loader), 'epoch'))\n","            elif f == 'checkpoint_latest.pth':\n","                checkpoint_files.append((filepath, f, size_mb, -1, 'latest'))\n","\n","    # Check if latest checkpoint is valid (should be ~15GB)\n","    latest_path = os.path.join(OUTPUT_DIR, 'checkpoint_latest.pth')\n","    if os.path.exists(latest_path):\n","        latest_size = os.path.getsize(latest_path) / (1024 * 1024)\n","        if latest_size > expected_size_mb * 0.9:  # Within 90% of expected size\n","            return None  # Latest is valid, no recovery needed\n","\n","    # Find the most recent valid checkpoint\n","    valid_checkpoints = [(p, f, s, n, t) for p, f, s, n, t in checkpoint_files\n","                         if s > expected_size_mb * 0.9 and t != 'latest']\n","\n","    if not valid_checkpoints:\n","        return None  # No valid backups\n","\n","    # Sort by step number (highest first)\n","    valid_checkpoints.sort(key=lambda x: x[3], reverse=True)\n","    return valid_checkpoints[0]  # Return best checkpoint info\n","\n","# Check for corrupted checkpoint and recover\n","if os.path.exists(checkpoint_path):\n","    try:\n","        # Try to load - this will fail if corrupted\n","        test_load = torch.load(checkpoint_path, weights_only=False)\n","        del test_load  # Free memory\n","    except Exception as e:\n","        print(f\"WARNING: checkpoint_latest.pth is corrupted!\")\n","        print(f\"Error: {e}\")\n","\n","        # Find best backup\n","        best_backup = find_best_checkpoint()\n","        if best_backup:\n","            backup_path, backup_name, backup_size, _, _ = best_backup\n","            print(f\"Found valid backup: {backup_name} ({backup_size:.1f} MB)\")\n","\n","            # Delete corrupted and restore from backup\n","            os.remove(checkpoint_path)\n","            shutil.copy(backup_path, checkpoint_path)\n","            print(f\"Restored checkpoint_latest.pth from {backup_name}\")\n","        else:\n","            print(\"No valid backup found. Deleting corrupted checkpoint - will start fresh.\")\n","            os.remove(checkpoint_path)\n","\n","# Resume from checkpoint if exists\n","if os.path.exists(checkpoint_path):\n","    print(\"Found checkpoint, resuming training...\")\n","    checkpoint = torch.load(checkpoint_path, weights_only=False)\n","\n","    # Check hyperparameter consistency\n","    saved_hyperparams = checkpoint.get('hyperparams', {})\n","    if saved_hyperparams:\n","        mismatches = []\n","        for key in ['batch_size', 'learning_rate', 'dropout']:\n","            if key in saved_hyperparams and saved_hyperparams[key] != hyperparams[key]:\n","                mismatches.append(f\"{key}: saved={saved_hyperparams[key]}, current={hyperparams[key]}\")\n","        if mismatches:\n","            print(f\"WARNING: Hyperparameter mismatch detected!\")\n","            for m in mismatches:\n","                print(f\"  {m}\")\n","            print(\"Training will continue with CURRENT hyperparameters.\")\n","\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    best_spearman = checkpoint.get('best_spearman', -1)\n","    history = checkpoint.get('history', history)\n","\n","    # Determine resume point\n","    saved_epoch = checkpoint['epoch']\n","    saved_batch = checkpoint.get('batch_idx', 0)\n","\n","    # Check if this was a mid-epoch save or end-of-epoch save\n","    if saved_batch > 0 and saved_batch < len(train_loader):\n","        # Mid-epoch checkpoint - resume from this epoch at the saved batch\n","        start_epoch = saved_epoch\n","        start_batch = saved_batch\n","        print(f\"Resuming mid-epoch: epoch {start_epoch+1}, batch {start_batch}\")\n","    else:\n","        # End-of-epoch checkpoint - start next epoch\n","        start_epoch = saved_epoch + 1\n","        start_batch = 0\n","        print(f\"Resuming from epoch {start_epoch+1}\")\n","\n","    # Restore scheduler states\n","    if 'scheduler_state_dict' in checkpoint:\n","        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","    else:\n","        # Fallback for old checkpoints\n","        global_step = checkpoint.get('global_step', start_epoch * len(train_loader))\n","        for _ in range(global_step):\n","            scheduler.step()\n","\n","    if 'plateau_scheduler_state_dict' in checkpoint:\n","        plateau_scheduler.load_state_dict(checkpoint['plateau_scheduler_state_dict'])\n","\n","    # Restore early stopping state\n","    if 'early_stopping_best' in checkpoint:\n","        early_stopping.best_score = checkpoint['early_stopping_best']\n","        early_stopping.counter = checkpoint.get('early_stopping_counter', 0)\n","\n","    # Restore RNG states for reproducibility\n","    if 'rng_state' in checkpoint:\n","        torch.set_rng_state(checkpoint['rng_state'])\n","    if 'cuda_rng_state' in checkpoint and torch.cuda.is_available():\n","        torch.cuda.set_rng_state(checkpoint['cuda_rng_state'])\n","    if 'numpy_rng_state' in checkpoint:\n","        np.random.set_state(checkpoint['numpy_rng_state'])\n","    if 'python_rng_state' in checkpoint:\n","        random.setstate(checkpoint['python_rng_state'])\n","    if 'sampler_rng_state' in checkpoint:\n","        g.set_state(checkpoint['sampler_rng_state'])\n","\n","    # Resume TensorBoard to same directory if saved\n","    if 'log_dir' in checkpoint:\n","        log_dir = checkpoint['log_dir']\n","        print(f\"Resuming TensorBoard logs to: {log_dir}\")\n","\n","    print(f\"Best Spearman so far: {best_spearman:.4f}\")\n","else:\n","    print(\"Starting fresh training...\")\n","\n","# TensorBoard logging - create new dir only if not resuming\n","if log_dir is None:\n","    log_dir = os.path.join(OUTPUT_DIR, 'runs', datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n","writer = SummaryWriter(log_dir=log_dir)\n","print(f\"TensorBoard logs: {log_dir}\")\n","\n","print(f\"Train batches: {len(train_loader)}\")\n","print(f\"Val batches: {len(val_loader)}\")\n","print(f\"Warmup steps: {num_warmup_steps}\")\n","print(f\"Total steps: {num_training_steps}\")\n","print(f\"Fused optimizer: {use_fused}\")\n","print(\"Schedulers: Warmup + Cosine (per-batch) + ReduceLROnPlateau (per-epoch)\")\n","print(\"Ready for training!\")"]},{"cell_type":"markdown","metadata":{"id":"final-header"},"source":["# Step 6: Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":161,"referenced_widgets":["4fdd7db102dc4e90b21db85df74ccba3","d1200fa516404ce9b0445d6bd416ded2","6dd9b2f81ce049169edb5b5609603905","b1012f2266c64f11bed0995b02ae79c1","7e85aa2491f648e6b44ef52e189898bb","faf1c9b99dd94a3ca047b573b1ef23e9","24c44e47fb4541f885cae96944859968","380a67b386af43d783a0ee3e123a9fa0","89575066be304a94a267b61623222a8e","07927c5856724b71a7d331358aa36e1c","7f33dbac6b4d49e5ac09a0ab0cae5d27"]},"id":"setup-final","outputId":"5d26ec92-1479-4332-9251-9538cd330b48"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","TRAINING\n","============================================================\n"]},{"output_type":"display_data","data":{"text/plain":["Epoch 1/50:   0%|          | 0/3495 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fdd7db102dc4e90b21db85df74ccba3"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n"]}],"source":["# Training loop with checkpoint resuming + TensorBoard logging\n","print(\"=\"*60)\n","print(\"TRAINING\")\n","print(\"=\"*60)\n","\n","# Track previous checkpoint files for deletion\n","prev_step_checkpoint = None\n","prev_epoch_checkpoint = None\n","\n","def permanent_delete(filepath):\n","    \"\"\"Permanently delete file, bypassing Google Drive trash\"\"\"\n","    if filepath and os.path.exists(filepath):\n","        try:\n","            # Try to use Google Drive API for permanent deletion\n","            from google.colab import drive\n","            import subprocess\n","            # Get the file ID and delete permanently\n","            result = subprocess.run(\n","                ['rm', '-f', filepath],\n","                capture_output=True, text=True\n","            )\n","            # Force flush to ensure deletion\n","            os.sync() if hasattr(os, 'sync') else None\n","        except:\n","            # Fallback to regular delete\n","            os.remove(filepath)\n","\n","def save_checkpoint(path, epoch, global_step, batch_idx=None):\n","    \"\"\"Helper to save checkpoint with all necessary states\"\"\"\n","    checkpoint_data = {\n","        'epoch': epoch,\n","        'global_step': global_step,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'plateau_scheduler_state_dict': plateau_scheduler.state_dict(),\n","        'best_spearman': best_spearman,\n","        'history': history,\n","        'early_stopping_best': early_stopping.best_score,\n","        'early_stopping_counter': early_stopping.counter,\n","        'log_dir': log_dir,\n","        'hyperparams': hyperparams,\n","        # RNG states for exact reproducibility\n","        'rng_state': torch.get_rng_state(),\n","        'numpy_rng_state': np.random.get_state(),\n","        'python_rng_state': random.getstate(),\n","        'sampler_rng_state': g.get_state(),\n","    }\n","    if torch.cuda.is_available():\n","        checkpoint_data['cuda_rng_state'] = torch.cuda.get_rng_state()\n","    if batch_idx is not None:\n","        checkpoint_data['batch_idx'] = batch_idx\n","    torch.save(checkpoint_data, path)\n","\n","for epoch in range(start_epoch, EPOCHS):\n","    start = time.time()\n","\n","    # Train\n","    model.train()\n","    total_loss = 0\n","    num_batches_processed = 0\n","\n","    # Determine where to start in this epoch\n","    skip_batches = start_batch if epoch == start_epoch else 0\n","\n","    if skip_batches > 0:\n","        print(f\"Skipping first {skip_batches} batches (already processed)\")\n","\n","    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n","\n","    for batch_idx, batch in enumerate(pbar):\n","        # Skip batches if resuming mid-epoch\n","        if batch_idx < skip_batches:\n","            continue\n","\n","        ab_seqs = batch['antibody_seqs']\n","        ag_seqs = batch['antigen_seqs']\n","        targets = batch['pKd'].to(device)\n","\n","        pKd_pred, class_logits = model(ab_seqs, ag_seqs, device)\n","        loss = criterion(pKd_pred, targets, class_logits)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","        total_loss += loss.item()\n","        num_batches_processed += 1\n","        global_step = epoch * len(train_loader) + batch_idx\n","        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}'})\n","\n","        # Log to TensorBoard every 100 batches\n","        if batch_idx % 100 == 0:\n","            writer.add_scalar('Train/BatchLoss', loss.item(), global_step)\n","            writer.add_scalar('Train/LearningRate', scheduler.get_last_lr()[0], global_step)\n","\n","        # Save checkpoint every 1000 steps\n","        if (global_step + 1) % 1000 == 0:\n","            step_checkpoint_path = os.path.join(OUTPUT_DIR, f'checkpoint_step_{global_step+1}.pth')\n","            save_checkpoint(step_checkpoint_path, epoch, global_step + 1, batch_idx + 1)\n","\n","            # Delete previous step checkpoint permanently\n","            if prev_step_checkpoint:\n","                permanent_delete(prev_step_checkpoint)\n","            prev_step_checkpoint = step_checkpoint_path\n","\n","            # Also update the latest checkpoint\n","            save_checkpoint(checkpoint_path, epoch, global_step + 1, batch_idx + 1)\n","\n","            print(f\"\\n  Saved step checkpoint at step {global_step+1}\")\n","\n","    # Validate\n","    metrics, val_preds, val_targets = evaluate(model, val_loader, device)\n","    elapsed = time.time() - start\n","    current_lr = scheduler.get_last_lr()[0]\n","\n","    # Step plateau scheduler\n","    plateau_scheduler.step(metrics['spearman'])\n","\n","    # Prediction distribution\n","    pred_mean = np.mean(val_preds)\n","    pred_std = np.std(val_preds)\n","    pred_min = np.min(val_preds)\n","    pred_max = np.max(val_preds)\n","\n","    # Log epoch metrics to TensorBoard\n","    avg_loss = total_loss / num_batches_processed if num_batches_processed > 0 else 0\n","    writer.add_scalar('Train/EpochLoss', avg_loss, epoch)\n","    writer.add_scalar('Val/Spearman', metrics['spearman'], epoch)\n","    writer.add_scalar('Val/Recall', metrics['recall'], epoch)\n","    writer.add_scalar('Val/RMSE', metrics['rmse'], epoch)\n","    writer.add_scalar('Val/PredMean', pred_mean, epoch)\n","    writer.add_scalar('Val/PredStd', pred_std, epoch)\n","    writer.add_histogram('Val/Predictions', val_preds, epoch)\n","    writer.add_histogram('Val/Targets', val_targets, epoch)\n","\n","    print(f\"Loss: {avg_loss:.4f} | Spearman: {metrics['spearman']:.4f} | \"\n","            f\"Recall: {metrics['recall']:.1f}% | LR: {current_lr:.2e} | Time: {elapsed:.1f}s\")\n","    print(f\"  Pred dist: mean={pred_mean:.2f}, std={pred_std:.2f}, range=[{pred_min:.2f}, {pred_max:.2f}]\")\n","\n","    # Warning if predictions collapse\n","    if pred_std < 0.5:\n","        print(f\"  WARNING: Low prediction variance! Model may be collapsing.\")\n","\n","    # Save best model\n","    if metrics['spearman'] > best_spearman:\n","        best_spearman = metrics['spearman']\n","        torch.save({\n","            'epoch': epoch,\n","            'global_step': (epoch + 1) * len(train_loader),\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'plateau_scheduler_state_dict': plateau_scheduler.state_dict(),\n","            'spearman': best_spearman,\n","            'hyperparams': hyperparams,\n","        }, model_path)\n","        print(f\"  * Saved best! (Spearman: {best_spearman:.4f})\")\n","\n","    # Update history\n","    history['loss'].append(avg_loss)\n","    history['spearman'].append(metrics['spearman'])\n","    history['recall'].append(metrics['recall'])\n","    history['lr'].append(current_lr)\n","\n","    # Save epoch checkpoint (batch_idx=0 signals end of epoch)\n","    epoch_checkpoint_path = os.path.join(OUTPUT_DIR, f'checkpoint_epoch_{epoch+1}.pth')\n","    save_checkpoint(epoch_checkpoint_path, epoch, (epoch + 1) * len(train_loader), batch_idx=0)\n","\n","    # Delete previous epoch checkpoint permanently\n","    if prev_epoch_checkpoint:\n","        permanent_delete(prev_epoch_checkpoint)\n","    prev_epoch_checkpoint = epoch_checkpoint_path\n","\n","    # Also update the latest checkpoint (for resuming)\n","    save_checkpoint(checkpoint_path, epoch, (epoch + 1) * len(train_loader), batch_idx=0)\n","\n","    print(f\"  Saved epoch {epoch+1} checkpoint\")\n","\n","    # Early stopping\n","    if early_stopping(metrics['spearman']):\n","        print(f\"\\nEarly stopping triggered after {epoch+1} epochs!\")\n","        print(f\"Best Spearman: {best_spearman:.4f}\")\n","        break\n","\n","# Close TensorBoard writer\n","writer.close()\n","print(f\"\\nTraining complete! Best Spearman: {best_spearman:.4f}\")\n","print(f\"TensorBoard logs saved to: {log_dir}\")\n","print(\"To view: tensorboard --logdir {log_dir}\")"]},{"cell_type":"markdown","metadata":{"id":"eval-header"},"source":["# Step 7: Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evaluate"},"outputs":[],"source":["# Load best and evaluate\n","checkpoint = torch.load(model_path, weights_only=False)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n","\n","# Validation\n","val_metrics, val_preds, val_targets = evaluate(model, val_loader, device)\n","print(f\"\\nVALIDATION:\")\n","print(f\"  Spearman: {val_metrics['spearman']:.4f}\")\n","print(f\"  RMSE: {val_metrics['rmse']:.4f}\")\n","print(f\"  Recall: {val_metrics['recall']:.1f}%\")\n","\n","# Test\n","test_metrics, test_preds, test_targets = evaluate(model, test_loader, device)\n","print(f\"\\nTEST (Final Performance):\")\n","print(f\"  Spearman: {test_metrics['spearman']:.4f}\")\n","print(f\"  RMSE: {test_metrics['rmse']:.4f}\")\n","print(f\"  Recall: {test_metrics['recall']:.1f}%\")\n","print(f\"  Precision: {test_metrics['precision']:.1f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"save-results"},"outputs":[],"source":["# Save results\n","pd.DataFrame({\n","    'true': test_targets, 'pred': test_preds,\n","    'error': test_preds - test_targets\n","}).to_csv(os.path.join(OUTPUT_DIR, 'test_predictions.csv'), index=False)\n","\n","with open(os.path.join(OUTPUT_DIR, 'metrics.json'), 'w') as f:\n","    json.dump({\n","        'test': test_metrics,\n","        'val': val_metrics,\n","        'hyperparameters': {\n","            'batch_size': BATCH_SIZE,\n","            'learning_rate': LEARNING_RATE,\n","            'dropout': DROPOUT,\n","            'huber_weight': HUBER_WEIGHT,\n","            'spearman_weight': SPEARMAN_WEIGHT,\n","            'class_weight': CLASS_WEIGHT,\n","            'use_cross_attention': USE_CROSS_ATTENTION,\n","            'warmup_epochs': WARMUP_EPOCHS,\n","            'epochs': EPOCHS\n","        }\n","    }, f, indent=2, default=float)\n","\n","print(f\"\\nResults saved to {OUTPUT_DIR}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"visualize"},"outputs":[],"source":["# Visualization\n","fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n","\n","# Training curve\n","axes[0].plot(history['spearman'], 'g-o')\n","axes[0].axhline(best_spearman, color='r', linestyle='--', label=f'Best: {best_spearman:.4f}')\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Validation Spearman')\n","axes[0].set_title('Training Progress')\n","axes[0].legend()\n","axes[0].grid(alpha=0.3)\n","\n","# Predictions\n","axes[1].scatter(test_targets, test_preds, alpha=0.3, s=10)\n","axes[1].plot([4, 14], [4, 14], 'r--', label='Perfect')\n","axes[1].set_xlabel('True pKd')\n","axes[1].set_ylabel('Predicted pKd')\n","axes[1].set_title(f'Test Set (Spearman: {test_metrics[\"spearman\"]:.4f})')\n","axes[1].legend()\n","axes[1].grid(alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(OUTPUT_DIR, 'results.png'), dpi=300)\n","plt.show()\n","\n","print(\"Done!\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"81d82d558106444bbf89d600157440ea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4aa01ad8fc3b46f8b6851b1a33fbecde","IPY_MODEL_6e7ca2930a0c4ef6b8f9730cbf5cdb33","IPY_MODEL_b0c29770e3c5418f954a34f748ee7dda"],"layout":"IPY_MODEL_374b4793a4064cc4b3a857ec0e95627e"}},"4aa01ad8fc3b46f8b6851b1a33fbecde":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_014352c59c5d483083d90905f4e68612","placeholder":"​","style":"IPY_MODEL_3e247ac618af4510a6f76979a89a6f29","value":"Loading checkpoint shards: 100%"}},"6e7ca2930a0c4ef6b8f9730cbf5cdb33":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_119ea7c6285f46df879496059b07f957","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6065315783fa410a94ec99fff7685a46","value":3}},"b0c29770e3c5418f954a34f748ee7dda":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ac4953b5d8845869c90ec513695ebbc","placeholder":"​","style":"IPY_MODEL_10f4996946424c82b105df5836a33bd7","value":" 3/3 [00:00&lt;00:00, 68.30it/s]"}},"374b4793a4064cc4b3a857ec0e95627e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"014352c59c5d483083d90905f4e68612":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e247ac618af4510a6f76979a89a6f29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"119ea7c6285f46df879496059b07f957":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6065315783fa410a94ec99fff7685a46":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2ac4953b5d8845869c90ec513695ebbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10f4996946424c82b105df5836a33bd7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39bba7a758974f858df365bf1ebf8707":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8282774e438c481187b4585a93b70754","IPY_MODEL_559ecdf1292f49808a0937b473ff0fac","IPY_MODEL_86b483893c2f47bcbce9070aa8641f86"],"layout":"IPY_MODEL_c1e60544232548e695476cfa5a8964bd"}},"8282774e438c481187b4585a93b70754":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c85ee11f36ad499eb5a96bc7ff8e4915","placeholder":"​","style":"IPY_MODEL_0811d809ddbb4af5b293a4c03ef21134","value":"Loading checkpoint shards: 100%"}},"559ecdf1292f49808a0937b473ff0fac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd836a78aeb548a7b4af871d5827dc70","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5c9585eee19f4a299627c0e6a61644dd","value":2}},"86b483893c2f47bcbce9070aa8641f86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_845aeb0baa97405390a51f3ea30cdcc8","placeholder":"​","style":"IPY_MODEL_964b015d6d154995ac17770288b45aad","value":" 2/2 [00:00&lt;00:00, 19.12it/s]"}},"c1e60544232548e695476cfa5a8964bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c85ee11f36ad499eb5a96bc7ff8e4915":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0811d809ddbb4af5b293a4c03ef21134":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd836a78aeb548a7b4af871d5827dc70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c9585eee19f4a299627c0e6a61644dd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"845aeb0baa97405390a51f3ea30cdcc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"964b015d6d154995ac17770288b45aad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4fdd7db102dc4e90b21db85df74ccba3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1200fa516404ce9b0445d6bd416ded2","IPY_MODEL_6dd9b2f81ce049169edb5b5609603905","IPY_MODEL_b1012f2266c64f11bed0995b02ae79c1"],"layout":"IPY_MODEL_7e85aa2491f648e6b44ef52e189898bb"}},"d1200fa516404ce9b0445d6bd416ded2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_faf1c9b99dd94a3ca047b573b1ef23e9","placeholder":"​","style":"IPY_MODEL_24c44e47fb4541f885cae96944859968","value":"Epoch 1/50:   0%"}},"6dd9b2f81ce049169edb5b5609603905":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_380a67b386af43d783a0ee3e123a9fa0","max":3495,"min":0,"orientation":"horizontal","style":"IPY_MODEL_89575066be304a94a267b61623222a8e","value":0}},"b1012f2266c64f11bed0995b02ae79c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07927c5856724b71a7d331358aa36e1c","placeholder":"​","style":"IPY_MODEL_7f33dbac6b4d49e5ac09a0ab0cae5d27","value":" 0/3495 [00:00&lt;?, ?it/s]"}},"7e85aa2491f648e6b44ef52e189898bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"faf1c9b99dd94a3ca047b573b1ef23e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24c44e47fb4541f885cae96944859968":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"380a67b386af43d783a0ee3e123a9fa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89575066be304a94a267b61623222a8e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"07927c5856724b71a7d331358aa36e1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f33dbac6b4d49e5ac09a0ab0cae5d27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}