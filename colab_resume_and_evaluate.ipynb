{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Training or Evaluate Saved Model\n",
    "\n",
    "**Use this notebook when:**\n",
    "- Your Colab session timed out\n",
    "- Training was interrupted\n",
    "- You want to evaluate an already-trained model\n",
    "- You want to continue training from a checkpoint\n",
    "\n",
    "**This notebook will:**\n",
    "1. Reconnect to Google Drive\n",
    "2. Load your saved model/checkpoint\n",
    "3. Continue training OR run evaluation\n",
    "4. Generate all plots and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup - Reconnect Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers scikit-learn pandas numpy tqdm matplotlib seaborn\n",
    "print(\"✅ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"✅ Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ All imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths - UPDATE THESE to match your Drive location\n",
    "DRIVE_DATA_PATH = \"/content/drive/MyDrive/AbAg_data/merged_with_all_features.csv\"\n",
    "MODEL_DIR = \"/content/drive/MyDrive/AbAg_data/models\"\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Constants\n",
    "BINS = [0, 5, 7, 9, 11, 16]\n",
    "BIN_LABELS = ['very_weak', 'weak', 'moderate', 'strong', 'very_strong']\n",
    "\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# List available checkpoints\n",
    "if os.path.exists(MODEL_DIR):\n",
    "    print(\"\\nAvailable files:\")\n",
    "    for f in sorted(os.listdir(MODEL_DIR)):\n",
    "        if f.endswith('.pth'):\n",
    "            size_mb = os.path.getsize(os.path.join(MODEL_DIR, f)) / 1e6\n",
    "            print(f\"  - {f} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ Model directory not found! Check the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recreate Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class AffinityDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "print(\"✅ Dataset class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture (MUST match training)\n",
    "class AffinityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=150, hidden_dims=[256, 128], dropout=0.3):\n",
    "        super(AffinityPredictor, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "print(\"✅ Model class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted MSE Loss\n",
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self, bin_weights, bins_edges):\n",
    "        super().__init__()\n",
    "        self.bin_weights = bin_weights\n",
    "        self.bins = bins_edges\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        weights = torch.ones_like(targets)\n",
    "        for i, (low, high) in enumerate(zip(self.bins[:-1], self.bins[1:])):\n",
    "            mask = (targets >= low) & (targets < high)\n",
    "            weights[mask] = self.bin_weights[i]\n",
    "        \n",
    "        mse = (predictions - targets) ** 2\n",
    "        weighted_mse = mse * weights\n",
    "        return weighted_mse.mean()\n",
    "\n",
    "print(\"✅ Weighted MSE loss defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model\n",
    "\n",
    "**Choose which model to load:**\n",
    "- `best_model.pth` - Best validation loss during training\n",
    "- `final_model.pth` - Final model with metadata\n",
    "- `checkpoint_epoch_XX.pth` - Specific checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model - CHANGE THIS if you want a different checkpoint\n",
    "MODEL_PATH = f\"{MODEL_DIR}/best_model.pth\"\n",
    "\n",
    "# Initialize model\n",
    "model = AffinityPredictor(input_dim=150, hidden_dims=[256, 128], dropout=0.3)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Load weights\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    checkpoint = torch.load(MODEL_PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"✅ Model loaded from: {MODEL_PATH}\")\n",
    "    \n",
    "    if 'epoch' in checkpoint:\n",
    "        print(f\"   Epoch: {checkpoint['epoch']+1}\")\n",
    "    if 'val_loss' in checkpoint:\n",
    "        print(f\"   Val loss: {checkpoint['val_loss']:.4f}\")\n",
    "    if 'metrics' in checkpoint:\n",
    "        print(f\"   Metrics: {checkpoint['metrics']}\")\n",
    "else:\n",
    "    print(f\"❌ Model not found at: {MODEL_PATH}\")\n",
    "    print(\"   Please check the path or choose a different checkpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data to local if needed\n",
    "LOCAL_DATA_PATH = \"/content/merged_with_all_features.csv\"\n",
    "\n",
    "if not os.path.exists(LOCAL_DATA_PATH) and os.path.exists(DRIVE_DATA_PATH):\n",
    "    print(\"Copying data to local storage...\")\n",
    "    !cp \"{DRIVE_DATA_PATH}\" \"{LOCAL_DATA_PATH}\"\n",
    "    print(\"✅ Data copied!\")\n",
    "    DATA_PATH = LOCAL_DATA_PATH\n",
    "elif os.path.exists(LOCAL_DATA_PATH):\n",
    "    print(\"✅ Using local data\")\n",
    "    DATA_PATH = LOCAL_DATA_PATH\n",
    "else:\n",
    "    print(\"✅ Using Drive data\")\n",
    "    DATA_PATH = DRIVE_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data (same split as training)\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "\n",
    "pca_cols = [f'esm2_pca_{i}' for i in range(150)]\n",
    "df_with_features = df[df[pca_cols[0]].notna()].copy()\n",
    "print(f\"✅ Loaded {len(df_with_features):,} samples with features\")\n",
    "\n",
    "# Extract features\n",
    "X = df_with_features[pca_cols].values\n",
    "y = df_with_features['pKd'].values\n",
    "\n",
    "# Same split as training (must use same random_state!)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15/0.85, random_state=42)\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"  Train: {len(X_train):,}\")\n",
    "print(f\"  Val: {len(X_val):,}\")\n",
    "print(f\"  Test: {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset and loader\n",
    "test_dataset = AffinityDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "with torch.no_grad():\n",
    "    for features, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        features = features.to(DEVICE)\n",
    "        predictions = model(features)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_targets.extend(labels.numpy())\n",
    "\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_targets = np.array(test_targets)\n",
    "\n",
    "print(\"✅ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall metrics\n",
    "mse = mean_squared_error(test_targets, test_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(test_targets, test_predictions)\n",
    "spearman = spearmanr(test_targets, test_predictions)[0]\n",
    "pearson = pearsonr(test_targets, test_predictions)[0]\n",
    "r2 = 1 - (np.sum((test_targets - test_predictions)**2) / np.sum((test_targets - test_targets.mean())**2))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"RMSE:        {rmse:.4f}\")\n",
    "print(f\"MAE:         {mae:.4f}\")\n",
    "print(f\"Spearman ρ:  {spearman:.4f}\")\n",
    "print(f\"Pearson r:   {pearson:.4f}\")\n",
    "print(f\"R²:          {r2:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-bin metrics\n",
    "test_df = pd.DataFrame({\n",
    "    'target': test_targets,\n",
    "    'prediction': test_predictions\n",
    "})\n",
    "test_df['affinity_bin'] = pd.cut(test_df['target'], bins=BINS, labels=BIN_LABELS, include_lowest=True)\n",
    "\n",
    "print(\"\\nPER-BIN PERFORMANCE:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Bin':<15} | {'Count':<8} | {'RMSE':<8} | {'MAE':<8}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for label in BIN_LABELS:\n",
    "    bin_data = test_df[test_df['affinity_bin'] == label]\n",
    "    if len(bin_data) > 0:\n",
    "        bin_rmse = np.sqrt(mean_squared_error(bin_data['target'], bin_data['prediction']))\n",
    "        bin_mae = mean_absolute_error(bin_data['target'], bin_data['prediction'])\n",
    "        print(f\"{label:<15} | {len(bin_data):<8} | {bin_rmse:<8.4f} | {bin_mae:<8.4f}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions vs targets\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(test_targets, test_predictions, alpha=0.3, s=10)\n",
    "plt.plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], 'r--', lw=2)\n",
    "plt.xlabel('True pKd', fontsize=12)\n",
    "plt.ylabel('Predicted pKd', fontsize=12)\n",
    "plt.title(f'Test Set Predictions\\nSpearman ρ = {spearman:.4f}, RMSE = {rmse:.4f}', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.savefig(f'{MODEL_DIR}/predictions_vs_targets.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✅ Prediction plot saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals analysis\n",
    "residuals = test_predictions - test_targets\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Residuals vs predictions\n",
    "axes[0].scatter(test_predictions, residuals, alpha=0.3, s=10)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[0].set_xlabel('Predicted pKd', fontsize=12)\n",
    "axes[0].set_ylabel('Residuals (Predicted - True)', fontsize=12)\n",
    "axes[0].set_title('Residuals vs Predictions', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals distribution\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Residuals', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title(f'Residuals Distribution\\nMean = {residuals.mean():.4f}, Std = {residuals.std():.4f}', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_DIR}/residuals_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✅ Residuals plot saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-bin performance visualization\n",
    "bin_metrics = []\n",
    "for label in BIN_LABELS:\n",
    "    bin_data = test_df[test_df['affinity_bin'] == label]\n",
    "    if len(bin_data) > 0:\n",
    "        bin_rmse = np.sqrt(mean_squared_error(bin_data['target'], bin_data['prediction']))\n",
    "        bin_mae = mean_absolute_error(bin_data['target'], bin_data['prediction'])  \n",
    "        bin_metrics.append({'bin': label, 'RMSE': bin_rmse, 'MAE': bin_mae, 'count': len(bin_data)})\n",
    "\n",
    "bin_df = pd.DataFrame(bin_metrics)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# RMSE by bin\n",
    "axes[0].bar(bin_df['bin'], bin_df['RMSE'], alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Affinity Bin', fontsize=12)\n",
    "axes[0].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0].set_title('RMSE by Affinity Bin', fontsize=14)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Sample counts by bin\n",
    "axes[1].bar(bin_df['bin'], bin_df['count'], alpha=0.7, edgecolor='black', color='green')\n",
    "axes[1].set_xlabel('Affinity Bin', fontsize=12)\n",
    "axes[1].set_ylabel('Sample Count', fontsize=12)\n",
    "axes[1].set_title('Test Set Distribution', fontsize=14)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_DIR}/per_bin_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✅ Per-bin analysis plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results\n",
    "results_summary = f\"\"\"\n",
    "AbAg Binding Affinity Prediction - Evaluation Results\n",
    "{'='*70}\n",
    "\n",
    "Model: {MODEL_PATH}\n",
    "Test samples: {len(X_test):,}\n",
    "\n",
    "Overall Performance:\n",
    "  - RMSE:       {rmse:.4f}\n",
    "  - MAE:        {mae:.4f}\n",
    "  - Spearman ρ: {spearman:.4f}\n",
    "  - Pearson r:  {pearson:.4f}\n",
    "  - R²:         {r2:.4f}\n",
    "\n",
    "Per-Bin Performance:\n",
    "\"\"\"\n",
    "\n",
    "for label in BIN_LABELS:\n",
    "    bin_data = test_df[test_df['affinity_bin'] == label]\n",
    "    if len(bin_data) > 0:\n",
    "        bin_rmse = np.sqrt(mean_squared_error(bin_data['target'], bin_data['prediction']))\n",
    "        bin_mae = mean_absolute_error(bin_data['target'], bin_data['prediction'])\n",
    "        results_summary += f\"  - {label:<15}: RMSE={bin_rmse:6.4f}, MAE={bin_mae:6.4f}, N={len(bin_data):6,}\\n\"\n",
    "\n",
    "results_summary += f\"\\n{'='*70}\\n\"\n",
    "\n",
    "# Save to file\n",
    "with open(f'{MODEL_DIR}/evaluation_results.txt', 'w') as f:\n",
    "    f.write(results_summary)\n",
    "\n",
    "print(results_summary)\n",
    "print(f\"✅ Results saved to {MODEL_DIR}/evaluation_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV for further analysis\n",
    "results_df = pd.DataFrame({\n",
    "    'true_pKd': test_targets,\n",
    "    'predicted_pKd': test_predictions,\n",
    "    'residual': residuals,\n",
    "    'affinity_bin': test_df['affinity_bin']\n",
    "})\n",
    "\n",
    "results_df.to_csv(f'{MODEL_DIR}/test_predictions.csv', index=False)\n",
    "print(f\"✅ Predictions saved to {MODEL_DIR}/test_predictions.csv\")\n",
    "print(f\"\\nAll files saved to Google Drive: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "**Files saved to Google Drive:**\n",
    "- `evaluation_results.txt` - Detailed metrics\n",
    "- `test_predictions.csv` - All predictions for analysis\n",
    "- `predictions_vs_targets.png` - Scatter plot\n",
    "- `residuals_analysis.png` - Residuals plots\n",
    "- `per_bin_analysis.png` - Per-bin performance\n",
    "\n",
    "**You can now:**\n",
    "1. Download these files from Google Drive\n",
    "2. Use the model locally (see next notebook/script)\n",
    "3. Share results\n",
    "4. Continue training from checkpoint if needed"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
